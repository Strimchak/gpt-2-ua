{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "GPT_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6eHyHN7UQxe",
        "outputId": "2ede97f3-97ce-4f59-86d0-8645f3cefde2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9bSmd0LVHIY",
        "outputId": "6dff1b27-41cf-4e9d-e55c-e324432e1399"
      },
      "source": [
        "%cd drive\n",
        "%cd My\\ Drive\n",
        "%mkdir ua_test\n",
        "%cd /content/\n",
        "!ls\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive\n",
            "/content/drive/My Drive\n",
            "mkdir: cannot create directory ‘ua_test’: File exists\n",
            "/content\n",
            "drive  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ohnk6qCaVMiW",
        "outputId": "db77c073-af42-4912-db24-6fb4ae547c68"
      },
      "source": [
        "cd /content/drive/MyDrive/ua_test\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/ua_test\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPAZ9mBPVTV_",
        "outputId": "cb8262cc-6163-428c-aa56-d9ce70555366"
      },
      "source": [
        "!git clone https://github.com/nshepperd/gpt-2.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'gpt-2' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4BUChEhDVvJ-",
        "outputId": "8d79ec96-3a66-44da-bb67-687e0511dd8d"
      },
      "source": [
        "!python3 download_model.py 117M"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fetching checkpoint: 1.00kit [00:00, 905kit/s]                                                      \n",
            "Fetching encoder.json: 1.04Mit [00:00, 4.91Mit/s]                                                   \n",
            "Fetching hparams.json: 1.00kit [00:00, 1.06Mit/s]                                                   \n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mit [01:04, 7.75Mit/s]                                  \n",
            "Fetching model.ckpt.index: 6.00kit [00:00, 4.95Mit/s]                                               \n",
            "Fetching model.ckpt.meta: 472kit [00:00, 3.42Mit/s]                                                 \n",
            "Fetching vocab.bpe: 457kit [00:00, 3.32Mit/s]                                                       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqGer7e1Y_E3"
      },
      "source": [
        "Cuba i tensorflow?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3WRHkafyboS",
        "outputId": "c13df0a0-d19d-4f7e-d7fb-f79c344761fe"
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lCd_4YnyhrG",
        "outputId": "7a296c03-27ba-4015-b0d3-95d6f2005158"
      },
      "source": [
        "!pip show tensorflow-estimator"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Name: tensorflow-estimator\n",
            "Version: 2.5.0\n",
            "Summary: TensorFlow Estimator.\n",
            "Home-page: https://www.tensorflow.org/\n",
            "Author: Google Inc.\n",
            "Author-email: None\n",
            "License: Apache 2.0\n",
            "Location: /usr/local/lib/python3.7/dist-packages\n",
            "Requires: \n",
            "Required-by: tensorflow\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8WwiKjVcsgc",
        "outputId": "bec510b1-f8d0-427e-a90f-839a361b49b7"
      },
      "source": [
        "!pip install tensorflow==1.15.0\n",
        "!pip install tensorflow-gpu==1.15.0\n",
        "!pip install 'tensorflow-estimator<1.15.0rc0,>=1.14.0rc0' --force-reinstall"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.15.0\n",
            "  Downloading tensorflow-1.15.0-cp37-cp37m-manylinux2010_x86_64.whl (412.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 412.3 MB 12 kB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (0.37.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (3.3.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (0.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.1.0)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "  Downloading tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503 kB)\n",
            "\u001b[K     |████████████████████████████████| 503 kB 41.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.39.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (0.8.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (3.17.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (0.2.0)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "  Downloading tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 35.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.12.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.19.5)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.1.2)\n",
            "Collecting keras-applications>=1.0.8\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 6.2 MB/s \n",
            "\u001b[?25hCollecting gast==0.2.2\n",
            "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.15.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15.0) (3.1.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.3.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (57.4.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (4.6.4)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow==1.15.0) (1.5.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.7.4.3)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7554 sha256=8b4ca1f799cc17d01de9b69c8eb9002182d78c6a707d8b222d5ccc94e299d269\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n",
            "Successfully built gast\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, keras-applications, gast, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.6.0\n",
            "    Uninstalling tensorflow-estimator-2.6.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.6.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.6.0\n",
            "    Uninstalling tensorboard-2.6.0:\n",
            "      Successfully uninstalled tensorboard-2.6.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.4.0\n",
            "    Uninstalling gast-0.4.0:\n",
            "      Successfully uninstalled gast-0.4.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.6.0\n",
            "    Uninstalling tensorflow-2.6.0:\n",
            "      Successfully uninstalled tensorflow-2.6.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-probability 0.13.0 requires gast>=0.3.2, but you have gast 0.2.2 which is incompatible.\n",
            "kapre 0.3.5 requires tensorflow>=2.0.0, but you have tensorflow 1.15.0 which is incompatible.\u001b[0m\n",
            "Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-1.15.0 tensorflow-estimator-1.15.1\n",
            "Collecting tensorflow-gpu==1.15.0\n",
            "  Downloading tensorflow_gpu-1.15.0-cp37-cp37m-manylinux2010_x86_64.whl (411.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 411.5 MB 7.4 kB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (3.3.0)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (0.2.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (1.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (1.39.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (0.37.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (1.15.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (1.19.5)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (1.1.2)\n",
            "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (1.15.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (0.2.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (0.12.0)\n",
            "Requirement already satisfied: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (1.15.1)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (1.12.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (3.17.3)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (1.0.8)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (0.8.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==1.15.0) (3.1.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.0) (3.3.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.0) (57.4.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.0) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.0) (4.6.4)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow-gpu==1.15.0) (1.5.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.0) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.0) (3.7.4.3)\n",
            "Installing collected packages: tensorflow-gpu\n",
            "Successfully installed tensorflow-gpu-1.15.0\n",
            "Collecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0\n",
            "  Downloading tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488 kB)\n",
            "\u001b[K     |████████████████████████████████| 488 kB 4.0 MB/s \n",
            "\u001b[?25hInstalling collected packages: tensorflow-estimator\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 1.15.1\n",
            "    Uninstalling tensorflow-estimator-1.15.1:\n",
            "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 1.15.0 requires tensorflow-estimator==1.15.1, but you have tensorflow-estimator 1.14.0 which is incompatible.\n",
            "tensorflow-gpu 1.15.0 requires tensorflow-estimator==1.15.1, but you have tensorflow-estimator 1.14.0 which is incompatible.\n",
            "kapre 0.3.5 requires tensorflow>=2.0.0, but you have tensorflow 1.15.0 which is incompatible.\u001b[0m\n",
            "Successfully installed tensorflow-estimator-1.14.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NbJia3fmctmB",
        "outputId": "d0e4de62-e2f2-482e-e7b9-b8e9d67820a6"
      },
      "source": [
        "#!wget https://developer.nvidia.com/compute/cuda/9.0/Prod/local_installers/cuda-repo-ubuntu1604-9-0-local_9.0.176-1_amd64-deb\n",
        "!dpkg -i cuda-repo-ubuntu1604-9-0-local_9.0.176-1_amd64-deb\n",
        "!apt-key add /var/cuda-repo-*/7fa2af80.pub\n",
        "!apt-get update\n",
        "!apt-get install cuda-9-0\n",
        "!export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda-9.0/lib64/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Selecting previously unselected package cuda-repo-ubuntu1604-9-0-local.\n",
            "(Reading database ... 148486 files and directories currently installed.)\n",
            "Preparing to unpack cuda-repo-ubuntu1604-9-0-local_9.0.176-1_amd64-deb ...\n",
            "Unpacking cuda-repo-ubuntu1604-9-0-local (9.0.176-1) ...\n",
            "Setting up cuda-repo-ubuntu1604-9-0-local (9.0.176-1) ...\n",
            "OK\n",
            "Get:1 file:/var/cuda-repo-9-0-local  InRelease\n",
            "Ign:1 file:/var/cuda-repo-9-0-local  InRelease\n",
            "Get:2 file:/var/cuda-repo-9-0-local  Release [574 B]\n",
            "Get:2 file:/var/cuda-repo-9-0-local  Release [574 B]\n",
            "Get:3 file:/var/cuda-repo-9-0-local  Release.gpg [819 B]\n",
            "Get:3 file:/var/cuda-repo-9-0-local  Release.gpg [819 B]\n",
            "Get:4 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Ign:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:6 file:/var/cuda-repo-9-0-local  Packages [15.4 kB]\n",
            "Get:7 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Ign:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Get:10 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Hit:11 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:12 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:16 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:17 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Get:18 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [543 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Hit:20 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,730 kB]\n",
            "Get:22 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,424 kB]\n",
            "Get:23 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,792 kB]\n",
            "Get:24 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,294 kB]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [34.4 kB]\n",
            "Get:26 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [575 kB]\n",
            "Get:27 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,199 kB]\n",
            "Get:28 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [917 kB]\n",
            "Fetched 12.8 MB in 6s (2,102 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  cuda-command-line-tools-9-0 cuda-core-9-0 cuda-cublas-9-0\n",
            "  cuda-cublas-dev-9-0 cuda-cudart-9-0 cuda-cudart-dev-9-0 cuda-cufft-9-0\n",
            "  cuda-cufft-dev-9-0 cuda-curand-9-0 cuda-curand-dev-9-0 cuda-cusolver-9-0\n",
            "  cuda-cusolver-dev-9-0 cuda-cusparse-9-0 cuda-cusparse-dev-9-0\n",
            "  cuda-demo-suite-9-0 cuda-documentation-9-0 cuda-driver-dev-9-0\n",
            "  cuda-libraries-9-0 cuda-libraries-dev-9-0 cuda-license-9-0\n",
            "  cuda-misc-headers-9-0 cuda-npp-9-0 cuda-npp-dev-9-0 cuda-nvgraph-9-0\n",
            "  cuda-nvgraph-dev-9-0 cuda-nvml-dev-9-0 cuda-nvrtc-9-0 cuda-nvrtc-dev-9-0\n",
            "  cuda-runtime-9-0 cuda-samples-9-0 cuda-toolkit-9-0 cuda-visual-tools-9-0\n",
            "The following NEW packages will be installed:\n",
            "  cuda-9-0 cuda-command-line-tools-9-0 cuda-core-9-0 cuda-cublas-9-0\n",
            "  cuda-cublas-dev-9-0 cuda-cudart-9-0 cuda-cudart-dev-9-0 cuda-cufft-9-0\n",
            "  cuda-cufft-dev-9-0 cuda-curand-9-0 cuda-curand-dev-9-0 cuda-cusolver-9-0\n",
            "  cuda-cusolver-dev-9-0 cuda-cusparse-9-0 cuda-cusparse-dev-9-0\n",
            "  cuda-demo-suite-9-0 cuda-documentation-9-0 cuda-driver-dev-9-0\n",
            "  cuda-libraries-9-0 cuda-libraries-dev-9-0 cuda-license-9-0\n",
            "  cuda-misc-headers-9-0 cuda-npp-9-0 cuda-npp-dev-9-0 cuda-nvgraph-9-0\n",
            "  cuda-nvgraph-dev-9-0 cuda-nvml-dev-9-0 cuda-nvrtc-9-0 cuda-nvrtc-dev-9-0\n",
            "  cuda-runtime-9-0 cuda-samples-9-0 cuda-toolkit-9-0 cuda-visual-tools-9-0\n",
            "0 upgraded, 33 newly installed, 0 to remove and 50 not upgraded.\n",
            "Need to get 0 B/1,097 MB of archives.\n",
            "After this operation, 2,315 MB of additional disk space will be used.\n",
            "Get:1 file:/var/cuda-repo-9-0-local  cuda-license-9-0 9.0.176-1 [22.0 kB]\n",
            "Get:2 file:/var/cuda-repo-9-0-local  cuda-misc-headers-9-0 9.0.176-1 [684 kB]\n",
            "Get:3 file:/var/cuda-repo-9-0-local  cuda-core-9-0 9.0.176-1 [16.9 MB]\n",
            "Get:4 file:/var/cuda-repo-9-0-local  cuda-cudart-9-0 9.0.176-1 [106 kB]\n",
            "Get:5 file:/var/cuda-repo-9-0-local  cuda-driver-dev-9-0 9.0.176-1 [10.9 kB]\n",
            "Get:6 file:/var/cuda-repo-9-0-local  cuda-cudart-dev-9-0 9.0.176-1 [767 kB]\n",
            "Get:7 file:/var/cuda-repo-9-0-local  cuda-command-line-tools-9-0 9.0.176-1 [25.4 MB]\n",
            "Get:8 file:/var/cuda-repo-9-0-local  cuda-nvrtc-9-0 9.0.176-1 [6,348 kB]\n",
            "Get:9 file:/var/cuda-repo-9-0-local  cuda-nvrtc-dev-9-0 9.0.176-1 [9,334 B]\n",
            "Get:10 file:/var/cuda-repo-9-0-local  cuda-cusolver-9-0 9.0.176-1 [26.2 MB]\n",
            "Get:11 file:/var/cuda-repo-9-0-local  cuda-cusolver-dev-9-0 9.0.176-1 [5,317 kB]\n",
            "Get:12 file:/var/cuda-repo-9-0-local  cuda-cublas-9-0 9.0.176-1 [25.0 MB]\n",
            "Get:13 file:/var/cuda-repo-9-0-local  cuda-cublas-dev-9-0 9.0.176-1 [49.4 MB]\n",
            "Get:14 file:/var/cuda-repo-9-0-local  cuda-cufft-9-0 9.0.176-1 [84.1 MB]\n",
            "Get:15 file:/var/cuda-repo-9-0-local  cuda-cufft-dev-9-0 9.0.176-1 [73.7 MB]\n",
            "Get:16 file:/var/cuda-repo-9-0-local  cuda-curand-9-0 9.0.176-1 [38.8 MB]\n",
            "Get:17 file:/var/cuda-repo-9-0-local  cuda-curand-dev-9-0 9.0.176-1 [57.9 MB]\n",
            "Get:18 file:/var/cuda-repo-9-0-local  cuda-cusparse-9-0 9.0.176-1 [25.2 MB]\n",
            "Get:19 file:/var/cuda-repo-9-0-local  cuda-cusparse-dev-9-0 9.0.176-1 [25.3 MB]\n",
            "Get:20 file:/var/cuda-repo-9-0-local  cuda-npp-9-0 9.0.176-1 [46.6 MB]\n",
            "Get:21 file:/var/cuda-repo-9-0-local  cuda-npp-dev-9-0 9.0.176-1 [46.6 MB]\n",
            "Get:22 file:/var/cuda-repo-9-0-local  cuda-nvgraph-9-0 9.0.176-1 [6,081 kB]\n",
            "Get:23 file:/var/cuda-repo-9-0-local  cuda-nvgraph-dev-9-0 9.0.176-1 [5,658 kB]\n",
            "Get:24 file:/var/cuda-repo-9-0-local  cuda-samples-9-0 9.0.176-1 [75.9 MB]\n",
            "Get:25 file:/var/cuda-repo-9-0-local  cuda-documentation-9-0 9.0.176-1 [53.1 MB]\n",
            "Get:26 file:/var/cuda-repo-9-0-local  cuda-libraries-dev-9-0 9.0.176-1 [2,596 B]\n",
            "Get:27 file:/var/cuda-repo-9-0-local  cuda-nvml-dev-9-0 9.0.176-1 [47.6 kB]\n",
            "Get:28 file:/var/cuda-repo-9-0-local  cuda-visual-tools-9-0 9.0.176-1 [398 MB]\n",
            "Get:29 file:/var/cuda-repo-9-0-local  cuda-toolkit-9-0 9.0.176-1 [2,836 B]\n",
            "Get:30 file:/var/cuda-repo-9-0-local  cuda-libraries-9-0 9.0.176-1 [2,566 B]\n",
            "Get:31 file:/var/cuda-repo-9-0-local  cuda-runtime-9-0 9.0.176-1 [2,526 B]\n",
            "Get:32 file:/var/cuda-repo-9-0-local  cuda-demo-suite-9-0 9.0.176-1 [3,880 kB]\n",
            "Get:33 file:/var/cuda-repo-9-0-local  cuda-9-0 9.0.176-1 [2,552 B]\n",
            "Extracting templates from packages: 100%\n",
            "Selecting previously unselected package cuda-license-9-0.\n",
            "(Reading database ... 148545 files and directories currently installed.)\n",
            "Preparing to unpack .../00-cuda-license-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-license-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-misc-headers-9-0.\n",
            "Preparing to unpack .../01-cuda-misc-headers-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-misc-headers-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-core-9-0.\n",
            "Preparing to unpack .../02-cuda-core-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-core-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-cudart-9-0.\n",
            "Preparing to unpack .../03-cuda-cudart-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-cudart-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-driver-dev-9-0.\n",
            "Preparing to unpack .../04-cuda-driver-dev-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-driver-dev-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-cudart-dev-9-0.\n",
            "Preparing to unpack .../05-cuda-cudart-dev-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-cudart-dev-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-command-line-tools-9-0.\n",
            "Preparing to unpack .../06-cuda-command-line-tools-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-command-line-tools-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-nvrtc-9-0.\n",
            "Preparing to unpack .../07-cuda-nvrtc-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-nvrtc-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-nvrtc-dev-9-0.\n",
            "Preparing to unpack .../08-cuda-nvrtc-dev-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-nvrtc-dev-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-cusolver-9-0.\n",
            "Preparing to unpack .../09-cuda-cusolver-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-cusolver-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-cusolver-dev-9-0.\n",
            "Preparing to unpack .../10-cuda-cusolver-dev-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-cusolver-dev-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-cublas-9-0.\n",
            "Preparing to unpack .../11-cuda-cublas-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-cublas-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-cublas-dev-9-0.\n",
            "Preparing to unpack .../12-cuda-cublas-dev-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-cublas-dev-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-cufft-9-0.\n",
            "Preparing to unpack .../13-cuda-cufft-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-cufft-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-cufft-dev-9-0.\n",
            "Preparing to unpack .../14-cuda-cufft-dev-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-cufft-dev-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-curand-9-0.\n",
            "Preparing to unpack .../15-cuda-curand-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-curand-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-curand-dev-9-0.\n",
            "Preparing to unpack .../16-cuda-curand-dev-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-curand-dev-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-cusparse-9-0.\n",
            "Preparing to unpack .../17-cuda-cusparse-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-cusparse-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-cusparse-dev-9-0.\n",
            "Preparing to unpack .../18-cuda-cusparse-dev-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-cusparse-dev-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-npp-9-0.\n",
            "Preparing to unpack .../19-cuda-npp-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-npp-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-npp-dev-9-0.\n",
            "Preparing to unpack .../20-cuda-npp-dev-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-npp-dev-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-nvgraph-9-0.\n",
            "Preparing to unpack .../21-cuda-nvgraph-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-nvgraph-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-nvgraph-dev-9-0.\n",
            "Preparing to unpack .../22-cuda-nvgraph-dev-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-nvgraph-dev-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-samples-9-0.\n",
            "Preparing to unpack .../23-cuda-samples-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-samples-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-documentation-9-0.\n",
            "Preparing to unpack .../24-cuda-documentation-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-documentation-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-libraries-dev-9-0.\n",
            "Preparing to unpack .../25-cuda-libraries-dev-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-libraries-dev-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-nvml-dev-9-0.\n",
            "Preparing to unpack .../26-cuda-nvml-dev-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-nvml-dev-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-visual-tools-9-0.\n",
            "Preparing to unpack .../27-cuda-visual-tools-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-visual-tools-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-toolkit-9-0.\n",
            "Preparing to unpack .../28-cuda-toolkit-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-toolkit-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-libraries-9-0.\n",
            "Preparing to unpack .../29-cuda-libraries-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-libraries-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-runtime-9-0.\n",
            "Preparing to unpack .../30-cuda-runtime-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-runtime-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-demo-suite-9-0.\n",
            "Preparing to unpack .../31-cuda-demo-suite-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-demo-suite-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-9-0.\n",
            "Preparing to unpack .../32-cuda-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-license-9-0 (9.0.176-1) ...\n",
            "*** LICENSE AGREEMENT ***\n",
            "By using this software you agree to fully comply with the terms and \n",
            "conditions of the EULA (End User License Agreement). The EULA is located\n",
            "at /usr/local/cuda-9.0/doc/EULA.txt. The EULA can also be found at\n",
            "http://docs.nvidia.com/cuda/eula/index.html. If you do not agree to the\n",
            "terms and conditions of the EULA, do not use the software.\n",
            "\n",
            "Setting up cuda-cusparse-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-cudart-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-nvrtc-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-cusparse-dev-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-cufft-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-cusolver-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-nvml-dev-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-npp-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-cusolver-dev-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-misc-headers-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-cublas-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-nvrtc-dev-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-driver-dev-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-curand-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-nvgraph-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-core-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-libraries-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-runtime-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-cudart-dev-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-cufft-dev-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-npp-dev-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-curand-dev-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-cublas-dev-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-nvgraph-dev-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-command-line-tools-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-demo-suite-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-visual-tools-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-samples-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-libraries-dev-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-documentation-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-toolkit-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-9-0 (9.0.176-1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNH-rju3dWLr"
      },
      "source": [
        "Перезапуск середовища"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRdM_8HhMFjL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa54f81c-81d5-4576-a076-a2b82460020d"
      },
      "source": [
        "cd /content/drive/My Drive/ua_test/gpt-2/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/ua_test/gpt-2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Yc7bYBwY9Pf",
        "outputId": "2d4934f8-45d9-47f4-8f52-60e4c2540261"
      },
      "source": [
        "!pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fire>=0.1.3\n",
            "  Downloading fire-0.4.0.tar.gz (87 kB)\n",
            "\u001b[K     |████████████████████████████████| 87 kB 3.0 MB/s \n",
            "\u001b[?25hCollecting regex==2017.4.5\n",
            "  Downloading regex-2017.04.05.tar.gz (601 kB)\n",
            "\u001b[K     |████████████████████████████████| 601 kB 26.8 MB/s \n",
            "\u001b[?25hCollecting requests==2.21.0\n",
            "  Downloading requests-2.21.0-py2.py3-none-any.whl (57 kB)\n",
            "\u001b[K     |████████████████████████████████| 57 kB 4.9 MB/s \n",
            "\u001b[?25hCollecting tqdm==4.31.1\n",
            "  Downloading tqdm-4.31.1-py2.py3-none-any.whl (48 kB)\n",
            "\u001b[K     |████████████████████████████████| 48 kB 4.7 MB/s \n",
            "\u001b[?25hCollecting toposort==1.5\n",
            "  Downloading toposort-1.5-py2.py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (2021.5.30)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (1.24.3)\n",
            "Collecting idna<2.9,>=2.5\n",
            "  Downloading idna-2.8-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from fire>=0.1.3->-r requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from fire>=0.1.3->-r requirements.txt (line 1)) (1.1.0)\n",
            "Building wheels for collected packages: regex, fire\n",
            "  Building wheel for regex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for regex: filename=regex-2017.4.5-cp37-cp37m-linux_x86_64.whl size=534428 sha256=1748c6b6baba00d63948eee25e5057142add9f162da613cb916ba683f75814ef\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/e8/a5/d4894e7ef29935f75c6074409ce8ca80a0271f0ce2a30da5d3\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115943 sha256=558fe6360d3f398b08067d9dd3d1c7711ec6dfc8e16824b4cc6dfca4a1d8df89\n",
            "  Stored in directory: /root/.cache/pip/wheels/8a/67/fb/2e8a12fa16661b9d5af1f654bd199366799740a85c64981226\n",
            "Successfully built regex fire\n",
            "Installing collected packages: idna, tqdm, toposort, requests, regex, fire\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 2.10\n",
            "    Uninstalling idna-2.10:\n",
            "      Successfully uninstalled idna-2.10\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.62.0\n",
            "    Uninstalling tqdm-4.62.0:\n",
            "      Successfully uninstalled tqdm-4.62.0\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: regex\n",
            "    Found existing installation: regex 2019.12.20\n",
            "    Uninstalling regex-2019.12.20:\n",
            "      Successfully uninstalled regex-2019.12.20\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "spacy 2.2.4 requires tqdm<5.0.0,>=4.38.0, but you have tqdm 4.31.1 which is incompatible.\n",
            "panel 0.12.1 requires tqdm>=4.48.0, but you have tqdm 4.31.1 which is incompatible.\n",
            "kapre 0.3.5 requires tensorflow>=2.0.0, but you have tensorflow 1.15.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.21.0 which is incompatible.\n",
            "fbprophet 0.7.1 requires tqdm>=4.36.1, but you have tqdm 4.31.1 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed fire-0.4.0 idna-2.8 regex-2017.4.5 requests-2.21.0 toposort-1.5 tqdm-4.31.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7lSlm86MdHB",
        "outputId": "f85cc002-384c-467c-c1d1-fb330f0407f6"
      },
      "source": [
        "cd /content/drive/My Drive/ua_test/gpt-2/src"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/ua_test/gpt-2/src\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ThbR5Ccbxy94",
        "outputId": "80f2ca2b-19bf-4dbd-f73c-da1882cb2c9c"
      },
      "source": [
        "!python encode.py outnospace.txt allinone1.npz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading files\n",
            "\r  0% 0/1 [00:00<?, ?it/s]tcmalloc: large alloc 1248935936 bytes == 0x55d282f82000 @  0x7fc360c881e7 0x55d23343fe68 0x55d233456e05 0x55d233456b88 0x55d23346407f 0x55d23345569b 0x55d2334e1f33 0x55d23340db59 0x55d23340da50 0x55d233481be0 0x55d23347c4ae 0x55d23340fa81 0x55d23340ef30 0x55d233410ee8 0x55d2334ed4f1 0x55d23358ccc1 0x55d23340db59 0x55d2334fefed 0x55d233481988 0x55d23347c4ae 0x55d23340f3ea 0x55d23347e32a 0x55d23340f30a 0x55d23347d3b5 0x55d23347c4ae 0x55d23347c1b3 0x55d233546182 0x55d2335464fd 0x55d2335463a6 0x55d23351d723 0x55d23351d3cc\n",
            "tcmalloc: large alloc 1233903616 bytes == 0x55d2fce4c000 @  0x7fc360c89615 0x55d233419afa 0x55d23340dafd 0x55d2334fefed 0x55d233481988 0x55d23347c7ad 0x55d23340f3ea 0x55d23347d60e 0x55d23347c4ae 0x55d23340f3ea 0x55d23347e32a 0x55d23340f30a 0x55d23347d3b5 0x55d23347c4ae 0x55d23347c1b3 0x55d233546182 0x55d2335464fd 0x55d2335463a6 0x55d23351d723 0x55d23351d3cc 0x7fc35fa72bf7 0x55d23351d2aa\n",
            "tcmalloc: large alloc 1542381568 bytes == 0x55d34670a000 @  0x7fc360c89615 0x55d23352743d 0x55d233419c44 0x55d23340dafd 0x55d2334fefed 0x55d233481988 0x55d23347c7ad 0x55d23340f3ea 0x55d23347d60e 0x55d23347c4ae 0x55d23340f3ea 0x55d23347e32a 0x55d23340f30a 0x55d23347d3b5 0x55d23347c4ae 0x55d23347c1b3 0x55d233546182 0x55d2335464fd 0x55d2335463a6 0x55d23351d723 0x55d23351d3cc 0x7fc35fa72bf7 0x55d23351d2aa\n",
            "tcmalloc: large alloc 1927979008 bytes == 0x55d2c02e4000 @  0x7fc360c89615 0x55d233419afa 0x55d23340dafd 0x55d2334fefed 0x55d233481988 0x55d23347c7ad 0x55d23340f3ea 0x55d23347d60e 0x55d23347c4ae 0x55d23340f3ea 0x55d23347e32a 0x55d23340f30a 0x55d23347d3b5 0x55d23347c4ae 0x55d23347c1b3 0x55d233546182 0x55d2335464fd 0x55d2335463a6 0x55d23351d723 0x55d23351d3cc 0x7fc35fa72bf7 0x55d23351d2aa\n",
            "^C\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWEYqRu-ast4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aebc7699-c46f-405c-abfc-fbffad4a5aea"
      },
      "source": [
        "!python train.py --dataset out4.npz --batch_size 1 --learning_rate 0.00001"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mПоказано результат, скорочений до останніх рядків (5000).\u001b[0m\n",
            "[495281 | 8771.68] loss=1.17 avg=0.93\n",
            "[495282 | 8772.56] loss=1.14 avg=0.94\n",
            "[495283 | 8773.44] loss=1.00 avg=0.94\n",
            "[495284 | 8774.31] loss=0.81 avg=0.93\n",
            "[495285 | 8775.19] loss=0.93 avg=0.93\n",
            "[495286 | 8776.06] loss=0.78 avg=0.93\n",
            "[495287 | 8776.93] loss=1.13 avg=0.93\n",
            "[495288 | 8777.81] loss=0.91 avg=0.93\n",
            "[495289 | 8778.68] loss=0.91 avg=0.93\n",
            "[495290 | 8779.56] loss=0.74 avg=0.93\n",
            "[495291 | 8780.43] loss=1.00 avg=0.93\n",
            "[495292 | 8781.31] loss=0.85 avg=0.93\n",
            "[495293 | 8782.18] loss=0.97 avg=0.93\n",
            "[495294 | 8783.06] loss=0.98 avg=0.93\n",
            "[495295 | 8783.94] loss=0.97 avg=0.93\n",
            "[495296 | 8784.81] loss=1.16 avg=0.94\n",
            "[495297 | 8785.69] loss=1.28 avg=0.94\n",
            "[495298 | 8786.56] loss=0.82 avg=0.94\n",
            "[495299 | 8787.44] loss=0.92 avg=0.94\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "і. Між різними, торговельними операціями відбивали європейський прапор в Мансурарі (Моріам), Ткачуна (Січей), Марина, Розума (Росса), Мердама, Руйні, Рибала, Сантрапі, Савана, Сорона, Тева, Приосній спадщині торпедиції Маратів перебували там з усвідомлень величезного мирного впливу на видатки цього розмаху. Разом із голодом відбувалися військові дії, зрештою спростував отруту частини кримінального захисту. Через відсутність потреб Марата, чим більшої сутності захищали захисту частково лише одне потічки.\n",
            "Хоча Марата жінкою виростила потреба в різних топонімах релігії, вона в захисті часто проводила збільшення її задовольняності в ім'я Шайла в безпосередньому стані. Марата, Марата того ж дня, назвав свою ним «біля Поля». «Ворожнече», тупика на лід арабського злета й біля кінохронічної інтерпретації в якому можна було корені Самійло Рибковскі (1903—1911). Марата, Мердама, як і його спогади. Оста\n",
            "\n",
            "[495300 | 8805.52] loss=0.80 avg=0.94\n",
            "[495301 | 8806.39] loss=0.77 avg=0.93\n",
            "[495302 | 8807.28] loss=1.08 avg=0.94\n",
            "[495303 | 8808.15] loss=0.91 avg=0.94\n",
            "[495304 | 8809.03] loss=0.83 avg=0.93\n",
            "[495305 | 8809.91] loss=0.88 avg=0.93\n",
            "[495306 | 8810.78] loss=1.28 avg=0.94\n",
            "[495307 | 8811.66] loss=0.95 avg=0.94\n",
            "[495308 | 8812.53] loss=1.05 avg=0.94\n",
            "[495309 | 8813.41] loss=0.89 avg=0.94\n",
            "[495310 | 8814.29] loss=1.10 avg=0.94\n",
            "[495311 | 8815.17] loss=0.95 avg=0.94\n",
            "[495312 | 8816.05] loss=1.04 avg=0.94\n",
            "[495313 | 8816.93] loss=0.91 avg=0.94\n",
            "[495314 | 8817.79] loss=0.78 avg=0.94\n",
            "[495315 | 8818.68] loss=0.82 avg=0.94\n",
            "[495316 | 8819.55] loss=0.90 avg=0.94\n",
            "[495317 | 8820.43] loss=0.80 avg=0.94\n",
            "[495318 | 8821.30] loss=0.88 avg=0.94\n",
            "[495319 | 8822.19] loss=1.06 avg=0.94\n",
            "[495320 | 8823.06] loss=0.88 avg=0.94\n",
            "[495321 | 8823.93] loss=0.95 avg=0.94\n",
            "[495322 | 8824.81] loss=0.95 avg=0.94\n",
            "[495323 | 8825.69] loss=1.00 avg=0.94\n",
            "[495324 | 8826.57] loss=0.83 avg=0.94\n",
            "[495325 | 8827.44] loss=1.12 avg=0.94\n",
            "[495326 | 8828.32] loss=1.04 avg=0.94\n",
            "[495327 | 8829.19] loss=0.73 avg=0.94\n",
            "[495328 | 8830.07] loss=0.87 avg=0.94\n",
            "[495329 | 8830.94] loss=1.02 avg=0.94\n",
            "[495330 | 8831.82] loss=0.99 avg=0.94\n",
            "[495331 | 8832.70] loss=0.99 avg=0.94\n",
            "[495332 | 8833.57] loss=0.92 avg=0.94\n",
            "[495333 | 8834.45] loss=0.69 avg=0.94\n",
            "[495334 | 8835.32] loss=0.93 avg=0.94\n",
            "[495335 | 8836.20] loss=0.86 avg=0.93\n",
            "[495336 | 8837.07] loss=0.85 avg=0.93\n",
            "[495337 | 8837.95] loss=1.08 avg=0.94\n",
            "[495338 | 8838.82] loss=0.95 avg=0.94\n",
            "[495339 | 8839.69] loss=0.82 avg=0.93\n",
            "[495340 | 8840.57] loss=0.88 avg=0.93\n",
            "[495341 | 8841.44] loss=1.01 avg=0.93\n",
            "[495342 | 8842.31] loss=1.04 avg=0.94\n",
            "[495343 | 8843.19] loss=0.92 avg=0.94\n",
            "[495344 | 8844.06] loss=1.02 avg=0.94\n",
            "[495345 | 8844.94] loss=0.80 avg=0.93\n",
            "[495346 | 8845.81] loss=0.90 avg=0.93\n",
            "[495347 | 8846.69] loss=1.25 avg=0.94\n",
            "[495348 | 8847.56] loss=0.89 avg=0.94\n",
            "[495349 | 8848.43] loss=0.90 avg=0.94\n",
            "[495350 | 8849.30] loss=0.92 avg=0.94\n",
            "[495351 | 8850.18] loss=0.84 avg=0.94\n",
            "[495352 | 8851.05] loss=0.84 avg=0.93\n",
            "[495353 | 8851.93] loss=0.96 avg=0.94\n",
            "[495354 | 8852.80] loss=0.94 avg=0.94\n",
            "[495355 | 8853.68] loss=0.99 avg=0.94\n",
            "[495356 | 8854.55] loss=1.00 avg=0.94\n",
            "[495357 | 8855.43] loss=0.95 avg=0.94\n",
            "[495358 | 8856.31] loss=1.21 avg=0.94\n",
            "[495359 | 8857.18] loss=0.99 avg=0.94\n",
            "[495360 | 8858.05] loss=0.73 avg=0.94\n",
            "[495361 | 8858.93] loss=1.16 avg=0.94\n",
            "[495362 | 8859.81] loss=0.75 avg=0.94\n",
            "[495363 | 8860.68] loss=1.20 avg=0.94\n",
            "[495364 | 8861.56] loss=0.79 avg=0.94\n",
            "[495365 | 8862.44] loss=0.93 avg=0.94\n",
            "[495366 | 8863.31] loss=0.95 avg=0.94\n",
            "[495367 | 8864.19] loss=1.05 avg=0.94\n",
            "[495368 | 8865.06] loss=0.82 avg=0.94\n",
            "[495369 | 8865.94] loss=0.82 avg=0.94\n",
            "[495370 | 8866.81] loss=0.81 avg=0.94\n",
            "[495371 | 8867.68] loss=0.94 avg=0.94\n",
            "[495372 | 8868.55] loss=1.13 avg=0.94\n",
            "[495373 | 8869.42] loss=0.87 avg=0.94\n",
            "[495374 | 8870.31] loss=0.78 avg=0.94\n",
            "[495375 | 8871.18] loss=0.88 avg=0.94\n",
            "[495376 | 8872.04] loss=1.02 avg=0.94\n",
            "[495377 | 8872.92] loss=0.93 avg=0.94\n",
            "[495378 | 8873.79] loss=1.06 avg=0.94\n",
            "[495379 | 8874.67] loss=1.05 avg=0.94\n",
            "[495380 | 8875.54] loss=0.85 avg=0.94\n",
            "[495381 | 8876.42] loss=0.31 avg=0.93\n",
            "[495382 | 8877.30] loss=0.87 avg=0.93\n",
            "[495383 | 8878.17] loss=0.91 avg=0.93\n",
            "[495384 | 8879.05] loss=0.93 avg=0.93\n",
            "[495385 | 8879.91] loss=0.84 avg=0.93\n",
            "[495386 | 8880.78] loss=0.97 avg=0.93\n",
            "[495387 | 8881.66] loss=0.22 avg=0.92\n",
            "[495388 | 8882.53] loss=0.71 avg=0.92\n",
            "[495389 | 8883.39] loss=1.01 avg=0.92\n",
            "[495390 | 8884.28] loss=1.07 avg=0.92\n",
            "[495391 | 8885.15] loss=1.02 avg=0.92\n",
            "[495392 | 8886.03] loss=0.93 avg=0.92\n",
            "[495393 | 8886.90] loss=0.78 avg=0.92\n",
            "[495394 | 8887.77] loss=1.38 avg=0.93\n",
            "[495395 | 8888.65] loss=0.78 avg=0.93\n",
            "[495396 | 8889.52] loss=0.94 avg=0.93\n",
            "[495397 | 8890.39] loss=0.85 avg=0.93\n",
            "[495398 | 8891.27] loss=0.89 avg=0.93\n",
            "[495399 | 8892.14] loss=0.79 avg=0.92\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "орективними бульбаобськими та емоційними групами СА (муркулазд) при воді основних рез'їденого бульбашовими та різні форми, незабаром проводилися бортові та непрокляті надзвуково-кременеві брати-фортеци мистецтва і мистецтва чародіння з наслідування основного та топографічного біогенових конфліктів. Конфесійні кремени, як і інші видатні витрати�, характерні для вивчення у інших умовах (початково-суспільних, виробничих та холодильних засобів, антигерних карток, деякі художники, аспекти, списанків та емоцій і антропогенної області). Галося також збагачення «Кременецькою народною музикою та районних спільнот».\n",
            "В конфесійному центрі теорії одним з мов для мовлення від імені філософа та поезії «Загородна категорія». Ввечері до XIX століття в одній з середінки її довгострокових виплат: «Переднє являє собою бажано м'яка близько 8 млн рупіхів, що науковим центром був вимовлений на задньому п\n",
            "\n",
            "[495400 | 8910.27] loss=1.07 avg=0.93\n",
            "[495401 | 8911.14] loss=0.65 avg=0.92\n",
            "[495402 | 8912.01] loss=1.07 avg=0.92\n",
            "[495403 | 8912.89] loss=1.06 avg=0.93\n",
            "[495404 | 8913.76] loss=1.04 avg=0.93\n",
            "[495405 | 8914.63] loss=1.00 avg=0.93\n",
            "[495406 | 8915.51] loss=0.92 avg=0.93\n",
            "[495407 | 8916.38] loss=0.86 avg=0.93\n",
            "[495408 | 8917.25] loss=0.93 avg=0.93\n",
            "[495409 | 8918.12] loss=0.84 avg=0.93\n",
            "[495410 | 8919.00] loss=0.95 avg=0.93\n",
            "[495411 | 8919.87] loss=1.16 avg=0.93\n",
            "[495412 | 8920.74] loss=0.80 avg=0.93\n",
            "[495413 | 8921.62] loss=0.93 avg=0.93\n",
            "[495414 | 8922.50] loss=0.91 avg=0.93\n",
            "[495415 | 8923.37] loss=0.98 avg=0.93\n",
            "[495416 | 8924.25] loss=1.19 avg=0.93\n",
            "[495417 | 8925.12] loss=0.81 avg=0.93\n",
            "[495418 | 8926.00] loss=0.99 avg=0.93\n",
            "[495419 | 8926.87] loss=0.81 avg=0.93\n",
            "[495420 | 8927.75] loss=0.84 avg=0.93\n",
            "[495421 | 8928.63] loss=1.12 avg=0.93\n",
            "[495422 | 8929.50] loss=0.84 avg=0.93\n",
            "[495423 | 8930.38] loss=0.94 avg=0.93\n",
            "[495424 | 8931.25] loss=0.82 avg=0.93\n",
            "[495425 | 8932.12] loss=0.90 avg=0.93\n",
            "[495426 | 8932.99] loss=1.14 avg=0.93\n",
            "[495427 | 8933.87] loss=1.02 avg=0.93\n",
            "[495428 | 8934.74] loss=0.88 avg=0.93\n",
            "[495429 | 8935.61] loss=1.01 avg=0.93\n",
            "[495430 | 8936.49] loss=1.17 avg=0.93\n",
            "[495431 | 8937.36] loss=1.08 avg=0.93\n",
            "[495432 | 8938.23] loss=0.93 avg=0.93\n",
            "[495433 | 8939.11] loss=0.81 avg=0.93\n",
            "[495434 | 8939.98] loss=1.01 avg=0.93\n",
            "[495435 | 8940.86] loss=0.76 avg=0.93\n",
            "[495436 | 8941.73] loss=0.87 avg=0.93\n",
            "[495437 | 8942.60] loss=1.16 avg=0.93\n",
            "[495438 | 8943.48] loss=0.80 avg=0.93\n",
            "[495439 | 8944.35] loss=1.10 avg=0.93\n",
            "[495440 | 8945.23] loss=0.84 avg=0.93\n",
            "[495441 | 8946.10] loss=1.21 avg=0.94\n",
            "[495442 | 8946.98] loss=0.78 avg=0.93\n",
            "[495443 | 8947.85] loss=0.71 avg=0.93\n",
            "[495444 | 8948.72] loss=0.77 avg=0.93\n",
            "[495445 | 8949.60] loss=0.91 avg=0.93\n",
            "[495446 | 8950.47] loss=0.87 avg=0.93\n",
            "[495447 | 8951.34] loss=0.88 avg=0.93\n",
            "[495448 | 8952.22] loss=1.04 avg=0.93\n",
            "[495449 | 8953.08] loss=1.15 avg=0.93\n",
            "[495450 | 8953.96] loss=0.76 avg=0.93\n",
            "[495451 | 8954.84] loss=0.79 avg=0.93\n",
            "[495452 | 8955.71] loss=0.96 avg=0.93\n",
            "[495453 | 8956.58] loss=1.06 avg=0.93\n",
            "[495454 | 8957.45] loss=0.95 avg=0.93\n",
            "[495455 | 8958.33] loss=0.98 avg=0.93\n",
            "[495456 | 8959.20] loss=0.93 avg=0.93\n",
            "[495457 | 8960.08] loss=1.02 avg=0.93\n",
            "[495458 | 8960.95] loss=0.93 avg=0.93\n",
            "[495459 | 8961.82] loss=0.88 avg=0.93\n",
            "[495460 | 8962.70] loss=0.75 avg=0.93\n",
            "[495461 | 8963.57] loss=0.86 avg=0.93\n",
            "[495462 | 8964.45] loss=0.94 avg=0.93\n",
            "[495463 | 8965.33] loss=0.93 avg=0.93\n",
            "[495464 | 8966.20] loss=1.03 avg=0.93\n",
            "[495465 | 8967.08] loss=0.75 avg=0.93\n",
            "[495466 | 8967.95] loss=0.83 avg=0.93\n",
            "[495467 | 8968.82] loss=0.98 avg=0.93\n",
            "[495468 | 8969.70] loss=0.89 avg=0.93\n",
            "[495469 | 8970.57] loss=1.06 avg=0.93\n",
            "[495470 | 8971.44] loss=0.99 avg=0.93\n",
            "[495471 | 8972.32] loss=0.92 avg=0.93\n",
            "[495472 | 8973.20] loss=0.86 avg=0.93\n",
            "[495473 | 8974.07] loss=0.88 avg=0.93\n",
            "[495474 | 8974.94] loss=1.02 avg=0.93\n",
            "[495475 | 8975.82] loss=0.95 avg=0.93\n",
            "[495476 | 8976.69] loss=0.96 avg=0.93\n",
            "[495477 | 8977.57] loss=1.04 avg=0.93\n",
            "[495478 | 8978.44] loss=0.76 avg=0.93\n",
            "[495479 | 8979.32] loss=1.07 avg=0.93\n",
            "[495480 | 8980.19] loss=1.01 avg=0.93\n",
            "[495481 | 8981.07] loss=0.89 avg=0.93\n",
            "[495482 | 8981.94] loss=0.93 avg=0.93\n",
            "[495483 | 8982.81] loss=0.90 avg=0.93\n",
            "[495484 | 8983.69] loss=0.98 avg=0.93\n",
            "[495485 | 8984.56] loss=0.91 avg=0.93\n",
            "[495486 | 8985.43] loss=0.98 avg=0.93\n",
            "[495487 | 8986.31] loss=1.13 avg=0.93\n",
            "[495488 | 8987.18] loss=1.00 avg=0.93\n",
            "[495489 | 8988.06] loss=0.98 avg=0.93\n",
            "[495490 | 8988.93] loss=1.00 avg=0.94\n",
            "[495491 | 8989.81] loss=0.80 avg=0.93\n",
            "[495492 | 8990.69] loss=0.75 avg=0.93\n",
            "[495493 | 8991.56] loss=0.91 avg=0.93\n",
            "[495494 | 8992.44] loss=0.83 avg=0.93\n",
            "[495495 | 8993.32] loss=0.96 avg=0.93\n",
            "[495496 | 8994.19] loss=0.86 avg=0.93\n",
            "[495497 | 8995.06] loss=0.87 avg=0.93\n",
            "[495498 | 8995.94] loss=1.05 avg=0.93\n",
            "[495499 | 8996.81] loss=1.31 avg=0.93\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "�ісяця, до македонського фільварку. Якість героїв у військах підтримала бідняки Хантай. Але він продовжує уособи допомоги Пікані (покинув своє імператорське військо, Марк зведений англійськими королями урядовців, членами якого були Олтарж та Ной полків, і Нео і Станіслав, підтверджує важливу роль македонського європейського призначення у льодовику. Деякі пакети англійського війська зазнали поразки, де був установлений тим часом більше наземним було обов'язковим. Більшість грецьких повинні схилялись до битви навернених осіб, Саймонів зазнавши сутички з лицевом. Рештою пакетів було затримано в районі Св. Залізниця, де у місті заохочувалось списки, виступало чільне керівництво, конжи до шиєфу, яке повинно було встановити друк. Тому кожна людина, разом з Слісом, але Саймонів у друк увійшла від Сліса до Ной угруповання. Завдяки його все з'ясовувались, що ризьке друку дійшло у річки Гуджвудіт Єленін (\n",
            "\n",
            "[495500 | 9014.90] loss=1.05 avg=0.94\n",
            "[495501 | 9015.77] loss=0.86 avg=0.94\n",
            "[495502 | 9016.64] loss=1.26 avg=0.94\n",
            "[495503 | 9017.52] loss=0.96 avg=0.94\n",
            "[495504 | 9018.39] loss=0.80 avg=0.94\n",
            "[495505 | 9019.26] loss=0.87 avg=0.94\n",
            "[495506 | 9020.14] loss=1.09 avg=0.94\n",
            "[495507 | 9021.01] loss=0.85 avg=0.94\n",
            "[495508 | 9021.89] loss=0.82 avg=0.94\n",
            "[495509 | 9022.77] loss=0.93 avg=0.94\n",
            "[495510 | 9023.64] loss=1.04 avg=0.94\n",
            "[495511 | 9024.52] loss=0.87 avg=0.94\n",
            "[495512 | 9025.39] loss=1.15 avg=0.94\n",
            "[495513 | 9026.28] loss=0.92 avg=0.94\n",
            "[495514 | 9027.15] loss=0.83 avg=0.94\n",
            "[495515 | 9028.03] loss=0.98 avg=0.94\n",
            "[495516 | 9028.90] loss=1.04 avg=0.94\n",
            "[495517 | 9029.78] loss=0.82 avg=0.94\n",
            "[495518 | 9030.65] loss=0.70 avg=0.93\n",
            "[495519 | 9031.52] loss=0.87 avg=0.93\n",
            "[495520 | 9032.41] loss=0.83 avg=0.93\n",
            "[495521 | 9033.28] loss=0.97 avg=0.93\n",
            "[495522 | 9034.16] loss=0.67 avg=0.93\n",
            "[495523 | 9035.03] loss=0.90 avg=0.93\n",
            "[495524 | 9035.90] loss=0.80 avg=0.93\n",
            "[495525 | 9036.78] loss=1.03 avg=0.93\n",
            "[495526 | 9037.65] loss=0.76 avg=0.93\n",
            "[495527 | 9038.53] loss=0.88 avg=0.93\n",
            "[495528 | 9039.40] loss=0.95 avg=0.93\n",
            "[495529 | 9040.28] loss=1.00 avg=0.93\n",
            "[495530 | 9041.15] loss=0.94 avg=0.93\n",
            "[495531 | 9042.02] loss=1.01 avg=0.93\n",
            "[495532 | 9042.90] loss=1.34 avg=0.93\n",
            "[495533 | 9043.77] loss=0.85 avg=0.93\n",
            "[495534 | 9044.64] loss=0.72 avg=0.93\n",
            "[495535 | 9045.52] loss=0.95 avg=0.93\n",
            "[495536 | 9046.40] loss=1.00 avg=0.93\n",
            "[495537 | 9047.27] loss=0.75 avg=0.93\n",
            "[495538 | 9048.15] loss=1.21 avg=0.93\n",
            "[495539 | 9049.02] loss=0.92 avg=0.93\n",
            "[495540 | 9049.89] loss=0.98 avg=0.93\n",
            "[495541 | 9050.77] loss=0.98 avg=0.93\n",
            "[495542 | 9051.64] loss=0.96 avg=0.93\n",
            "[495543 | 9052.51] loss=1.03 avg=0.94\n",
            "[495544 | 9053.39] loss=0.77 avg=0.93\n",
            "[495545 | 9054.27] loss=0.91 avg=0.93\n",
            "[495546 | 9055.14] loss=0.85 avg=0.93\n",
            "[495547 | 9056.02] loss=0.96 avg=0.93\n",
            "[495548 | 9056.89] loss=1.61 avg=0.94\n",
            "[495549 | 9057.77] loss=0.94 avg=0.94\n",
            "[495550 | 9058.64] loss=0.75 avg=0.94\n",
            "[495551 | 9059.51] loss=0.99 avg=0.94\n",
            "[495552 | 9060.38] loss=0.92 avg=0.94\n",
            "[495553 | 9061.25] loss=1.00 avg=0.94\n",
            "[495554 | 9062.13] loss=1.00 avg=0.94\n",
            "[495555 | 9063.00] loss=0.90 avg=0.94\n",
            "[495556 | 9063.87] loss=0.89 avg=0.94\n",
            "[495557 | 9064.75] loss=1.02 avg=0.94\n",
            "[495558 | 9065.63] loss=0.76 avg=0.94\n",
            "[495559 | 9066.50] loss=1.04 avg=0.94\n",
            "[495560 | 9067.37] loss=1.04 avg=0.94\n",
            "[495561 | 9068.25] loss=1.00 avg=0.94\n",
            "[495562 | 9069.12] loss=0.95 avg=0.94\n",
            "[495563 | 9069.99] loss=0.87 avg=0.94\n",
            "[495564 | 9070.87] loss=0.71 avg=0.94\n",
            "[495565 | 9071.74] loss=0.89 avg=0.94\n",
            "[495566 | 9072.62] loss=0.91 avg=0.94\n",
            "[495567 | 9073.49] loss=1.00 avg=0.94\n",
            "[495568 | 9074.37] loss=1.05 avg=0.94\n",
            "[495569 | 9075.24] loss=0.90 avg=0.94\n",
            "[495570 | 9076.11] loss=0.85 avg=0.94\n",
            "[495571 | 9077.00] loss=0.87 avg=0.94\n",
            "[495572 | 9077.87] loss=0.82 avg=0.94\n",
            "[495573 | 9078.74] loss=0.94 avg=0.94\n",
            "[495574 | 9079.62] loss=0.80 avg=0.93\n",
            "[495575 | 9080.49] loss=0.87 avg=0.93\n",
            "[495576 | 9081.36] loss=0.86 avg=0.93\n",
            "[495577 | 9082.23] loss=0.99 avg=0.93\n",
            "[495578 | 9083.11] loss=1.15 avg=0.94\n",
            "[495579 | 9083.99] loss=1.09 avg=0.94\n",
            "[495580 | 9084.87] loss=0.98 avg=0.94\n",
            "[495581 | 9085.74] loss=0.90 avg=0.94\n",
            "[495582 | 9086.61] loss=0.78 avg=0.94\n",
            "[495583 | 9087.49] loss=1.06 avg=0.94\n",
            "[495584 | 9088.36] loss=0.62 avg=0.93\n",
            "[495585 | 9089.23] loss=0.86 avg=0.93\n",
            "[495586 | 9090.10] loss=0.92 avg=0.93\n",
            "[495587 | 9090.97] loss=0.94 avg=0.93\n",
            "[495588 | 9091.85] loss=0.78 avg=0.93\n",
            "[495589 | 9092.73] loss=1.02 avg=0.93\n",
            "[495590 | 9093.60] loss=0.82 avg=0.93\n",
            "[495591 | 9094.48] loss=0.79 avg=0.93\n",
            "[495592 | 9095.35] loss=0.94 avg=0.93\n",
            "[495593 | 9096.23] loss=0.95 avg=0.93\n",
            "[495594 | 9097.10] loss=0.91 avg=0.93\n",
            "[495595 | 9097.97] loss=0.87 avg=0.93\n",
            "[495596 | 9098.85] loss=0.97 avg=0.93\n",
            "[495597 | 9099.72] loss=0.94 avg=0.93\n",
            "[495598 | 9100.61] loss=1.06 avg=0.93\n",
            "[495599 | 9101.48] loss=0.90 avg=0.93\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "ого на основі рівнозначного нанесення персонажів, а потім, незважаючи на всю свою достовірність полегшувати своє бачення у вигляді процесу; ті, хто втратив звичання рівнозначних проявів для того, щоб блокувати життя та гроші у ранньому секторі та не було ефективно) та створити певні поливні симптоми, які були встановлені у своїх видах, нанесення на звичайні прилади до розвитку тальки, вимірюванню самого його сектора. Об'єкти американських компаній почали встановлювати антиграфічні поливні та елементи (за фазами, радісному прочитанню, художньої чуттєвої), хоча й поєднувати перевагу потужним клієнтовим станам бачити таким чином існування лесбійок, як збагачення і гартування того, чи цього досі, свідчення про те, скромності і прояві життя особою обліткам цього прояву допомагають авторитивним життєвим способам, що дещо відносять його сюжетні стани, в культурних способах їх обурювання, і звичая так\n",
            "\n",
            "[495600 | 9119.60] loss=1.02 avg=0.93\n",
            "[495601 | 9120.48] loss=0.94 avg=0.93\n",
            "[495602 | 9121.35] loss=0.76 avg=0.93\n",
            "[495603 | 9122.23] loss=0.98 avg=0.93\n",
            "[495604 | 9123.10] loss=0.83 avg=0.93\n",
            "[495605 | 9123.98] loss=0.74 avg=0.93\n",
            "[495606 | 9124.85] loss=1.07 avg=0.93\n",
            "[495607 | 9125.72] loss=0.85 avg=0.93\n",
            "[495608 | 9126.60] loss=0.96 avg=0.93\n",
            "[495609 | 9127.47] loss=0.87 avg=0.93\n",
            "[495610 | 9128.35] loss=0.92 avg=0.93\n",
            "[495611 | 9129.22] loss=1.03 avg=0.93\n",
            "[495612 | 9130.10] loss=0.75 avg=0.93\n",
            "[495613 | 9130.98] loss=1.02 avg=0.93\n",
            "[495614 | 9131.85] loss=0.83 avg=0.93\n",
            "[495615 | 9132.73] loss=0.92 avg=0.93\n",
            "[495616 | 9133.61] loss=0.93 avg=0.93\n",
            "[495617 | 9134.49] loss=1.08 avg=0.93\n",
            "[495618 | 9135.36] loss=1.08 avg=0.93\n",
            "[495619 | 9136.25] loss=0.77 avg=0.93\n",
            "[495620 | 9137.12] loss=1.01 avg=0.93\n",
            "[495621 | 9138.00] loss=0.94 avg=0.93\n",
            "[495622 | 9138.87] loss=0.91 avg=0.93\n",
            "[495623 | 9139.74] loss=1.07 avg=0.93\n",
            "[495624 | 9140.62] loss=0.95 avg=0.93\n",
            "[495625 | 9141.49] loss=0.93 avg=0.93\n",
            "[495626 | 9142.37] loss=1.08 avg=0.93\n",
            "[495627 | 9143.24] loss=0.95 avg=0.93\n",
            "[495628 | 9144.11] loss=0.94 avg=0.93\n",
            "[495629 | 9144.99] loss=0.88 avg=0.93\n",
            "[495630 | 9145.86] loss=0.92 avg=0.93\n",
            "[495631 | 9146.74] loss=0.74 avg=0.93\n",
            "[495632 | 9147.61] loss=0.83 avg=0.93\n",
            "[495633 | 9148.48] loss=1.09 avg=0.93\n",
            "[495634 | 9149.36] loss=1.07 avg=0.93\n",
            "[495635 | 9150.23] loss=1.07 avg=0.93\n",
            "[495636 | 9151.10] loss=1.16 avg=0.94\n",
            "[495637 | 9151.98] loss=1.02 avg=0.94\n",
            "[495638 | 9152.85] loss=0.85 avg=0.94\n",
            "[495639 | 9153.73] loss=0.87 avg=0.93\n",
            "[495640 | 9154.61] loss=1.08 avg=0.94\n",
            "[495641 | 9155.49] loss=0.89 avg=0.94\n",
            "[495642 | 9156.36] loss=0.96 avg=0.94\n",
            "[495643 | 9157.24] loss=0.68 avg=0.93\n",
            "[495644 | 9158.11] loss=0.63 avg=0.93\n",
            "[495645 | 9158.99] loss=0.82 avg=0.93\n",
            "[495646 | 9159.86] loss=0.92 avg=0.93\n",
            "[495647 | 9160.74] loss=1.28 avg=0.93\n",
            "[495648 | 9161.61] loss=0.78 avg=0.93\n",
            "[495649 | 9162.48] loss=1.04 avg=0.93\n",
            "[495650 | 9163.36] loss=1.01 avg=0.93\n",
            "[495651 | 9164.23] loss=0.90 avg=0.93\n",
            "[495652 | 9165.12] loss=0.85 avg=0.93\n",
            "[495653 | 9165.99] loss=0.93 avg=0.93\n",
            "[495654 | 9166.87] loss=1.09 avg=0.93\n",
            "[495655 | 9167.75] loss=0.86 avg=0.93\n",
            "[495656 | 9168.62] loss=1.11 avg=0.93\n",
            "[495657 | 9169.50] loss=1.20 avg=0.94\n",
            "[495658 | 9170.37] loss=0.89 avg=0.94\n",
            "[495659 | 9171.25] loss=0.91 avg=0.94\n",
            "[495660 | 9172.12] loss=0.98 avg=0.94\n",
            "[495661 | 9172.99] loss=1.07 avg=0.94\n",
            "[495662 | 9173.87] loss=0.94 avg=0.94\n",
            "[495663 | 9174.74] loss=1.08 avg=0.94\n",
            "[495664 | 9175.62] loss=0.76 avg=0.94\n",
            "[495665 | 9176.49] loss=0.66 avg=0.93\n",
            "[495666 | 9177.38] loss=1.24 avg=0.94\n",
            "[495667 | 9178.24] loss=0.89 avg=0.94\n",
            "[495668 | 9179.11] loss=0.94 avg=0.94\n",
            "[495669 | 9179.99] loss=0.80 avg=0.94\n",
            "[495670 | 9180.86] loss=1.02 avg=0.94\n",
            "[495671 | 9181.75] loss=0.88 avg=0.94\n",
            "[495672 | 9182.62] loss=0.98 avg=0.94\n",
            "[495673 | 9183.50] loss=0.85 avg=0.94\n",
            "[495674 | 9184.38] loss=0.88 avg=0.94\n",
            "[495675 | 9185.25] loss=0.95 avg=0.94\n",
            "[495676 | 9186.13] loss=0.90 avg=0.94\n",
            "[495677 | 9187.01] loss=0.95 avg=0.94\n",
            "[495678 | 9187.89] loss=1.10 avg=0.94\n",
            "[495679 | 9188.76] loss=1.02 avg=0.94\n",
            "[495680 | 9189.64] loss=0.77 avg=0.94\n",
            "[495681 | 9190.51] loss=1.04 avg=0.94\n",
            "[495682 | 9191.38] loss=0.70 avg=0.93\n",
            "[495683 | 9192.25] loss=0.78 avg=0.93\n",
            "[495684 | 9193.13] loss=0.88 avg=0.93\n",
            "[495685 | 9194.01] loss=0.75 avg=0.93\n",
            "[495686 | 9194.88] loss=0.88 avg=0.93\n",
            "[495687 | 9195.75] loss=0.60 avg=0.93\n",
            "[495688 | 9196.63] loss=0.79 avg=0.93\n",
            "[495689 | 9197.51] loss=0.85 avg=0.92\n",
            "[495690 | 9198.39] loss=0.99 avg=0.93\n",
            "[495691 | 9199.26] loss=0.88 avg=0.93\n",
            "[495692 | 9200.14] loss=0.97 avg=0.93\n",
            "[495693 | 9201.01] loss=0.92 avg=0.93\n",
            "[495694 | 9201.89] loss=0.74 avg=0.92\n",
            "[495695 | 9202.76] loss=0.81 avg=0.92\n",
            "[495696 | 9203.65] loss=0.94 avg=0.92\n",
            "[495697 | 9204.52] loss=1.09 avg=0.92\n",
            "[495698 | 9205.39] loss=0.76 avg=0.92\n",
            "[495699 | 9206.27] loss=1.06 avg=0.92\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "ших частин своїх дітей були введені 5 тижнів, рекордно 5 — за об їх містичних належать жінок та громадськість. У ніше з перших християн, не мали свого часу, були тільки білогвардійцями-чоловіками-чоловіками-жінкамичами: за життя знала про жіночі та зі світом навколо національного класу, про протягом усього року було укладено близько 900 осіб, а завоювання — про решту жінок, а також про міркування та обмундирування, що на початку ХХ сторіччя італійська інтелігенція. Будівля жінок, яка закінчувалася, була знята з особливої підстави добиратися перед часів заходу заруху.\n",
            "З огляду на участь у профілактики і розважальних джерел хлопців належало широкому відомому у пригодництві. Нові експериментування були регіональною декількома завданням. У 1950-х роках на участь війни у світі побували джерело на шпиталях.\n",
            "Ступінь італійців і жінок мала великий експонувальний досвід: повна кількість ураження, сн�\n",
            "\n",
            "[495700 | 9224.43] loss=0.99 avg=0.92\n",
            "[495701 | 9225.30] loss=1.18 avg=0.93\n",
            "[495702 | 9226.18] loss=0.72 avg=0.93\n",
            "[495703 | 9227.05] loss=0.96 avg=0.93\n",
            "[495704 | 9227.92] loss=1.13 avg=0.93\n",
            "[495705 | 9228.79] loss=0.87 avg=0.93\n",
            "[495706 | 9229.67] loss=1.08 avg=0.93\n",
            "[495707 | 9230.55] loss=0.86 avg=0.93\n",
            "[495708 | 9231.42] loss=0.97 avg=0.93\n",
            "[495709 | 9232.28] loss=0.98 avg=0.93\n",
            "[495710 | 9233.16] loss=1.21 avg=0.93\n",
            "[495711 | 9234.04] loss=0.89 avg=0.93\n",
            "[495712 | 9234.92] loss=0.79 avg=0.93\n",
            "[495713 | 9235.79] loss=0.77 avg=0.93\n",
            "[495714 | 9236.67] loss=0.92 avg=0.93\n",
            "[495715 | 9237.55] loss=0.98 avg=0.93\n",
            "[495716 | 9238.42] loss=1.06 avg=0.93\n",
            "[495717 | 9239.30] loss=0.85 avg=0.93\n",
            "[495718 | 9240.17] loss=0.95 avg=0.93\n",
            "[495719 | 9241.05] loss=0.83 avg=0.93\n",
            "[495720 | 9241.92] loss=1.37 avg=0.93\n",
            "[495721 | 9242.80] loss=0.96 avg=0.93\n",
            "[495722 | 9243.68] loss=0.92 avg=0.93\n",
            "[495723 | 9244.54] loss=0.96 avg=0.93\n",
            "[495724 | 9245.42] loss=0.98 avg=0.93\n",
            "[495725 | 9246.30] loss=1.07 avg=0.93\n",
            "[495726 | 9247.17] loss=0.92 avg=0.93\n",
            "[495727 | 9248.04] loss=0.33 avg=0.93\n",
            "[495728 | 9248.93] loss=0.75 avg=0.93\n",
            "[495729 | 9249.80] loss=1.05 avg=0.93\n",
            "[495730 | 9250.67] loss=1.02 avg=0.93\n",
            "[495731 | 9251.54] loss=0.96 avg=0.93\n",
            "[495732 | 9252.42] loss=0.85 avg=0.93\n",
            "[495733 | 9253.29] loss=0.95 avg=0.93\n",
            "[495734 | 9254.17] loss=1.19 avg=0.93\n",
            "[495735 | 9255.04] loss=0.97 avg=0.93\n",
            "[495736 | 9255.92] loss=1.13 avg=0.93\n",
            "[495737 | 9256.78] loss=0.84 avg=0.93\n",
            "[495738 | 9257.66] loss=1.04 avg=0.93\n",
            "[495739 | 9258.54] loss=0.77 avg=0.93\n",
            "[495740 | 9259.41] loss=1.24 avg=0.94\n",
            "[495741 | 9260.28] loss=0.96 avg=0.94\n",
            "[495742 | 9261.17] loss=0.79 avg=0.93\n",
            "[495743 | 9262.04] loss=0.86 avg=0.93\n",
            "[495744 | 9262.91] loss=0.77 avg=0.93\n",
            "[495745 | 9263.79] loss=0.91 avg=0.93\n",
            "[495746 | 9264.66] loss=1.18 avg=0.93\n",
            "[495747 | 9265.53] loss=1.16 avg=0.94\n",
            "[495748 | 9266.40] loss=1.08 avg=0.94\n",
            "[495749 | 9267.28] loss=0.88 avg=0.94\n",
            "[495750 | 9268.15] loss=0.87 avg=0.94\n",
            "[495751 | 9269.03] loss=1.01 avg=0.94\n",
            "[495752 | 9269.91] loss=0.78 avg=0.94\n",
            "[495753 | 9270.79] loss=1.02 avg=0.94\n",
            "[495754 | 9271.65] loss=0.96 avg=0.94\n",
            "[495755 | 9272.53] loss=0.70 avg=0.93\n",
            "[495756 | 9273.40] loss=0.78 avg=0.93\n",
            "[495757 | 9274.28] loss=0.84 avg=0.93\n",
            "[495758 | 9275.15] loss=0.99 avg=0.93\n",
            "[495759 | 9276.03] loss=0.80 avg=0.93\n",
            "[495760 | 9276.91] loss=0.94 avg=0.93\n",
            "[495761 | 9277.78] loss=0.91 avg=0.93\n",
            "[495762 | 9278.65] loss=1.02 avg=0.93\n",
            "[495763 | 9279.53] loss=0.97 avg=0.93\n",
            "[495764 | 9280.39] loss=1.17 avg=0.93\n",
            "[495765 | 9281.27] loss=0.80 avg=0.93\n",
            "[495766 | 9282.14] loss=1.34 avg=0.94\n",
            "[495767 | 9283.02] loss=0.84 avg=0.94\n",
            "[495768 | 9283.89] loss=0.98 avg=0.94\n",
            "[495769 | 9284.77] loss=1.29 avg=0.94\n",
            "[495770 | 9285.64] loss=1.00 avg=0.94\n",
            "[495771 | 9286.52] loss=1.06 avg=0.94\n",
            "[495772 | 9287.39] loss=1.13 avg=0.94\n",
            "[495773 | 9288.27] loss=0.85 avg=0.94\n",
            "[495774 | 9289.14] loss=0.92 avg=0.94\n",
            "[495775 | 9290.02] loss=1.02 avg=0.94\n",
            "[495776 | 9290.89] loss=0.94 avg=0.94\n",
            "[495777 | 9291.77] loss=0.85 avg=0.94\n",
            "[495778 | 9292.64] loss=0.86 avg=0.94\n",
            "[495779 | 9293.52] loss=0.83 avg=0.94\n",
            "[495780 | 9294.39] loss=1.08 avg=0.94\n",
            "[495781 | 9295.26] loss=0.81 avg=0.94\n",
            "[495782 | 9296.14] loss=1.02 avg=0.94\n",
            "[495783 | 9297.01] loss=0.99 avg=0.94\n",
            "[495784 | 9297.88] loss=0.91 avg=0.94\n",
            "[495785 | 9298.75] loss=0.89 avg=0.94\n",
            "[495786 | 9299.63] loss=0.78 avg=0.94\n",
            "[495787 | 9300.51] loss=1.06 avg=0.94\n",
            "[495788 | 9301.38] loss=0.85 avg=0.94\n",
            "[495789 | 9302.25] loss=1.01 avg=0.94\n",
            "[495790 | 9303.13] loss=1.19 avg=0.94\n",
            "[495791 | 9304.00] loss=0.81 avg=0.94\n",
            "[495792 | 9304.88] loss=0.95 avg=0.94\n",
            "[495793 | 9305.75] loss=0.87 avg=0.94\n",
            "[495794 | 9306.63] loss=1.04 avg=0.94\n",
            "[495795 | 9307.50] loss=0.68 avg=0.94\n",
            "[495796 | 9308.38] loss=0.86 avg=0.94\n",
            "[495797 | 9309.25] loss=0.93 avg=0.94\n",
            "[495798 | 9310.12] loss=0.82 avg=0.94\n",
            "[495799 | 9311.00] loss=1.20 avg=0.94\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " зупинилася більшою тривалістю житлової компанії. У період перемоги збільшується майже до 400 м.\n",
            "22 березня в плані відбувся реалізація фахівцями на обхваті ворога. У Вінницях та Універсах зустрічається другий стиль — ДХТИ і ГИО знову з'явилося в готиці шалом компанії ревокла.\n",
            "Загалом Радянська Армія затверджена в роках Перегінського окружного з'їзду компанії, основна програма депортації в рамках епідемії і пропозиції влади із завданням керувати угода про особливий бюджет ООН; ставши важливим джерелом талісманської компартії у призначеній частині таких військових програм ревоклів до паперів вторгнення пана Путіна.\n",
            "За поширеним списком, заготовка Путіна пропонувалася свідченням епідеміології від Європи як незалежно від тяжкої відмови до паперу початку 1960-х років у басейні незалежної парадигми міста Вінниччини.\n",
            "Згідно з чинним змістом Залізний союз Лівії у війн\n",
            "\n",
            "[495800 | 9329.22] loss=0.87 avg=0.94\n",
            "[495801 | 9330.08] loss=0.72 avg=0.94\n",
            "[495802 | 9330.96] loss=0.76 avg=0.94\n",
            "[495803 | 9331.84] loss=0.84 avg=0.93\n",
            "[495804 | 9332.71] loss=0.83 avg=0.93\n",
            "[495805 | 9333.59] loss=1.00 avg=0.93\n",
            "[495806 | 9334.46] loss=1.08 avg=0.94\n",
            "[495807 | 9335.33] loss=1.23 avg=0.94\n",
            "[495808 | 9336.20] loss=1.00 avg=0.94\n",
            "[495809 | 9337.08] loss=0.73 avg=0.94\n",
            "[495810 | 9337.95] loss=0.87 avg=0.94\n",
            "[495811 | 9338.83] loss=1.07 avg=0.94\n",
            "[495812 | 9339.70] loss=1.01 avg=0.94\n",
            "[495813 | 9340.58] loss=1.03 avg=0.94\n",
            "[495814 | 9341.45] loss=1.02 avg=0.94\n",
            "[495815 | 9342.33] loss=0.89 avg=0.94\n",
            "[495816 | 9343.20] loss=0.92 avg=0.94\n",
            "[495817 | 9344.08] loss=1.22 avg=0.94\n",
            "[495818 | 9344.95] loss=0.79 avg=0.94\n",
            "[495819 | 9345.83] loss=0.98 avg=0.94\n",
            "[495820 | 9346.70] loss=0.89 avg=0.94\n",
            "[495821 | 9347.57] loss=1.02 avg=0.94\n",
            "[495822 | 9348.45] loss=0.97 avg=0.94\n",
            "[495823 | 9349.32] loss=0.78 avg=0.94\n",
            "[495824 | 9350.19] loss=0.99 avg=0.94\n",
            "[495825 | 9351.07] loss=0.86 avg=0.94\n",
            "[495826 | 9351.95] loss=0.99 avg=0.94\n",
            "[495827 | 9352.83] loss=0.72 avg=0.94\n",
            "[495828 | 9353.70] loss=0.78 avg=0.94\n",
            "[495829 | 9354.57] loss=0.96 avg=0.94\n",
            "[495830 | 9355.45] loss=0.99 avg=0.94\n",
            "[495831 | 9356.33] loss=1.02 avg=0.94\n",
            "[495832 | 9357.20] loss=0.92 avg=0.94\n",
            "[495833 | 9358.08] loss=0.90 avg=0.94\n",
            "[495834 | 9358.96] loss=0.94 avg=0.94\n",
            "[495835 | 9359.83] loss=0.86 avg=0.94\n",
            "[495836 | 9360.70] loss=0.82 avg=0.94\n",
            "[495837 | 9361.57] loss=0.76 avg=0.93\n",
            "[495838 | 9362.44] loss=1.16 avg=0.94\n",
            "[495839 | 9363.32] loss=0.84 avg=0.93\n",
            "[495840 | 9364.19] loss=1.00 avg=0.94\n",
            "[495841 | 9365.07] loss=0.89 avg=0.94\n",
            "[495842 | 9365.94] loss=0.89 avg=0.93\n",
            "[495843 | 9366.81] loss=0.83 avg=0.93\n",
            "[495844 | 9367.69] loss=0.79 avg=0.93\n",
            "[495845 | 9368.56] loss=0.93 avg=0.93\n",
            "[495846 | 9369.44] loss=0.97 avg=0.93\n",
            "[495847 | 9370.31] loss=0.97 avg=0.93\n",
            "[495848 | 9371.18] loss=1.02 avg=0.93\n",
            "[495849 | 9372.06] loss=1.03 avg=0.93\n",
            "[495850 | 9372.93] loss=1.02 avg=0.94\n",
            "[495851 | 9373.80] loss=1.03 avg=0.94\n",
            "[495852 | 9374.68] loss=0.91 avg=0.94\n",
            "[495853 | 9375.55] loss=0.92 avg=0.94\n",
            "[495854 | 9376.43] loss=1.02 avg=0.94\n",
            "[495855 | 9377.30] loss=1.15 avg=0.94\n",
            "[495856 | 9378.18] loss=1.01 avg=0.94\n",
            "[495857 | 9379.05] loss=1.05 avg=0.94\n",
            "[495858 | 9379.92] loss=0.64 avg=0.94\n",
            "[495859 | 9380.79] loss=0.86 avg=0.94\n",
            "[495860 | 9381.67] loss=0.82 avg=0.94\n",
            "[495861 | 9382.54] loss=1.12 avg=0.94\n",
            "[495862 | 9383.42] loss=0.96 avg=0.94\n",
            "[495863 | 9384.29] loss=0.85 avg=0.94\n",
            "[495864 | 9385.17] loss=0.86 avg=0.94\n",
            "[495865 | 9386.04] loss=0.96 avg=0.94\n",
            "[495866 | 9386.91] loss=0.96 avg=0.94\n",
            "[495867 | 9387.79] loss=0.94 avg=0.94\n",
            "[495868 | 9388.66] loss=0.92 avg=0.94\n",
            "[495869 | 9389.53] loss=0.98 avg=0.94\n",
            "[495870 | 9390.41] loss=0.94 avg=0.94\n",
            "[495871 | 9391.28] loss=1.24 avg=0.94\n",
            "[495872 | 9392.16] loss=0.73 avg=0.94\n",
            "[495873 | 9393.04] loss=0.94 avg=0.94\n",
            "[495874 | 9393.92] loss=1.05 avg=0.94\n",
            "[495875 | 9394.79] loss=0.36 avg=0.93\n",
            "[495876 | 9395.66] loss=1.02 avg=0.93\n",
            "[495877 | 9396.54] loss=1.04 avg=0.94\n",
            "[495878 | 9397.42] loss=0.88 avg=0.93\n",
            "[495879 | 9398.29] loss=0.95 avg=0.93\n",
            "[495880 | 9399.17] loss=0.89 avg=0.93\n",
            "[495881 | 9400.04] loss=0.82 avg=0.93\n",
            "[495882 | 9400.91] loss=1.09 avg=0.93\n",
            "[495883 | 9401.79] loss=0.85 avg=0.93\n",
            "[495884 | 9402.67] loss=0.88 avg=0.93\n",
            "[495885 | 9403.53] loss=1.02 avg=0.93\n",
            "[495886 | 9404.41] loss=0.81 avg=0.93\n",
            "[495887 | 9405.29] loss=1.02 avg=0.93\n",
            "[495888 | 9406.16] loss=1.04 avg=0.94\n",
            "[495889 | 9407.03] loss=1.31 avg=0.94\n",
            "[495890 | 9407.91] loss=0.90 avg=0.94\n",
            "[495891 | 9408.78] loss=0.82 avg=0.94\n",
            "[495892 | 9409.66] loss=1.06 avg=0.94\n",
            "[495893 | 9410.54] loss=0.99 avg=0.94\n",
            "[495894 | 9411.41] loss=1.10 avg=0.94\n",
            "[495895 | 9412.29] loss=0.84 avg=0.94\n",
            "[495896 | 9413.16] loss=0.93 avg=0.94\n",
            "[495897 | 9414.04] loss=1.15 avg=0.94\n",
            "[495898 | 9414.91] loss=0.95 avg=0.94\n",
            "[495899 | 9415.78] loss=1.00 avg=0.94\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "ах відомий в німецьких магнатах Лиса  Антіофан. Руська сім'я має кордон на Бармені антиохоче мального абатства або заслуговує на його найглуші простори. Берніга були зустрінені в Акклу, і хвора біля англійського торгового порту. Як хвилини в наш крила і жорсткі служби, що дало йому оминуту віруючість ченців і сторін, він перебиває хворобу на користь Селі, Лисуван Аніпій.\n",
            "Село з 1740 по 1770 рр. за збірну Бараярцьких островів було включено до ескалації Село, включено Лис на територію Бармені. Допомогу згідно з Барменом («жирбом, кудий лід годучий, що відважав служіння, в своїй оповіді вдаюйте і відправлене впорядковуючись там боронить все-одноразові маси») прийшла до Кригана, і їм було відзначено з шеренгів . Криган згадувала кар'єру, що дуже сильно переміщалась на північному схилі. Острів Одюха, теж зумів згрегувати необхідну кількість трубчастої конструкції з металевою камен\n",
            "\n",
            "[495900 | 9433.96] loss=0.91 avg=0.94\n",
            "[495901 | 9434.84] loss=0.77 avg=0.94\n",
            "[495902 | 9435.70] loss=0.88 avg=0.94\n",
            "[495903 | 9436.57] loss=0.79 avg=0.94\n",
            "[495904 | 9437.45] loss=0.82 avg=0.94\n",
            "[495905 | 9438.33] loss=0.82 avg=0.94\n",
            "[495906 | 9439.21] loss=1.00 avg=0.94\n",
            "[495907 | 9440.08] loss=0.90 avg=0.94\n",
            "[495908 | 9440.96] loss=0.86 avg=0.94\n",
            "[495909 | 9441.83] loss=0.89 avg=0.94\n",
            "[495910 | 9442.71] loss=0.90 avg=0.93\n",
            "[495911 | 9443.58] loss=1.01 avg=0.94\n",
            "[495912 | 9444.45] loss=0.93 avg=0.94\n",
            "[495913 | 9445.33] loss=0.95 avg=0.94\n",
            "[495914 | 9446.20] loss=0.73 avg=0.93\n",
            "[495915 | 9447.08] loss=1.03 avg=0.93\n",
            "[495916 | 9447.96] loss=1.02 avg=0.94\n",
            "[495917 | 9448.82] loss=1.07 avg=0.94\n",
            "[495918 | 9449.70] loss=0.79 avg=0.94\n",
            "[495919 | 9450.58] loss=1.06 avg=0.94\n",
            "[495920 | 9451.46] loss=0.94 avg=0.94\n",
            "[495921 | 9452.33] loss=1.01 avg=0.94\n",
            "[495922 | 9453.21] loss=1.17 avg=0.94\n",
            "[495923 | 9454.08] loss=0.96 avg=0.94\n",
            "[495924 | 9454.96] loss=0.80 avg=0.94\n",
            "[495925 | 9455.84] loss=0.81 avg=0.94\n",
            "[495926 | 9456.72] loss=1.03 avg=0.94\n",
            "[495927 | 9457.59] loss=0.83 avg=0.94\n",
            "[495928 | 9458.47] loss=1.08 avg=0.94\n",
            "[495929 | 9459.35] loss=0.91 avg=0.94\n",
            "[495930 | 9460.22] loss=0.80 avg=0.94\n",
            "[495931 | 9461.10] loss=0.79 avg=0.94\n",
            "[495932 | 9461.97] loss=0.95 avg=0.94\n",
            "[495933 | 9462.85] loss=0.75 avg=0.93\n",
            "[495934 | 9463.73] loss=0.71 avg=0.93\n",
            "[495935 | 9464.61] loss=0.87 avg=0.93\n",
            "[495936 | 9465.48] loss=0.91 avg=0.93\n",
            "[495937 | 9466.36] loss=0.85 avg=0.93\n",
            "[495938 | 9467.23] loss=0.88 avg=0.93\n",
            "[495939 | 9468.10] loss=1.06 avg=0.93\n",
            "[495940 | 9468.97] loss=0.99 avg=0.93\n",
            "[495941 | 9469.84] loss=0.89 avg=0.93\n",
            "[495942 | 9470.72] loss=0.93 avg=0.93\n",
            "[495943 | 9471.59] loss=0.78 avg=0.93\n",
            "[495944 | 9472.47] loss=0.98 avg=0.93\n",
            "[495945 | 9473.35] loss=0.90 avg=0.93\n",
            "[495946 | 9474.22] loss=0.89 avg=0.93\n",
            "[495947 | 9475.10] loss=0.74 avg=0.93\n",
            "[495948 | 9475.97] loss=1.30 avg=0.93\n",
            "[495949 | 9476.84] loss=1.05 avg=0.93\n",
            "[495950 | 9477.72] loss=1.00 avg=0.93\n",
            "[495951 | 9478.60] loss=0.83 avg=0.93\n",
            "[495952 | 9479.48] loss=0.93 avg=0.93\n",
            "[495953 | 9480.35] loss=0.92 avg=0.93\n",
            "[495954 | 9481.23] loss=1.15 avg=0.93\n",
            "[495955 | 9482.10] loss=1.06 avg=0.93\n",
            "[495956 | 9482.98] loss=0.85 avg=0.93\n",
            "[495957 | 9483.86] loss=0.76 avg=0.93\n",
            "[495958 | 9484.73] loss=1.20 avg=0.94\n",
            "[495959 | 9485.61] loss=0.88 avg=0.93\n",
            "[495960 | 9486.48] loss=1.05 avg=0.94\n",
            "[495961 | 9487.35] loss=0.88 avg=0.94\n",
            "[495962 | 9488.22] loss=1.05 avg=0.94\n",
            "[495963 | 9489.09] loss=1.44 avg=0.94\n",
            "[495964 | 9489.97] loss=0.95 avg=0.94\n",
            "[495965 | 9490.84] loss=0.79 avg=0.94\n",
            "[495966 | 9491.71] loss=1.20 avg=0.94\n",
            "[495967 | 9492.59] loss=1.00 avg=0.94\n",
            "[495968 | 9493.46] loss=0.84 avg=0.94\n",
            "[495969 | 9494.34] loss=0.91 avg=0.94\n",
            "[495970 | 9495.21] loss=1.02 avg=0.94\n",
            "[495971 | 9496.09] loss=0.85 avg=0.94\n",
            "[495972 | 9496.97] loss=1.04 avg=0.94\n",
            "[495973 | 9497.84] loss=0.93 avg=0.94\n",
            "[495974 | 9498.72] loss=0.88 avg=0.94\n",
            "[495975 | 9499.59] loss=0.98 avg=0.94\n",
            "[495976 | 9500.47] loss=0.83 avg=0.94\n",
            "[495977 | 9501.34] loss=0.94 avg=0.94\n",
            "[495978 | 9502.23] loss=0.78 avg=0.94\n",
            "[495979 | 9503.10] loss=0.90 avg=0.94\n",
            "[495980 | 9503.98] loss=1.06 avg=0.94\n",
            "[495981 | 9504.85] loss=0.76 avg=0.94\n",
            "[495982 | 9505.73] loss=0.82 avg=0.94\n",
            "[495983 | 9506.60] loss=0.91 avg=0.94\n",
            "[495984 | 9507.48] loss=0.79 avg=0.94\n",
            "[495985 | 9508.36] loss=1.01 avg=0.94\n",
            "[495986 | 9509.23] loss=1.03 avg=0.94\n",
            "[495987 | 9510.11] loss=1.01 avg=0.94\n",
            "[495988 | 9510.98] loss=0.98 avg=0.94\n",
            "[495989 | 9511.86] loss=0.94 avg=0.94\n",
            "[495990 | 9512.74] loss=0.75 avg=0.94\n",
            "[495991 | 9513.62] loss=1.10 avg=0.94\n",
            "[495992 | 9514.50] loss=0.77 avg=0.94\n",
            "[495993 | 9515.38] loss=1.03 avg=0.94\n",
            "[495994 | 9516.25] loss=1.00 avg=0.94\n",
            "[495995 | 9517.12] loss=0.76 avg=0.94\n",
            "[495996 | 9518.00] loss=0.99 avg=0.94\n",
            "[495997 | 9518.87] loss=0.81 avg=0.94\n",
            "[495998 | 9519.75] loss=0.96 avg=0.94\n",
            "[495999 | 9520.63] loss=0.86 avg=0.94\n",
            "Saving checkpoint/run1/model-496000\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "� \"колонкою під Стрий, Лінни, а в т. ч. е. \"Пшениці ставився Ермітарва \". Вважається, що зрештою Західний храм Самуанров, гармата якого виповнили цілком непоганими, постраждали від вірменітету. Тоді Візе було відвідувача короною, під час якого у Західну художню частину Ермітарва, і вже були позиційними картинками, добре здійснювали здоровий політичний бум. Незважаючи на себе, ця ще й відома як Горобець, так в іншій лізі вони почали сягати успадкування, оскільки в той час німецький уряд двічі нагадував найвищу популярність у районах села Білоустан. До сучасних цілей під час греблі простіше розгадували і вручну воїни. Однак відносини деяких жінок спорудили боротьбу за підсудкову обозу — німецькі царські «стріт» вдалося зруйнувати оновлення.\n",
            "Поховану в Східній Європі, Горобій, у Каліфорній Америці існував статус, під час перебування в короля Станіслава Ежек посприяв його відносну\n",
            "\n",
            "[496000 | 9543.10] loss=1.12 avg=0.94\n",
            "[496001 | 9543.97] loss=1.02 avg=0.94\n",
            "[496002 | 9544.85] loss=0.76 avg=0.94\n",
            "[496003 | 9545.72] loss=0.94 avg=0.94\n",
            "[496004 | 9546.60] loss=0.89 avg=0.94\n",
            "[496005 | 9547.48] loss=0.99 avg=0.94\n",
            "[496006 | 9548.35] loss=0.85 avg=0.94\n",
            "[496007 | 9549.23] loss=0.84 avg=0.93\n",
            "[496008 | 9550.11] loss=0.95 avg=0.93\n",
            "[496009 | 9550.98] loss=0.88 avg=0.93\n",
            "[496010 | 9551.86] loss=1.15 avg=0.94\n",
            "[496011 | 9552.74] loss=0.84 avg=0.94\n",
            "[496012 | 9553.61] loss=0.81 avg=0.93\n",
            "[496013 | 9554.49] loss=0.83 avg=0.93\n",
            "[496014 | 9555.36] loss=0.75 avg=0.93\n",
            "[496015 | 9556.24] loss=1.17 avg=0.93\n",
            "[496016 | 9557.11] loss=0.85 avg=0.93\n",
            "[496017 | 9557.98] loss=0.89 avg=0.93\n",
            "[496018 | 9558.86] loss=1.01 avg=0.93\n",
            "[496019 | 9559.74] loss=0.94 avg=0.93\n",
            "[496020 | 9560.62] loss=1.11 avg=0.93\n",
            "[496021 | 9561.49] loss=1.04 avg=0.94\n",
            "[496022 | 9562.37] loss=0.77 avg=0.93\n",
            "[496023 | 9563.24] loss=0.80 avg=0.93\n",
            "[496024 | 9564.12] loss=1.20 avg=0.94\n",
            "[496025 | 9565.00] loss=0.83 avg=0.93\n",
            "[496026 | 9565.87] loss=0.57 avg=0.93\n",
            "[496027 | 9566.75] loss=1.16 avg=0.93\n",
            "[496028 | 9567.62] loss=1.08 avg=0.93\n",
            "[496029 | 9568.51] loss=1.02 avg=0.94\n",
            "[496030 | 9569.39] loss=0.88 avg=0.94\n",
            "[496031 | 9570.27] loss=0.95 avg=0.94\n",
            "[496032 | 9571.14] loss=0.97 avg=0.94\n",
            "[496033 | 9572.01] loss=1.14 avg=0.94\n",
            "[496034 | 9572.88] loss=0.43 avg=0.93\n",
            "[496035 | 9573.75] loss=0.87 avg=0.93\n",
            "[496036 | 9574.63] loss=1.04 avg=0.93\n",
            "[496037 | 9575.50] loss=0.46 avg=0.93\n",
            "[496038 | 9576.38] loss=0.92 avg=0.93\n",
            "[496039 | 9577.25] loss=0.92 avg=0.93\n",
            "[496040 | 9578.13] loss=0.91 avg=0.93\n",
            "[496041 | 9579.01] loss=0.92 avg=0.93\n",
            "[496042 | 9579.87] loss=0.88 avg=0.93\n",
            "[496043 | 9580.76] loss=0.99 avg=0.93\n",
            "[496044 | 9581.63] loss=0.86 avg=0.93\n",
            "[496045 | 9582.50] loss=0.83 avg=0.93\n",
            "[496046 | 9583.38] loss=0.93 avg=0.93\n",
            "[496047 | 9584.25] loss=0.95 avg=0.93\n",
            "[496048 | 9585.13] loss=0.96 avg=0.93\n",
            "[496049 | 9586.00] loss=0.81 avg=0.93\n",
            "[496050 | 9586.87] loss=1.11 avg=0.93\n",
            "[496051 | 9587.76] loss=0.85 avg=0.93\n",
            "[496052 | 9588.63] loss=0.78 avg=0.93\n",
            "[496053 | 9589.51] loss=0.85 avg=0.92\n",
            "[496054 | 9590.39] loss=0.96 avg=0.92\n",
            "[496055 | 9591.27] loss=0.88 avg=0.92\n",
            "[496056 | 9592.15] loss=1.11 avg=0.93\n",
            "[496057 | 9593.02] loss=0.89 avg=0.93\n",
            "[496058 | 9593.89] loss=0.73 avg=0.92\n",
            "[496059 | 9594.77] loss=0.92 avg=0.92\n",
            "[496060 | 9595.64] loss=0.90 avg=0.92\n",
            "[496061 | 9596.52] loss=1.01 avg=0.92\n",
            "[496062 | 9597.39] loss=0.85 avg=0.92\n",
            "[496063 | 9598.26] loss=0.92 avg=0.92\n",
            "[496064 | 9599.14] loss=1.01 avg=0.92\n",
            "[496065 | 9600.02] loss=1.03 avg=0.93\n",
            "[496066 | 9600.90] loss=0.98 avg=0.93\n",
            "[496067 | 9601.77] loss=1.07 avg=0.93\n",
            "[496068 | 9602.64] loss=1.07 avg=0.93\n",
            "[496069 | 9603.52] loss=0.95 avg=0.93\n",
            "[496070 | 9604.39] loss=0.86 avg=0.93\n",
            "[496071 | 9605.27] loss=0.96 avg=0.93\n",
            "[496072 | 9606.15] loss=1.09 avg=0.93\n",
            "[496073 | 9607.02] loss=0.84 avg=0.93\n",
            "[496074 | 9607.89] loss=1.06 avg=0.93\n",
            "[496075 | 9608.77] loss=0.89 avg=0.93\n",
            "[496076 | 9609.64] loss=1.19 avg=0.93\n",
            "[496077 | 9610.51] loss=1.21 avg=0.94\n",
            "[496078 | 9611.39] loss=0.80 avg=0.93\n",
            "[496079 | 9612.27] loss=0.78 avg=0.93\n",
            "[496080 | 9613.14] loss=0.87 avg=0.93\n",
            "[496081 | 9614.01] loss=0.81 avg=0.93\n",
            "[496082 | 9614.90] loss=0.88 avg=0.93\n",
            "[496083 | 9615.77] loss=0.96 avg=0.93\n",
            "[496084 | 9616.64] loss=0.82 avg=0.93\n",
            "[496085 | 9617.52] loss=1.05 avg=0.93\n",
            "[496086 | 9618.39] loss=0.89 avg=0.93\n",
            "[496087 | 9619.27] loss=1.05 avg=0.93\n",
            "[496088 | 9620.14] loss=0.79 avg=0.93\n",
            "[496089 | 9621.02] loss=1.12 avg=0.93\n",
            "[496090 | 9621.89] loss=0.82 avg=0.93\n",
            "[496091 | 9622.76] loss=0.71 avg=0.93\n",
            "[496092 | 9623.63] loss=0.96 avg=0.93\n",
            "[496093 | 9624.51] loss=0.89 avg=0.93\n",
            "[496094 | 9625.37] loss=0.83 avg=0.93\n",
            "[496095 | 9626.25] loss=0.94 avg=0.93\n",
            "[496096 | 9627.13] loss=0.75 avg=0.93\n",
            "[496097 | 9628.00] loss=1.06 avg=0.93\n",
            "[496098 | 9628.88] loss=0.81 avg=0.93\n",
            "[496099 | 9629.75] loss=0.98 avg=0.93\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "�очасні щілини британського комітатського заводу.\n",
            "В результаті Британської експедиції ресурси залежали від потреби. Це нові заводи і зрушені частки в експлуатацію експлуатації.\n",
            "Поставляючи за специфічним процесом, їі плуги здебільшого впритул до невідновлення неначе залучали зброї в експлуатацію, коли дзеркальний модуль додавався режим робити набагато внутрішньому значенню.\n",
            "Основні показники в Кареліф були наявністю лінгвістобентропаузану в продажі (12,5-3,6 міль і 7 градусів забезпечень), для пошукових характеристик-новачків в залежності від вигляду і новачків. На розміщення енергоблоками броньового і штангами можуть враховувати трав'янистих кіл, сертральний розв'язок (20,6-30 міль). Лінгвісти були найчастішими, у печінки — Споруду в світі (7,7). Враховуючи можливе вигляду дуже пояснення, залізна пухка і природний сертральний простір — в інших характеристиках броньовим і штангами п\n",
            "\n",
            "[496100 | 9647.86] loss=1.11 avg=0.93\n",
            "[496101 | 9648.74] loss=1.18 avg=0.93\n",
            "[496102 | 9649.61] loss=1.09 avg=0.93\n",
            "[496103 | 9650.49] loss=1.05 avg=0.93\n",
            "[496104 | 9651.37] loss=0.98 avg=0.93\n",
            "[496105 | 9652.24] loss=0.83 avg=0.93\n",
            "[496106 | 9653.12] loss=0.77 avg=0.93\n",
            "[496107 | 9653.99] loss=1.22 avg=0.93\n",
            "[496108 | 9654.87] loss=1.02 avg=0.94\n",
            "[496109 | 9655.74] loss=0.93 avg=0.94\n",
            "[496110 | 9656.62] loss=0.78 avg=0.93\n",
            "[496111 | 9657.50] loss=1.15 avg=0.94\n",
            "[496112 | 9658.37] loss=0.86 avg=0.94\n",
            "[496113 | 9659.25] loss=0.95 avg=0.94\n",
            "[496114 | 9660.12] loss=1.12 avg=0.94\n",
            "[496115 | 9661.00] loss=0.77 avg=0.94\n",
            "[496116 | 9661.88] loss=0.92 avg=0.94\n",
            "[496117 | 9662.75] loss=0.76 avg=0.93\n",
            "[496118 | 9663.62] loss=0.46 avg=0.93\n",
            "[496119 | 9664.50] loss=1.01 avg=0.93\n",
            "[496120 | 9665.38] loss=0.95 avg=0.93\n",
            "[496121 | 9666.25] loss=1.27 avg=0.93\n",
            "[496122 | 9667.13] loss=1.04 avg=0.93\n",
            "[496123 | 9668.00] loss=0.99 avg=0.93\n",
            "[496124 | 9668.88] loss=0.83 avg=0.93\n",
            "[496125 | 9669.75] loss=0.83 avg=0.93\n",
            "[496126 | 9670.63] loss=0.85 avg=0.93\n",
            "[496127 | 9671.50] loss=0.91 avg=0.93\n",
            "[496128 | 9672.37] loss=0.87 avg=0.93\n",
            "[496129 | 9673.25] loss=0.98 avg=0.93\n",
            "[496130 | 9674.12] loss=0.90 avg=0.93\n",
            "[496131 | 9675.00] loss=1.00 avg=0.93\n",
            "[496132 | 9675.88] loss=0.76 avg=0.93\n",
            "[496133 | 9676.75] loss=0.77 avg=0.93\n",
            "[496134 | 9677.63] loss=0.87 avg=0.93\n",
            "[496135 | 9678.50] loss=0.69 avg=0.93\n",
            "[496136 | 9679.38] loss=1.06 avg=0.93\n",
            "[496137 | 9680.25] loss=0.92 avg=0.93\n",
            "[496138 | 9681.12] loss=0.98 avg=0.93\n",
            "[496139 | 9682.00] loss=0.89 avg=0.93\n",
            "[496140 | 9682.88] loss=0.93 avg=0.93\n",
            "[496141 | 9683.75] loss=0.89 avg=0.93\n",
            "[496142 | 9684.62] loss=0.73 avg=0.92\n",
            "[496143 | 9685.51] loss=1.00 avg=0.93\n",
            "[496144 | 9686.38] loss=1.11 avg=0.93\n",
            "[496145 | 9687.26] loss=0.99 avg=0.93\n",
            "[496146 | 9688.14] loss=1.04 avg=0.93\n",
            "[496147 | 9689.02] loss=0.77 avg=0.93\n",
            "[496148 | 9689.89] loss=1.01 avg=0.93\n",
            "[496149 | 9690.77] loss=0.92 avg=0.93\n",
            "[496150 | 9691.64] loss=1.09 avg=0.93\n",
            "[496151 | 9692.51] loss=0.93 avg=0.93\n",
            "[496152 | 9693.39] loss=0.86 avg=0.93\n",
            "[496153 | 9694.26] loss=0.99 avg=0.93\n",
            "[496154 | 9695.14] loss=1.09 avg=0.93\n",
            "[496155 | 9696.01] loss=1.00 avg=0.93\n",
            "[496156 | 9696.88] loss=1.21 avg=0.93\n",
            "[496157 | 9697.76] loss=0.72 avg=0.93\n",
            "[496158 | 9698.63] loss=0.84 avg=0.93\n",
            "[496159 | 9699.51] loss=0.91 avg=0.93\n",
            "[496160 | 9700.39] loss=0.76 avg=0.93\n",
            "[496161 | 9701.27] loss=0.68 avg=0.93\n",
            "[496162 | 9702.14] loss=0.89 avg=0.93\n",
            "[496163 | 9703.02] loss=1.15 avg=0.93\n",
            "[496164 | 9703.89] loss=0.92 avg=0.93\n",
            "[496165 | 9704.77] loss=1.11 avg=0.93\n",
            "[496166 | 9705.64] loss=1.03 avg=0.93\n",
            "[496167 | 9706.51] loss=1.00 avg=0.93\n",
            "[496168 | 9707.39] loss=0.99 avg=0.93\n",
            "[496169 | 9708.26] loss=0.89 avg=0.93\n",
            "[496170 | 9709.13] loss=0.86 avg=0.93\n",
            "[496171 | 9710.01] loss=0.94 avg=0.93\n",
            "[496172 | 9710.88] loss=1.01 avg=0.93\n",
            "[496173 | 9711.76] loss=0.84 avg=0.93\n",
            "[496174 | 9712.63] loss=0.90 avg=0.93\n",
            "[496175 | 9713.50] loss=0.88 avg=0.93\n",
            "[496176 | 9714.38] loss=0.88 avg=0.93\n",
            "[496177 | 9715.25] loss=0.77 avg=0.93\n",
            "[496178 | 9716.13] loss=0.90 avg=0.93\n",
            "[496179 | 9717.00] loss=0.85 avg=0.93\n",
            "[496180 | 9717.87] loss=0.65 avg=0.93\n",
            "[496181 | 9718.75] loss=1.05 avg=0.93\n",
            "[496182 | 9719.63] loss=0.88 avg=0.93\n",
            "[496183 | 9720.51] loss=1.06 avg=0.93\n",
            "[496184 | 9721.38] loss=1.19 avg=0.93\n",
            "[496185 | 9722.26] loss=1.06 avg=0.93\n",
            "[496186 | 9723.13] loss=0.87 avg=0.93\n",
            "[496187 | 9724.00] loss=0.94 avg=0.93\n",
            "[496188 | 9724.88] loss=0.89 avg=0.93\n",
            "[496189 | 9725.75] loss=1.00 avg=0.93\n",
            "[496190 | 9726.63] loss=0.82 avg=0.93\n",
            "[496191 | 9727.50] loss=0.82 avg=0.93\n",
            "[496192 | 9728.38] loss=1.02 avg=0.93\n",
            "[496193 | 9729.25] loss=0.79 avg=0.93\n",
            "[496194 | 9730.12] loss=0.84 avg=0.93\n",
            "[496195 | 9731.00] loss=0.90 avg=0.93\n",
            "[496196 | 9731.87] loss=1.00 avg=0.93\n",
            "[496197 | 9732.74] loss=1.33 avg=0.93\n",
            "[496198 | 9733.62] loss=0.96 avg=0.93\n",
            "[496199 | 9734.49] loss=0.83 avg=0.93\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "о-блокади вирівнюють рента стану щитом. Як моделі, це може зберігатися як сполучати порядок блокади відновлених або сприяти болтачанської міфологічної символіки. Останній внесено до рента нагороди, правнук для болтонів і патологоризму, а манги — як центром блокади відновлених обрисів відновлених унікальних від'їздах.\n",
            "Час свердловина — невеликий газ, який містить лише навколовець на безпосереднього небі пустотливе рухливе господарство. Завдяки акварелі обрисів блокади виріжня, беручи аби потребувати болті суперечок, але час Закону про порушення отримання постатті, можливо зі слабкої символіки, отримуючи доступ до себе в окрузі Братказ, взяття харчових балок, рухливості та рельєфи Черчилля. Всюди більшою мірою, якщо регулятивні літературні регіони блокади мають достоїти паспорткові зносини в одній кілометр яким мала банкрут-лейна. Оскільки внести до контролю і сполучення морів донині про поход\n",
            "\n",
            "[496200 | 9752.89] loss=2.16 avg=0.94\n",
            "[496201 | 9753.76] loss=1.12 avg=0.94\n",
            "[496202 | 9754.64] loss=1.08 avg=0.95\n",
            "[496203 | 9755.51] loss=1.30 avg=0.95\n",
            "[496204 | 9756.38] loss=0.89 avg=0.95\n",
            "[496205 | 9757.26] loss=1.09 avg=0.95\n",
            "[496206 | 9758.13] loss=0.23 avg=0.94\n",
            "[496207 | 9759.00] loss=0.89 avg=0.94\n",
            "[496208 | 9759.88] loss=0.99 avg=0.94\n",
            "[496209 | 9760.75] loss=1.19 avg=0.95\n",
            "[496210 | 9761.63] loss=1.00 avg=0.95\n",
            "[496211 | 9762.50] loss=0.94 avg=0.95\n",
            "[496212 | 9763.38] loss=1.03 avg=0.95\n",
            "[496213 | 9764.25] loss=0.93 avg=0.95\n",
            "[496214 | 9765.13] loss=0.78 avg=0.95\n",
            "[496215 | 9766.00] loss=0.89 avg=0.94\n",
            "[496216 | 9766.87] loss=0.88 avg=0.94\n",
            "[496217 | 9767.75] loss=1.06 avg=0.95\n",
            "[496218 | 9768.63] loss=0.98 avg=0.95\n",
            "[496219 | 9769.51] loss=0.89 avg=0.94\n",
            "[496220 | 9770.38] loss=0.93 avg=0.94\n",
            "[496221 | 9771.26] loss=0.77 avg=0.94\n",
            "[496222 | 9772.14] loss=0.93 avg=0.94\n",
            "[496223 | 9773.01] loss=0.92 avg=0.94\n",
            "[496224 | 9773.89] loss=1.15 avg=0.94\n",
            "[496225 | 9774.77] loss=0.92 avg=0.94\n",
            "[496226 | 9775.64] loss=1.08 avg=0.95\n",
            "[496227 | 9776.51] loss=0.92 avg=0.95\n",
            "[496228 | 9777.39] loss=0.89 avg=0.95\n",
            "[496229 | 9778.26] loss=0.88 avg=0.94\n",
            "[496230 | 9779.13] loss=1.04 avg=0.95\n",
            "[496231 | 9780.01] loss=0.96 avg=0.95\n",
            "[496232 | 9780.88] loss=0.85 avg=0.94\n",
            "[496233 | 9781.76] loss=0.96 avg=0.94\n",
            "[496234 | 9782.63] loss=0.87 avg=0.94\n",
            "[496235 | 9783.50] loss=0.81 avg=0.94\n",
            "[496236 | 9784.38] loss=0.93 avg=0.94\n",
            "[496237 | 9785.25] loss=1.00 avg=0.94\n",
            "[496238 | 9786.13] loss=0.95 avg=0.94\n",
            "[496239 | 9787.00] loss=0.92 avg=0.94\n",
            "[496240 | 9787.88] loss=1.01 avg=0.94\n",
            "[496241 | 9788.75] loss=1.01 avg=0.94\n",
            "[496242 | 9789.62] loss=0.82 avg=0.94\n",
            "[496243 | 9790.50] loss=1.05 avg=0.94\n",
            "[496244 | 9791.37] loss=0.89 avg=0.94\n",
            "[496245 | 9792.25] loss=1.19 avg=0.95\n",
            "[496246 | 9793.13] loss=0.97 avg=0.95\n",
            "[496247 | 9794.00] loss=0.75 avg=0.94\n",
            "[496248 | 9794.88] loss=0.66 avg=0.94\n",
            "[496249 | 9795.75] loss=1.14 avg=0.94\n",
            "[496250 | 9796.63] loss=1.11 avg=0.95\n",
            "[496251 | 9797.50] loss=0.73 avg=0.94\n",
            "[496252 | 9798.38] loss=0.96 avg=0.94\n",
            "[496253 | 9799.25] loss=0.91 avg=0.94\n",
            "[496254 | 9800.11] loss=0.87 avg=0.94\n",
            "[496255 | 9800.99] loss=0.94 avg=0.94\n",
            "[496256 | 9801.86] loss=0.99 avg=0.94\n",
            "[496257 | 9802.74] loss=0.99 avg=0.94\n",
            "[496258 | 9803.61] loss=1.12 avg=0.94\n",
            "[496259 | 9804.49] loss=0.91 avg=0.94\n",
            "[496260 | 9805.37] loss=0.93 avg=0.94\n",
            "[496261 | 9806.24] loss=0.80 avg=0.94\n",
            "[496262 | 9807.12] loss=0.82 avg=0.94\n",
            "[496263 | 9807.99] loss=0.71 avg=0.94\n",
            "[496264 | 9808.87] loss=1.10 avg=0.94\n",
            "[496265 | 9809.75] loss=1.14 avg=0.94\n",
            "[496266 | 9810.62] loss=0.94 avg=0.94\n",
            "[496267 | 9811.50] loss=0.88 avg=0.94\n",
            "[496268 | 9812.37] loss=0.88 avg=0.94\n",
            "[496269 | 9813.25] loss=0.92 avg=0.94\n",
            "[496270 | 9814.12] loss=0.95 avg=0.94\n",
            "[496271 | 9815.00] loss=1.08 avg=0.94\n",
            "[496272 | 9815.88] loss=0.85 avg=0.94\n",
            "[496273 | 9816.75] loss=0.95 avg=0.94\n",
            "[496274 | 9817.63] loss=0.88 avg=0.94\n",
            "[496275 | 9818.51] loss=0.99 avg=0.94\n",
            "[496276 | 9819.38] loss=0.79 avg=0.94\n",
            "[496277 | 9820.26] loss=0.95 avg=0.94\n",
            "[496278 | 9821.13] loss=0.75 avg=0.94\n",
            "[496279 | 9822.00] loss=0.84 avg=0.94\n",
            "[496280 | 9822.89] loss=0.94 avg=0.94\n",
            "[496281 | 9823.76] loss=0.88 avg=0.94\n",
            "[496282 | 9824.63] loss=0.94 avg=0.94\n",
            "[496283 | 9825.51] loss=1.07 avg=0.94\n",
            "[496284 | 9826.39] loss=0.79 avg=0.94\n",
            "[496285 | 9827.26] loss=0.90 avg=0.94\n",
            "[496286 | 9828.13] loss=0.96 avg=0.94\n",
            "[496287 | 9829.01] loss=1.07 avg=0.94\n",
            "[496288 | 9829.88] loss=0.92 avg=0.94\n",
            "[496289 | 9830.76] loss=1.00 avg=0.94\n",
            "[496290 | 9831.64] loss=1.05 avg=0.94\n",
            "[496291 | 9832.52] loss=1.01 avg=0.94\n",
            "[496292 | 9833.39] loss=0.82 avg=0.94\n",
            "[496293 | 9834.26] loss=0.80 avg=0.94\n",
            "[496294 | 9835.13] loss=1.46 avg=0.94\n",
            "[496295 | 9836.01] loss=0.98 avg=0.94\n",
            "[496296 | 9836.87] loss=0.82 avg=0.94\n",
            "[496297 | 9837.76] loss=0.74 avg=0.94\n",
            "[496298 | 9838.63] loss=0.85 avg=0.94\n",
            "[496299 | 9839.50] loss=0.87 avg=0.94\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "�инше наступають за власний перелік впливу хореографії навколо інших гумових організмів, строгічних відводі та досвідченої погруддя дається шляхи погодження швидкого насильства. На зміну водним імпульсом, вирізняються видимою до гострих об'єктів погруддя, але може бути переважно його як природних гіпотез. У людині є дією, що гум'яна гумова кулястого чесна, розвинена. На більшоші рідини виділяється натискання дуже діючої класифікації, оскільки вона буде зроблена з високої вмістом рогозоїдів. Люди на суші гумових організмів продумує світло, що потім чесно вона не здуву, витлумачених білів, які до розташування гумової власності можуть бути пошкоджені.\n",
            "Але до землевласників іноді і можуть однак те саме так і інше перериваються швидкісними насильствами, поєднання когнітивного структурного шуму, яке вирізняється порушливим і сотні років інших (полювання, сприйняті швидкостям в гумових о\n",
            "\n",
            "[496300 | 9857.59] loss=0.73 avg=0.94\n",
            "[496301 | 9858.46] loss=1.02 avg=0.94\n",
            "[496302 | 9859.34] loss=0.75 avg=0.94\n",
            "[496303 | 9860.22] loss=0.76 avg=0.93\n",
            "[496304 | 9861.10] loss=0.98 avg=0.93\n",
            "[496305 | 9861.97] loss=0.73 avg=0.93\n",
            "[496306 | 9862.85] loss=1.00 avg=0.93\n",
            "[496307 | 9863.72] loss=1.00 avg=0.93\n",
            "[496308 | 9864.59] loss=1.14 avg=0.94\n",
            "[496309 | 9865.46] loss=0.72 avg=0.93\n",
            "[496310 | 9866.34] loss=1.08 avg=0.93\n",
            "[496311 | 9867.21] loss=0.83 avg=0.93\n",
            "[496312 | 9868.09] loss=1.03 avg=0.93\n",
            "[496313 | 9868.96] loss=0.92 avg=0.93\n",
            "[496314 | 9869.84] loss=0.81 avg=0.93\n",
            "[496315 | 9870.71] loss=0.91 avg=0.93\n",
            "[496316 | 9871.59] loss=1.01 avg=0.93\n",
            "[496317 | 9872.46] loss=0.96 avg=0.93\n",
            "[496318 | 9873.34] loss=1.15 avg=0.94\n",
            "[496319 | 9874.21] loss=0.76 avg=0.93\n",
            "[496320 | 9875.08] loss=0.79 avg=0.93\n",
            "[496321 | 9875.96] loss=0.98 avg=0.93\n",
            "[496322 | 9876.84] loss=1.19 avg=0.94\n",
            "[496323 | 9877.72] loss=0.83 avg=0.94\n",
            "[496324 | 9878.59] loss=0.80 avg=0.93\n",
            "[496325 | 9879.46] loss=1.23 avg=0.94\n",
            "[496326 | 9880.34] loss=0.86 avg=0.94\n",
            "[496327 | 9881.22] loss=1.01 avg=0.94\n",
            "[496328 | 9882.10] loss=0.93 avg=0.94\n",
            "[496329 | 9882.97] loss=1.17 avg=0.94\n",
            "[496330 | 9883.84] loss=0.83 avg=0.94\n",
            "[496331 | 9884.71] loss=0.76 avg=0.94\n",
            "[496332 | 9885.59] loss=0.80 avg=0.93\n",
            "[496333 | 9886.46] loss=1.13 avg=0.94\n",
            "[496334 | 9887.34] loss=0.79 avg=0.94\n",
            "[496335 | 9888.22] loss=0.99 avg=0.94\n",
            "[496336 | 9889.09] loss=0.94 avg=0.94\n",
            "[496337 | 9889.96] loss=0.89 avg=0.94\n",
            "[496338 | 9890.84] loss=0.92 avg=0.94\n",
            "[496339 | 9891.71] loss=0.85 avg=0.93\n",
            "[496340 | 9892.59] loss=0.83 avg=0.93\n",
            "[496341 | 9893.47] loss=0.88 avg=0.93\n",
            "[496342 | 9894.35] loss=0.92 avg=0.93\n",
            "[496343 | 9895.22] loss=1.07 avg=0.93\n",
            "[496344 | 9896.10] loss=0.81 avg=0.93\n",
            "[496345 | 9896.97] loss=1.07 avg=0.93\n",
            "[496346 | 9897.85] loss=1.01 avg=0.93\n",
            "[496347 | 9898.72] loss=0.90 avg=0.93\n",
            "[496348 | 9899.60] loss=1.26 avg=0.94\n",
            "[496349 | 9900.48] loss=0.68 avg=0.94\n",
            "[496350 | 9901.35] loss=0.98 avg=0.94\n",
            "[496351 | 9902.23] loss=0.94 avg=0.94\n",
            "[496352 | 9903.10] loss=0.89 avg=0.94\n",
            "[496353 | 9903.97] loss=0.87 avg=0.93\n",
            "[496354 | 9904.85] loss=0.98 avg=0.93\n",
            "[496355 | 9905.73] loss=0.84 avg=0.93\n",
            "[496356 | 9906.60] loss=1.52 avg=0.94\n",
            "[496357 | 9907.47] loss=0.72 avg=0.94\n",
            "[496358 | 9908.35] loss=1.00 avg=0.94\n",
            "[496359 | 9909.22] loss=0.83 avg=0.94\n",
            "[496360 | 9910.10] loss=0.94 avg=0.94\n",
            "[496361 | 9910.97] loss=0.72 avg=0.93\n",
            "[496362 | 9911.85] loss=0.92 avg=0.93\n",
            "[496363 | 9912.72] loss=0.84 avg=0.93\n",
            "[496364 | 9913.59] loss=0.80 avg=0.93\n",
            "[496365 | 9914.48] loss=1.02 avg=0.93\n",
            "[496366 | 9915.35] loss=1.01 avg=0.93\n",
            "[496367 | 9916.22] loss=0.93 avg=0.93\n",
            "[496368 | 9917.09] loss=0.85 avg=0.93\n",
            "[496369 | 9917.96] loss=0.99 avg=0.93\n",
            "[496370 | 9918.84] loss=0.99 avg=0.93\n",
            "[496371 | 9919.71] loss=1.04 avg=0.94\n",
            "[496372 | 9920.59] loss=0.92 avg=0.94\n",
            "[496373 | 9921.47] loss=0.94 avg=0.94\n",
            "[496374 | 9922.34] loss=1.07 avg=0.94\n",
            "[496375 | 9923.22] loss=0.73 avg=0.93\n",
            "[496376 | 9924.10] loss=0.90 avg=0.93\n",
            "[496377 | 9924.98] loss=0.79 avg=0.93\n",
            "[496378 | 9925.85] loss=0.88 avg=0.93\n",
            "[496379 | 9926.73] loss=1.25 avg=0.94\n",
            "[496380 | 9927.60] loss=0.92 avg=0.94\n",
            "[496381 | 9928.47] loss=0.78 avg=0.93\n",
            "[496382 | 9929.36] loss=0.99 avg=0.93\n",
            "[496383 | 9930.23] loss=0.83 avg=0.93\n",
            "[496384 | 9931.11] loss=0.84 avg=0.93\n",
            "[496385 | 9931.98] loss=0.95 avg=0.93\n",
            "[496386 | 9932.85] loss=0.81 avg=0.93\n",
            "[496387 | 9933.73] loss=0.68 avg=0.93\n",
            "[496388 | 9934.60] loss=1.01 avg=0.93\n",
            "[496389 | 9935.48] loss=0.99 avg=0.93\n",
            "[496390 | 9936.36] loss=0.99 avg=0.93\n",
            "[496391 | 9937.24] loss=1.03 avg=0.93\n",
            "[496392 | 9938.10] loss=1.03 avg=0.93\n",
            "[496393 | 9938.98] loss=0.99 avg=0.93\n",
            "[496394 | 9939.86] loss=0.91 avg=0.93\n",
            "[496395 | 9940.73] loss=0.95 avg=0.93\n",
            "[496396 | 9941.60] loss=1.06 avg=0.93\n",
            "[496397 | 9942.48] loss=0.99 avg=0.94\n",
            "[496398 | 9943.36] loss=0.72 avg=0.93\n",
            "[496399 | 9944.23] loss=0.92 avg=0.93\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "�аків, Академія Панамбуса, Велика Британія та інші. Висловлювалася зміст коактивізму, пов'язаних з математикою, підтримувалася публічністю правління Саксонією й свобод для античної інтерпретації: у Саксонії та Іспанії, в Туреччині та Австро-Угорщині, угорці Саксонії або Свіфтам «людина», «Ехіди не не надто» та «інформандрові корені». Ваші версії Свіфтам:\n",
            "У Лівобережжі монастир та Єжова Оскола взагалі прихилили новий або відпусток Священних Синів до Риму, до місця молодої душпастирювальної кари, яку повинні ставитися до лижного коріння і передавати спосіб організації самой управління. Вони також дійшли шляхом збереження свого батька, розселивши за містяг 1-3, бойкоти між бойом і супротивником та паломництва. Але вже в 1566 році вони з бойовиками втрутилися перед публікацією серед сотенників Угорщини. Відповідно до своєї держками він правив на захоплені береги Ордену Солханою й\n",
            "\n",
            "[496400 | 9962.39] loss=0.71 avg=0.93\n",
            "[496401 | 9963.26] loss=0.74 avg=0.93\n",
            "[496402 | 9964.14] loss=0.93 avg=0.93\n",
            "[496403 | 9965.01] loss=0.86 avg=0.93\n",
            "[496404 | 9965.89] loss=0.97 avg=0.93\n",
            "[496405 | 9966.77] loss=0.86 avg=0.93\n",
            "[496406 | 9967.64] loss=0.96 avg=0.93\n",
            "[496407 | 9968.52] loss=1.05 avg=0.93\n",
            "[496408 | 9969.39] loss=0.87 avg=0.93\n",
            "[496409 | 9970.27] loss=0.86 avg=0.93\n",
            "[496410 | 9971.14] loss=0.82 avg=0.93\n",
            "[496411 | 9972.01] loss=0.88 avg=0.93\n",
            "[496412 | 9972.89] loss=0.87 avg=0.93\n",
            "[496413 | 9973.76] loss=0.86 avg=0.93\n",
            "[496414 | 9974.64] loss=1.07 avg=0.93\n",
            "[496415 | 9975.51] loss=0.78 avg=0.93\n",
            "[496416 | 9976.38] loss=0.78 avg=0.92\n",
            "[496417 | 9977.26] loss=0.87 avg=0.92\n",
            "[496418 | 9978.14] loss=1.16 avg=0.93\n",
            "[496419 | 9979.02] loss=1.07 avg=0.93\n",
            "[496420 | 9979.88] loss=0.71 avg=0.92\n",
            "[496421 | 9980.76] loss=0.99 avg=0.93\n",
            "[496422 | 9981.64] loss=0.79 avg=0.92\n",
            "[496423 | 9982.52] loss=1.02 avg=0.93\n",
            "[496424 | 9983.39] loss=0.97 avg=0.93\n",
            "[496425 | 9984.27] loss=0.99 avg=0.93\n",
            "[496426 | 9985.15] loss=0.87 avg=0.93\n",
            "[496427 | 9986.01] loss=0.96 avg=0.93\n",
            "[496428 | 9986.89] loss=0.68 avg=0.92\n",
            "[496429 | 9987.76] loss=0.73 avg=0.92\n",
            "[496430 | 9988.63] loss=0.75 avg=0.92\n",
            "[496431 | 9989.52] loss=0.94 avg=0.92\n",
            "[496432 | 9990.39] loss=0.90 avg=0.92\n",
            "[496433 | 9991.27] loss=1.04 avg=0.92\n",
            "[496434 | 9992.14] loss=0.81 avg=0.92\n",
            "[496435 | 9993.01] loss=1.22 avg=0.92\n",
            "[496436 | 9993.89] loss=0.80 avg=0.92\n",
            "[496437 | 9994.76] loss=1.11 avg=0.92\n",
            "[496438 | 9995.64] loss=1.03 avg=0.92\n",
            "[496439 | 9996.51] loss=1.06 avg=0.93\n",
            "[496440 | 9997.39] loss=0.92 avg=0.93\n",
            "[496441 | 9998.26] loss=1.08 avg=0.93\n",
            "[496442 | 9999.14] loss=0.83 avg=0.93\n",
            "[496443 | 10000.01] loss=0.96 avg=0.93\n",
            "[496444 | 10000.88] loss=0.98 avg=0.93\n",
            "[496445 | 10001.76] loss=0.82 avg=0.93\n",
            "[496446 | 10002.63] loss=0.97 avg=0.93\n",
            "[496447 | 10003.51] loss=1.06 avg=0.93\n",
            "[496448 | 10004.39] loss=1.03 avg=0.93\n",
            "[496449 | 10005.26] loss=0.81 avg=0.93\n",
            "[496450 | 10006.13] loss=1.02 avg=0.93\n",
            "[496451 | 10007.00] loss=0.98 avg=0.93\n",
            "[496452 | 10007.88] loss=0.92 avg=0.93\n",
            "[496453 | 10008.76] loss=0.95 avg=0.93\n",
            "[496454 | 10009.63] loss=0.94 avg=0.93\n",
            "[496455 | 10010.51] loss=1.01 avg=0.93\n",
            "[496456 | 10011.38] loss=0.94 avg=0.93\n",
            "[496457 | 10012.26] loss=1.08 avg=0.93\n",
            "[496458 | 10013.13] loss=0.77 avg=0.93\n",
            "[496459 | 10014.01] loss=0.89 avg=0.93\n",
            "[496460 | 10014.88] loss=0.82 avg=0.93\n",
            "[496461 | 10015.76] loss=0.98 avg=0.93\n",
            "[496462 | 10016.64] loss=1.24 avg=0.93\n",
            "[496463 | 10017.51] loss=0.90 avg=0.93\n",
            "[496464 | 10018.39] loss=1.02 avg=0.93\n",
            "[496465 | 10019.26] loss=1.01 avg=0.93\n",
            "[496466 | 10020.14] loss=0.86 avg=0.93\n",
            "[496467 | 10021.01] loss=0.96 avg=0.93\n",
            "[496468 | 10021.88] loss=0.95 avg=0.93\n",
            "[496469 | 10022.76] loss=1.07 avg=0.93\n",
            "[496470 | 10023.63] loss=1.05 avg=0.94\n",
            "[496471 | 10024.51] loss=0.86 avg=0.94\n",
            "[496472 | 10025.38] loss=0.92 avg=0.93\n",
            "[496473 | 10026.27] loss=0.91 avg=0.93\n",
            "[496474 | 10027.14] loss=1.09 avg=0.94\n",
            "[496475 | 10028.02] loss=0.98 avg=0.94\n",
            "[496476 | 10028.89] loss=0.86 avg=0.94\n",
            "[496477 | 10029.77] loss=1.08 avg=0.94\n",
            "[496478 | 10030.65] loss=0.90 avg=0.94\n",
            "[496479 | 10031.53] loss=0.70 avg=0.93\n",
            "[496480 | 10032.40] loss=0.78 avg=0.93\n",
            "[496481 | 10033.28] loss=1.00 avg=0.93\n",
            "[496482 | 10034.15] loss=0.72 avg=0.93\n",
            "[496483 | 10035.02] loss=1.01 avg=0.93\n",
            "[496484 | 10035.90] loss=1.05 avg=0.93\n",
            "[496485 | 10036.77] loss=0.80 avg=0.93\n",
            "[496486 | 10037.64] loss=0.99 avg=0.93\n",
            "[496487 | 10038.52] loss=0.90 avg=0.93\n",
            "[496488 | 10039.39] loss=0.78 avg=0.93\n",
            "[496489 | 10040.27] loss=1.17 avg=0.93\n",
            "[496490 | 10041.14] loss=0.89 avg=0.93\n",
            "[496491 | 10042.02] loss=0.83 avg=0.93\n",
            "[496492 | 10042.90] loss=1.02 avg=0.93\n",
            "[496493 | 10043.78] loss=1.00 avg=0.93\n",
            "[496494 | 10044.65] loss=1.02 avg=0.93\n",
            "[496495 | 10045.53] loss=0.89 avg=0.93\n",
            "[496496 | 10046.41] loss=0.53 avg=0.93\n",
            "[496497 | 10047.28] loss=0.98 avg=0.93\n",
            "[496498 | 10048.15] loss=0.93 avg=0.93\n",
            "[496499 | 10049.03] loss=0.93 avg=0.93\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "орі Симон Лунге. Заключаючи \"до\" можна припустити, що уболівальні дефіцити займають лише 13/100 морського порту Систоглиця, \"що переправляє на позначку полону, відпочиваюче \"до\". Однак для дошких кокерав сильно відхиляла практичність, щоб графиня була, єдина діяльність і, отже, зібрала дві частини зем'яного шляху, і ця заклинано не впадала від чогось додому, і її дуже важко, що являє собою повені станкові камені. Перший сильний ніж облога непристикована і сталь обертану зібрали що систоглиця Лунге, оскільки поранений більш задіяною безпосередньо увімкнулася: 20 юристи, вірни від  — для обряду, нгодив приймачі мошок своїх ворогів, які кусали землі займають безпомічну ланку і описують рахунку. Деякі важливі вигадані свідчення приймальної рифектаїв спонсорують той діалог, який підключається до виявлених портів і моргву і ельза купю і глибоко пупок. В той день загину було близько 10 000—15 000\n",
            "\n",
            "[496500 | 10067.41] loss=0.97 avg=0.93\n",
            "[496501 | 10068.28] loss=0.82 avg=0.93\n",
            "[496502 | 10069.15] loss=1.04 avg=0.93\n",
            "[496503 | 10070.03] loss=0.79 avg=0.93\n",
            "[496504 | 10070.91] loss=0.92 avg=0.93\n",
            "[496505 | 10071.78] loss=0.76 avg=0.93\n",
            "[496506 | 10072.66] loss=1.05 avg=0.93\n",
            "[496507 | 10073.54] loss=0.95 avg=0.93\n",
            "[496508 | 10074.40] loss=0.97 avg=0.93\n",
            "[496509 | 10075.28] loss=0.87 avg=0.93\n",
            "[496510 | 10076.15] loss=1.23 avg=0.93\n",
            "[496511 | 10077.03] loss=0.85 avg=0.93\n",
            "[496512 | 10077.90] loss=0.84 avg=0.93\n",
            "[496513 | 10078.77] loss=0.82 avg=0.93\n",
            "[496514 | 10079.65] loss=0.89 avg=0.93\n",
            "[496515 | 10080.53] loss=0.82 avg=0.93\n",
            "[496516 | 10081.41] loss=0.90 avg=0.93\n",
            "[496517 | 10082.28] loss=1.04 avg=0.93\n",
            "[496518 | 10083.16] loss=0.96 avg=0.93\n",
            "[496519 | 10084.03] loss=0.94 avg=0.93\n",
            "[496520 | 10084.91] loss=1.10 avg=0.93\n",
            "[496521 | 10085.79] loss=1.20 avg=0.93\n",
            "[496522 | 10086.67] loss=1.12 avg=0.93\n",
            "[496523 | 10087.54] loss=1.04 avg=0.94\n",
            "[496524 | 10088.42] loss=1.09 avg=0.94\n",
            "[496525 | 10089.30] loss=0.83 avg=0.94\n",
            "[496526 | 10090.16] loss=0.98 avg=0.94\n",
            "[496527 | 10091.04] loss=0.99 avg=0.94\n",
            "[496528 | 10091.92] loss=0.83 avg=0.94\n",
            "[496529 | 10092.80] loss=0.73 avg=0.93\n",
            "[496530 | 10093.67] loss=0.84 avg=0.93\n",
            "[496531 | 10094.55] loss=1.07 avg=0.93\n",
            "[496532 | 10095.42] loss=0.90 avg=0.93\n",
            "[496533 | 10096.29] loss=0.96 avg=0.93\n",
            "[496534 | 10097.18] loss=1.01 avg=0.94\n",
            "[496535 | 10098.05] loss=0.87 avg=0.93\n",
            "[496536 | 10098.92] loss=0.86 avg=0.93\n",
            "[496537 | 10099.80] loss=0.75 avg=0.93\n",
            "[496538 | 10100.68] loss=1.04 avg=0.93\n",
            "[496539 | 10101.55] loss=1.08 avg=0.93\n",
            "[496540 | 10102.42] loss=1.16 avg=0.94\n",
            "[496541 | 10103.30] loss=0.77 avg=0.94\n",
            "[496542 | 10104.17] loss=0.92 avg=0.93\n",
            "[496543 | 10105.04] loss=1.14 avg=0.94\n",
            "[496544 | 10105.92] loss=1.07 avg=0.94\n",
            "[496545 | 10106.79] loss=0.83 avg=0.94\n",
            "[496546 | 10107.67] loss=0.90 avg=0.94\n",
            "[496547 | 10108.54] loss=0.90 avg=0.94\n",
            "[496548 | 10109.42] loss=0.80 avg=0.93\n",
            "[496549 | 10110.28] loss=0.94 avg=0.94\n",
            "[496550 | 10111.15] loss=0.84 avg=0.93\n",
            "[496551 | 10112.03] loss=0.91 avg=0.93\n",
            "[496552 | 10112.90] loss=1.01 avg=0.93\n",
            "[496553 | 10113.77] loss=1.12 avg=0.94\n",
            "[496554 | 10114.65] loss=0.89 avg=0.94\n",
            "[496555 | 10115.53] loss=0.91 avg=0.94\n",
            "[496556 | 10116.41] loss=1.00 avg=0.94\n",
            "[496557 | 10117.28] loss=1.06 avg=0.94\n",
            "[496558 | 10118.15] loss=0.85 avg=0.94\n",
            "[496559 | 10119.03] loss=0.92 avg=0.94\n",
            "[496560 | 10119.91] loss=0.90 avg=0.94\n",
            "[496561 | 10120.79] loss=0.90 avg=0.94\n",
            "[496562 | 10121.67] loss=0.70 avg=0.93\n",
            "[496563 | 10122.54] loss=0.86 avg=0.93\n",
            "[496564 | 10123.41] loss=0.87 avg=0.93\n",
            "[496565 | 10124.29] loss=0.97 avg=0.93\n",
            "[496566 | 10125.17] loss=0.84 avg=0.93\n",
            "[496567 | 10126.05] loss=0.75 avg=0.93\n",
            "[496568 | 10126.92] loss=0.90 avg=0.93\n",
            "[496569 | 10127.79] loss=1.08 avg=0.93\n",
            "[496570 | 10128.66] loss=0.83 avg=0.93\n",
            "[496571 | 10129.53] loss=0.78 avg=0.93\n",
            "[496572 | 10130.41] loss=1.09 avg=0.93\n",
            "[496573 | 10131.28] loss=0.92 avg=0.93\n",
            "[496574 | 10132.15] loss=0.96 avg=0.93\n",
            "[496575 | 10133.03] loss=0.85 avg=0.93\n",
            "[496576 | 10133.90] loss=1.07 avg=0.93\n",
            "[496577 | 10134.78] loss=0.74 avg=0.93\n",
            "[496578 | 10135.65] loss=0.90 avg=0.93\n",
            "[496579 | 10136.53] loss=0.92 avg=0.93\n",
            "[496580 | 10137.41] loss=1.11 avg=0.93\n",
            "[496581 | 10138.28] loss=0.91 avg=0.93\n",
            "[496582 | 10139.16] loss=1.02 avg=0.93\n",
            "[496583 | 10140.03] loss=0.19 avg=0.92\n",
            "[496584 | 10140.91] loss=0.91 avg=0.92\n",
            "[496585 | 10141.78] loss=0.85 avg=0.92\n",
            "[496586 | 10142.66] loss=1.03 avg=0.92\n",
            "[496587 | 10143.53] loss=0.98 avg=0.92\n",
            "[496588 | 10144.40] loss=0.86 avg=0.92\n",
            "[496589 | 10145.28] loss=1.01 avg=0.92\n",
            "[496590 | 10146.15] loss=0.92 avg=0.92\n",
            "[496591 | 10147.03] loss=0.89 avg=0.92\n",
            "[496592 | 10147.91] loss=0.81 avg=0.92\n",
            "[496593 | 10148.78] loss=1.06 avg=0.92\n",
            "[496594 | 10149.66] loss=0.79 avg=0.92\n",
            "[496595 | 10150.52] loss=0.97 avg=0.92\n",
            "[496596 | 10151.40] loss=0.88 avg=0.92\n",
            "[496597 | 10152.28] loss=1.11 avg=0.92\n",
            "[496598 | 10153.15] loss=0.95 avg=0.92\n",
            "[496599 | 10154.03] loss=1.07 avg=0.93\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " (тільки водоймами: тутешні острови — Київ, англійські, Чернігів, Балтабарські білизняні гармати, турери — чотири балтові стрілецькі буряки суглині.  Союз до походу, над яким тутешні колішні й овочеві.  Також отримала назву \"кратер\" — військова площа (чотири двох установки), а також військово-повітряній стрілецькій.\n",
            "В верхів'ях району рав'яні союзи складають головний общинний клас. Наприклад, Погорілі Кувейту — бельгійців Митрополита — лише трьом одному храмовому та роскомолововому селах.\n",
            "Мати, що спричинили постійний проблему в терені Богоявленої Висоцької церкви у Ченстохові. Декілька років розпочався споживання значної кількості платежів, налагоджених злочинцями, часто виникають завдяки роботу та богослужінниці. Землі відійшли на подружжя в Оттаві, де за снігопада відійшли смном Комнезії та день невдалих богослужінь у омаженні церкви, вище одному солдатів. Коли стіни Пути вдал\n",
            "\n",
            "[496600 | 10172.25] loss=0.98 avg=0.93\n",
            "[496601 | 10173.12] loss=0.73 avg=0.92\n",
            "[496602 | 10174.00] loss=0.77 avg=0.92\n",
            "[496603 | 10174.87] loss=0.96 avg=0.92\n",
            "[496604 | 10175.74] loss=0.88 avg=0.92\n",
            "[496605 | 10176.62] loss=0.96 avg=0.92\n",
            "[496606 | 10177.50] loss=0.90 avg=0.92\n",
            "[496607 | 10178.37] loss=1.08 avg=0.93\n",
            "[496608 | 10179.24] loss=0.90 avg=0.92\n",
            "[496609 | 10180.12] loss=0.81 avg=0.92\n",
            "[496610 | 10180.99] loss=1.01 avg=0.92\n",
            "[496611 | 10181.86] loss=0.85 avg=0.92\n",
            "[496612 | 10182.74] loss=0.98 avg=0.92\n",
            "[496613 | 10183.61] loss=0.94 avg=0.92\n",
            "[496614 | 10184.49] loss=0.96 avg=0.92\n",
            "[496615 | 10185.37] loss=0.99 avg=0.93\n",
            "[496616 | 10186.24] loss=0.90 avg=0.93\n",
            "[496617 | 10187.11] loss=0.94 avg=0.93\n",
            "[496618 | 10187.98] loss=0.76 avg=0.92\n",
            "[496619 | 10188.86] loss=1.00 avg=0.92\n",
            "[496620 | 10189.74] loss=0.80 avg=0.92\n",
            "[496621 | 10190.61] loss=0.96 avg=0.92\n",
            "[496622 | 10191.49] loss=0.87 avg=0.92\n",
            "[496623 | 10192.36] loss=0.90 avg=0.92\n",
            "[496624 | 10193.24] loss=0.88 avg=0.92\n",
            "[496625 | 10194.11] loss=0.98 avg=0.92\n",
            "[496626 | 10195.00] loss=0.96 avg=0.92\n",
            "[496627 | 10195.87] loss=1.26 avg=0.93\n",
            "[496628 | 10196.75] loss=0.91 avg=0.93\n",
            "[496629 | 10197.62] loss=1.07 avg=0.93\n",
            "[496630 | 10198.50] loss=1.12 avg=0.93\n",
            "[496631 | 10199.38] loss=0.82 avg=0.93\n",
            "[496632 | 10200.25] loss=0.77 avg=0.93\n",
            "[496633 | 10201.12] loss=0.85 avg=0.93\n",
            "[496634 | 10202.00] loss=0.84 avg=0.93\n",
            "[496635 | 10202.88] loss=1.10 avg=0.93\n",
            "[496636 | 10203.75] loss=0.89 avg=0.93\n",
            "[496637 | 10204.63] loss=0.94 avg=0.93\n",
            "[496638 | 10205.50] loss=0.76 avg=0.93\n",
            "[496639 | 10206.36] loss=1.03 avg=0.93\n",
            "[496640 | 10207.24] loss=0.84 avg=0.93\n",
            "[496641 | 10208.12] loss=0.87 avg=0.93\n",
            "[496642 | 10208.99] loss=0.97 avg=0.93\n",
            "[496643 | 10209.87] loss=0.91 avg=0.93\n",
            "[496644 | 10210.74] loss=0.72 avg=0.92\n",
            "[496645 | 10211.62] loss=1.01 avg=0.92\n",
            "[496646 | 10212.49] loss=0.89 avg=0.92\n",
            "[496647 | 10213.37] loss=0.92 avg=0.92\n",
            "[496648 | 10214.24] loss=0.96 avg=0.92\n",
            "[496649 | 10215.11] loss=0.84 avg=0.92\n",
            "[496650 | 10215.99] loss=0.86 avg=0.92\n",
            "[496651 | 10216.87] loss=0.99 avg=0.92\n",
            "[496652 | 10217.75] loss=1.07 avg=0.92\n",
            "[496653 | 10218.62] loss=0.93 avg=0.92\n",
            "[496654 | 10219.50] loss=1.01 avg=0.93\n",
            "[496655 | 10220.38] loss=0.79 avg=0.92\n",
            "[496656 | 10221.26] loss=0.90 avg=0.92\n",
            "[496657 | 10222.13] loss=0.86 avg=0.92\n",
            "[496658 | 10223.01] loss=0.86 avg=0.92\n",
            "[496659 | 10223.88] loss=1.07 avg=0.92\n",
            "[496660 | 10224.75] loss=0.92 avg=0.92\n",
            "[496661 | 10225.63] loss=0.94 avg=0.92\n",
            "[496662 | 10226.50] loss=0.98 avg=0.93\n",
            "[496663 | 10227.39] loss=0.87 avg=0.92\n",
            "[496664 | 10228.25] loss=0.73 avg=0.92\n",
            "[496665 | 10229.14] loss=1.06 avg=0.92\n",
            "[496666 | 10230.01] loss=0.87 avg=0.92\n",
            "[496667 | 10230.88] loss=1.06 avg=0.92\n",
            "[496668 | 10231.76] loss=1.04 avg=0.93\n",
            "[496669 | 10232.63] loss=0.86 avg=0.93\n",
            "[496670 | 10233.51] loss=1.06 avg=0.93\n",
            "[496671 | 10234.38] loss=0.90 avg=0.93\n",
            "[496672 | 10235.26] loss=0.80 avg=0.92\n",
            "[496673 | 10236.14] loss=0.95 avg=0.93\n",
            "[496674 | 10237.01] loss=0.87 avg=0.92\n",
            "[496675 | 10237.89] loss=0.88 avg=0.92\n",
            "[496676 | 10238.76] loss=1.13 avg=0.93\n",
            "[496677 | 10239.64] loss=0.91 avg=0.93\n",
            "[496678 | 10240.51] loss=1.01 avg=0.93\n",
            "[496679 | 10241.39] loss=0.75 avg=0.93\n",
            "[496680 | 10242.27] loss=0.71 avg=0.92\n",
            "[496681 | 10243.14] loss=0.83 avg=0.92\n",
            "[496682 | 10244.01] loss=0.79 avg=0.92\n",
            "[496683 | 10244.89] loss=1.04 avg=0.92\n",
            "[496684 | 10245.77] loss=0.86 avg=0.92\n",
            "[496685 | 10246.64] loss=0.87 avg=0.92\n",
            "[496686 | 10247.51] loss=0.85 avg=0.92\n",
            "[496687 | 10248.39] loss=0.78 avg=0.92\n",
            "[496688 | 10249.26] loss=0.90 avg=0.92\n",
            "[496689 | 10250.14] loss=0.90 avg=0.92\n",
            "[496690 | 10251.01] loss=0.90 avg=0.92\n",
            "[496691 | 10251.89] loss=1.00 avg=0.92\n",
            "[496692 | 10252.76] loss=0.74 avg=0.92\n",
            "[496693 | 10253.63] loss=0.78 avg=0.92\n",
            "[496694 | 10254.51] loss=1.03 avg=0.92\n",
            "[496695 | 10255.38] loss=0.83 avg=0.92\n",
            "[496696 | 10256.26] loss=0.84 avg=0.92\n",
            "[496697 | 10257.14] loss=0.90 avg=0.92\n",
            "[496698 | 10258.02] loss=0.98 avg=0.92\n",
            "[496699 | 10258.89] loss=0.87 avg=0.92\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "лю, мімеребудова і сніжна поліплоїдних орієнталізованих агрегатів. Багату художність також присвячені єгиптянам Якубовських християн.\n",
            "Результати «Похід на Ідолину» — виникла внаслідок перемоги Іриноборчка через те, що «Імператор Морохнги та Олег звикли вчасно нам часу». Саму Ірину знову вирішив вчасно самовизнати себе шляхом. Несподівано глухо через Іринчук Кубас навчався на передумови вчасної стабільності і своїх травм, за яких менші чаю дивилися під якою Сальєра згадувати. Вчені про ці рекрути приватної власності також були досить зруйновані посланцями осіб, яка розташована імператором Михайлом Степанівним полком. На наступні роки похід представлені польськими загонами. Однак саме більшість з братів, які «з'явилися в середині чотирьох газет звинувачених принцань одного» відкинула саму знання щодо братів. В середині чотирьох газет збірка свідчить, що картин мілсійщини будуть тільки\n",
            "\n",
            "[496700 | 10276.97] loss=0.92 avg=0.92\n",
            "[496701 | 10277.85] loss=1.62 avg=0.92\n",
            "[496702 | 10278.72] loss=0.86 avg=0.92\n",
            "[496703 | 10279.60] loss=0.88 avg=0.92\n",
            "[496704 | 10280.47] loss=0.79 avg=0.92\n",
            "[496705 | 10281.34] loss=1.12 avg=0.92\n",
            "[496706 | 10282.22] loss=0.99 avg=0.92\n",
            "[496707 | 10283.09] loss=1.06 avg=0.92\n",
            "[496708 | 10283.97] loss=0.98 avg=0.92\n",
            "[496709 | 10284.84] loss=1.00 avg=0.93\n",
            "[496710 | 10285.72] loss=0.78 avg=0.92\n",
            "[496711 | 10286.60] loss=0.94 avg=0.92\n",
            "[496712 | 10287.47] loss=0.76 avg=0.92\n",
            "[496713 | 10288.35] loss=1.06 avg=0.92\n",
            "[496714 | 10289.22] loss=0.94 avg=0.92\n",
            "[496715 | 10290.10] loss=0.98 avg=0.92\n",
            "[496716 | 10290.97] loss=1.24 avg=0.93\n",
            "[496717 | 10291.85] loss=0.78 avg=0.93\n",
            "[496718 | 10292.73] loss=0.88 avg=0.93\n",
            "[496719 | 10293.61] loss=0.94 avg=0.93\n",
            "[496720 | 10294.49] loss=1.13 avg=0.93\n",
            "[496721 | 10295.36] loss=1.12 avg=0.93\n",
            "[496722 | 10296.23] loss=0.84 avg=0.93\n",
            "[496723 | 10297.11] loss=0.72 avg=0.93\n",
            "[496724 | 10297.99] loss=0.91 avg=0.93\n",
            "[496725 | 10298.86] loss=0.90 avg=0.93\n",
            "[496726 | 10299.74] loss=0.92 avg=0.93\n",
            "[496727 | 10300.61] loss=0.99 avg=0.93\n",
            "[496728 | 10301.48] loss=0.88 avg=0.93\n",
            "[496729 | 10302.35] loss=0.90 avg=0.93\n",
            "[496730 | 10303.23] loss=0.97 avg=0.93\n",
            "[496731 | 10304.10] loss=1.06 avg=0.93\n",
            "[496732 | 10304.98] loss=0.78 avg=0.93\n",
            "[496733 | 10305.86] loss=1.05 avg=0.93\n",
            "[496734 | 10306.74] loss=1.11 avg=0.93\n",
            "[496735 | 10307.60] loss=0.91 avg=0.93\n",
            "[496736 | 10308.48] loss=1.04 avg=0.93\n",
            "[496737 | 10309.36] loss=0.79 avg=0.93\n",
            "[496738 | 10310.24] loss=1.03 avg=0.93\n",
            "[496739 | 10311.11] loss=0.87 avg=0.93\n",
            "[496740 | 10311.99] loss=0.88 avg=0.93\n",
            "[496741 | 10312.86] loss=0.84 avg=0.93\n",
            "[496742 | 10313.73] loss=1.18 avg=0.93\n",
            "[496743 | 10314.61] loss=0.92 avg=0.93\n",
            "[496744 | 10315.48] loss=1.21 avg=0.93\n",
            "[496745 | 10316.35] loss=0.80 avg=0.93\n",
            "[496746 | 10317.23] loss=0.90 avg=0.93\n",
            "[496747 | 10318.10] loss=0.81 avg=0.93\n",
            "[496748 | 10318.98] loss=0.93 avg=0.93\n",
            "[496749 | 10319.85] loss=1.06 avg=0.93\n",
            "[496750 | 10320.73] loss=1.18 avg=0.93\n",
            "[496751 | 10321.60] loss=0.97 avg=0.93\n",
            "[496752 | 10322.47] loss=1.06 avg=0.94\n",
            "[496753 | 10323.35] loss=1.11 avg=0.94\n",
            "[496754 | 10324.22] loss=0.93 avg=0.94\n",
            "[496755 | 10325.10] loss=1.04 avg=0.94\n",
            "[496756 | 10325.98] loss=0.92 avg=0.94\n",
            "[496757 | 10326.86] loss=0.84 avg=0.94\n",
            "[496758 | 10327.74] loss=1.03 avg=0.94\n",
            "[496759 | 10328.61] loss=0.91 avg=0.94\n",
            "[496760 | 10329.49] loss=0.90 avg=0.94\n",
            "[496761 | 10330.36] loss=0.83 avg=0.94\n",
            "[496762 | 10331.24] loss=0.82 avg=0.94\n",
            "[496763 | 10332.11] loss=1.03 avg=0.94\n",
            "[496764 | 10332.99] loss=0.81 avg=0.94\n",
            "[496765 | 10333.86] loss=0.83 avg=0.93\n",
            "[496766 | 10334.73] loss=0.84 avg=0.93\n",
            "[496767 | 10335.61] loss=0.81 avg=0.93\n",
            "[496768 | 10336.48] loss=0.79 avg=0.93\n",
            "[496769 | 10337.34] loss=0.66 avg=0.93\n",
            "[496770 | 10338.22] loss=1.16 avg=0.93\n",
            "[496771 | 10339.10] loss=1.10 avg=0.93\n",
            "[496772 | 10339.97] loss=0.84 avg=0.93\n",
            "[496773 | 10340.84] loss=0.90 avg=0.93\n",
            "[496774 | 10341.72] loss=0.88 avg=0.93\n",
            "[496775 | 10342.59] loss=0.86 avg=0.93\n",
            "[496776 | 10343.46] loss=0.91 avg=0.93\n",
            "[496777 | 10344.34] loss=0.90 avg=0.93\n",
            "[496778 | 10345.21] loss=0.82 avg=0.93\n",
            "[496779 | 10346.08] loss=0.94 avg=0.93\n",
            "[496780 | 10346.96] loss=1.00 avg=0.93\n",
            "[496781 | 10347.83] loss=0.94 avg=0.93\n",
            "[496782 | 10348.71] loss=0.95 avg=0.93\n",
            "[496783 | 10349.58] loss=1.44 avg=0.93\n",
            "[496784 | 10350.46] loss=1.10 avg=0.94\n",
            "[496785 | 10351.33] loss=0.78 avg=0.93\n",
            "[496786 | 10352.20] loss=0.74 avg=0.93\n",
            "[496787 | 10353.08] loss=1.07 avg=0.93\n",
            "[496788 | 10353.95] loss=0.80 avg=0.93\n",
            "[496789 | 10354.83] loss=0.94 avg=0.93\n",
            "[496790 | 10355.70] loss=0.89 avg=0.93\n",
            "[496791 | 10356.58] loss=0.85 avg=0.93\n",
            "[496792 | 10357.46] loss=0.86 avg=0.93\n",
            "[496793 | 10358.33] loss=1.07 avg=0.93\n",
            "[496794 | 10359.21] loss=1.03 avg=0.93\n",
            "[496795 | 10360.08] loss=0.64 avg=0.93\n",
            "[496796 | 10360.95] loss=1.16 avg=0.93\n",
            "[496797 | 10361.83] loss=0.95 avg=0.93\n",
            "[496798 | 10362.70] loss=0.89 avg=0.93\n",
            "[496799 | 10363.57] loss=0.90 avg=0.93\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "еротиком, навіть килимами це було безбожне. Але більше на Вишнянців, власники більшовиків, дрібно людей із селян бачить станцію, яка знайшла в сьогодні без зображення і лиха (присягними прикрасили відповідні погрозки якості). Погром вони накреслими, де до свого часу було не в погромі; тоді як він пішов не тільки як віскі і селян. У Другій світовій війні віросповідання створило Стамбульське питання семінарії за підозрою на різні часи.\n",
            "Тотальною планкою став австрійський уряд Стригіміра. Це могло вщухатися вище Стамбульське питання про вказівку до вступу у них, таких як представник. Деякі науковці поповнювали думку Нікарагуа, а відомий з думкою про неможливість щодо міністра. Незважаючи на те, що свого духу важко оголювати важкоутворену сестру. Теплін, який претендує на розпорядження Стамбульського питання, до можливої 12 чоловіка чергової робочих пристроїв. Якщо Нікарагуа не може бути іс\n",
            "\n",
            "[496800 | 10381.81] loss=0.92 avg=0.93\n",
            "[496801 | 10382.68] loss=0.89 avg=0.93\n",
            "[496802 | 10383.56] loss=0.97 avg=0.93\n",
            "[496803 | 10384.43] loss=0.83 avg=0.93\n",
            "[496804 | 10385.31] loss=0.74 avg=0.93\n",
            "[496805 | 10386.18] loss=0.82 avg=0.93\n",
            "[496806 | 10387.05] loss=0.74 avg=0.93\n",
            "[496807 | 10387.94] loss=1.39 avg=0.93\n",
            "[496808 | 10388.81] loss=0.23 avg=0.92\n",
            "[496809 | 10389.68] loss=0.77 avg=0.92\n",
            "[496810 | 10390.56] loss=0.93 avg=0.92\n",
            "[496811 | 10391.43] loss=0.91 avg=0.92\n",
            "[496812 | 10392.31] loss=0.90 avg=0.92\n",
            "[496813 | 10393.18] loss=0.92 avg=0.92\n",
            "[496814 | 10394.06] loss=0.93 avg=0.92\n",
            "[496815 | 10394.93] loss=0.74 avg=0.92\n",
            "[496816 | 10395.81] loss=0.83 avg=0.92\n",
            "[496817 | 10396.68] loss=1.21 avg=0.92\n",
            "[496818 | 10397.55] loss=1.04 avg=0.92\n",
            "[496819 | 10398.43] loss=1.08 avg=0.92\n",
            "[496820 | 10399.30] loss=0.92 avg=0.92\n",
            "[496821 | 10400.18] loss=0.91 avg=0.92\n",
            "[496822 | 10401.05] loss=1.06 avg=0.93\n",
            "[496823 | 10401.92] loss=1.14 avg=0.93\n",
            "[496824 | 10402.80] loss=0.92 avg=0.93\n",
            "[496825 | 10403.68] loss=0.82 avg=0.93\n",
            "[496826 | 10404.55] loss=0.84 avg=0.93\n",
            "[496827 | 10405.42] loss=0.92 avg=0.93\n",
            "[496828 | 10406.30] loss=0.93 avg=0.93\n",
            "[496829 | 10407.18] loss=1.04 avg=0.93\n",
            "[496830 | 10408.05] loss=1.15 avg=0.93\n",
            "[496831 | 10408.92] loss=1.37 avg=0.93\n",
            "[496832 | 10409.80] loss=0.92 avg=0.93\n",
            "[496833 | 10410.68] loss=1.11 avg=0.94\n",
            "[496834 | 10411.55] loss=0.81 avg=0.93\n",
            "[496835 | 10412.43] loss=0.86 avg=0.93\n",
            "[496836 | 10413.30] loss=0.92 avg=0.93\n",
            "[496837 | 10414.17] loss=0.81 avg=0.93\n",
            "[496838 | 10415.05] loss=0.94 avg=0.93\n",
            "[496839 | 10415.93] loss=1.04 avg=0.93\n",
            "[496840 | 10416.80] loss=1.07 avg=0.93\n",
            "[496841 | 10417.68] loss=1.19 avg=0.94\n",
            "[496842 | 10418.56] loss=0.83 avg=0.94\n",
            "[496843 | 10419.43] loss=1.03 avg=0.94\n",
            "[496844 | 10420.31] loss=1.05 avg=0.94\n",
            "[496845 | 10421.18] loss=0.87 avg=0.94\n",
            "[496846 | 10422.05] loss=0.81 avg=0.94\n",
            "[496847 | 10422.93] loss=1.05 avg=0.94\n",
            "[496848 | 10423.80] loss=0.90 avg=0.94\n",
            "[496849 | 10424.68] loss=1.07 avg=0.94\n",
            "[496850 | 10425.56] loss=0.90 avg=0.94\n",
            "[496851 | 10426.43] loss=0.88 avg=0.94\n",
            "[496852 | 10427.30] loss=1.06 avg=0.94\n",
            "[496853 | 10428.18] loss=0.98 avg=0.94\n",
            "[496854 | 10429.06] loss=1.04 avg=0.94\n",
            "[496855 | 10429.93] loss=1.02 avg=0.94\n",
            "[496856 | 10430.80] loss=0.77 avg=0.94\n",
            "[496857 | 10431.68] loss=1.22 avg=0.94\n",
            "[496858 | 10432.55] loss=0.96 avg=0.94\n",
            "[496859 | 10433.43] loss=0.95 avg=0.94\n",
            "[496860 | 10434.30] loss=0.83 avg=0.94\n",
            "[496861 | 10435.18] loss=0.88 avg=0.94\n",
            "[496862 | 10436.05] loss=0.86 avg=0.94\n",
            "[496863 | 10436.92] loss=0.95 avg=0.94\n",
            "[496864 | 10437.80] loss=0.99 avg=0.94\n",
            "[496865 | 10438.67] loss=1.00 avg=0.94\n",
            "[496866 | 10439.55] loss=1.01 avg=0.94\n",
            "[496867 | 10440.42] loss=0.94 avg=0.94\n",
            "[496868 | 10441.30] loss=0.98 avg=0.94\n",
            "[496869 | 10442.17] loss=0.98 avg=0.94\n",
            "[496870 | 10443.05] loss=0.83 avg=0.94\n",
            "[496871 | 10443.92] loss=0.67 avg=0.94\n",
            "[496872 | 10444.79] loss=0.80 avg=0.94\n",
            "[496873 | 10445.68] loss=0.92 avg=0.94\n",
            "[496874 | 10446.55] loss=0.86 avg=0.94\n",
            "[496875 | 10447.43] loss=0.93 avg=0.94\n",
            "[496876 | 10448.30] loss=0.87 avg=0.94\n",
            "[496877 | 10449.18] loss=0.68 avg=0.93\n",
            "[496878 | 10450.05] loss=0.67 avg=0.93\n",
            "[496879 | 10450.92] loss=1.00 avg=0.93\n",
            "[496880 | 10451.80] loss=0.80 avg=0.93\n",
            "[496881 | 10452.68] loss=0.88 avg=0.93\n",
            "[496882 | 10453.55] loss=1.10 avg=0.93\n",
            "[496883 | 10454.43] loss=1.04 avg=0.93\n",
            "[496884 | 10455.30] loss=1.00 avg=0.93\n",
            "[496885 | 10456.18] loss=0.79 avg=0.93\n",
            "[496886 | 10457.05] loss=0.94 avg=0.93\n",
            "[496887 | 10457.93] loss=1.07 avg=0.93\n",
            "[496888 | 10458.80] loss=0.95 avg=0.93\n",
            "[496889 | 10459.67] loss=0.87 avg=0.93\n",
            "[496890 | 10460.55] loss=1.16 avg=0.93\n",
            "[496891 | 10461.42] loss=0.94 avg=0.93\n",
            "[496892 | 10462.29] loss=1.03 avg=0.94\n",
            "[496893 | 10463.16] loss=0.92 avg=0.94\n",
            "[496894 | 10464.04] loss=0.94 avg=0.94\n",
            "[496895 | 10464.92] loss=0.96 avg=0.94\n",
            "[496896 | 10465.79] loss=0.88 avg=0.93\n",
            "[496897 | 10466.66] loss=0.88 avg=0.93\n",
            "[496898 | 10467.54] loss=1.08 avg=0.94\n",
            "[496899 | 10468.40] loss=0.71 avg=0.93\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            ". Після контратаки Серени в Курдінії було заарештовано 10 років союзників гармати, особливо для них стратеги дому «» та «Цивільники» (осідали національне поїздо).\n",
            "Після контрабандного значення для позицій військ з панівною адміністративною битвою за відкриття родини 2-го класу почали грати «Чюрко» і збирали домашні кодували їх ворожими плантаціями. Адміністративне повернення заочно в країні були прокладені з новими ударними дошками, 12 осіб європейськими битвами. Практично воно було виявлено так званим «Носець-Нижевським льварем», якщо серед принципів використовувались наприкінці «Носець-Нижевське», за їх припущення зазначившись «Носець-Нижевським льварем» невідомо. Воно також було суттєво поліпшувати карфагеняни, які на тодішньому Курдисти при нанесенні наплачувальної печери, а також проводив щонайменше через два місяці довоєнних приміщень. Радий Курдисти докорінно став і довів великі роки, коли гравця до\n",
            "\n",
            "[496900 | 10486.43] loss=1.02 avg=0.93\n",
            "[496901 | 10487.30] loss=0.82 avg=0.93\n",
            "[496902 | 10488.17] loss=0.95 avg=0.93\n",
            "[496903 | 10489.06] loss=0.73 avg=0.93\n",
            "[496904 | 10489.93] loss=1.24 avg=0.93\n",
            "[496905 | 10490.80] loss=0.80 avg=0.93\n",
            "[496906 | 10491.68] loss=0.91 avg=0.93\n",
            "[496907 | 10492.56] loss=0.96 avg=0.93\n",
            "[496908 | 10493.43] loss=0.84 avg=0.93\n",
            "[496909 | 10494.30] loss=0.72 avg=0.93\n",
            "[496910 | 10495.18] loss=1.02 avg=0.93\n",
            "[496911 | 10496.05] loss=0.90 avg=0.93\n",
            "[496912 | 10496.92] loss=0.83 avg=0.93\n",
            "[496913 | 10497.80] loss=0.98 avg=0.93\n",
            "[496914 | 10498.68] loss=0.98 avg=0.93\n",
            "[496915 | 10499.56] loss=0.93 avg=0.93\n",
            "[496916 | 10500.43] loss=1.03 avg=0.93\n",
            "[496917 | 10501.31] loss=0.82 avg=0.93\n",
            "[496918 | 10502.18] loss=0.90 avg=0.93\n",
            "[496919 | 10503.05] loss=0.90 avg=0.93\n",
            "[496920 | 10503.93] loss=0.92 avg=0.93\n",
            "[496921 | 10504.81] loss=1.10 avg=0.93\n",
            "[496922 | 10505.69] loss=0.96 avg=0.93\n",
            "[496923 | 10506.56] loss=0.94 avg=0.93\n",
            "[496924 | 10507.44] loss=0.97 avg=0.93\n",
            "[496925 | 10508.31] loss=0.72 avg=0.93\n",
            "[496926 | 10509.18] loss=1.14 avg=0.93\n",
            "[496927 | 10510.06] loss=0.85 avg=0.93\n",
            "[496928 | 10510.93] loss=1.05 avg=0.93\n",
            "[496929 | 10511.81] loss=1.04 avg=0.93\n",
            "[496930 | 10512.68] loss=1.05 avg=0.93\n",
            "[496931 | 10513.57] loss=1.03 avg=0.94\n",
            "[496932 | 10514.44] loss=0.81 avg=0.93\n",
            "[496933 | 10515.32] loss=0.94 avg=0.93\n",
            "[496934 | 10516.19] loss=1.10 avg=0.94\n",
            "[496935 | 10517.07] loss=0.96 avg=0.94\n",
            "[496936 | 10517.94] loss=0.99 avg=0.94\n",
            "[496937 | 10518.81] loss=0.88 avg=0.94\n",
            "[496938 | 10519.70] loss=0.85 avg=0.94\n",
            "[496939 | 10520.57] loss=0.89 avg=0.93\n",
            "[496940 | 10521.45] loss=0.87 avg=0.93\n",
            "[496941 | 10522.32] loss=0.80 avg=0.93\n",
            "[496942 | 10523.20] loss=0.99 avg=0.93\n",
            "[496943 | 10524.08] loss=1.22 avg=0.94\n",
            "[496944 | 10524.95] loss=0.91 avg=0.94\n",
            "[496945 | 10525.82] loss=1.10 avg=0.94\n",
            "[496946 | 10526.69] loss=0.78 avg=0.94\n",
            "[496947 | 10527.57] loss=1.11 avg=0.94\n",
            "[496948 | 10528.44] loss=1.05 avg=0.94\n",
            "[496949 | 10529.31] loss=1.08 avg=0.94\n",
            "[496950 | 10530.19] loss=0.85 avg=0.94\n",
            "[496951 | 10531.06] loss=1.01 avg=0.94\n",
            "[496952 | 10531.95] loss=0.88 avg=0.94\n",
            "[496953 | 10532.82] loss=0.88 avg=0.94\n",
            "[496954 | 10533.69] loss=0.86 avg=0.94\n",
            "[496955 | 10534.57] loss=0.91 avg=0.94\n",
            "[496956 | 10535.44] loss=0.98 avg=0.94\n",
            "[496957 | 10536.32] loss=1.01 avg=0.94\n",
            "[496958 | 10537.19] loss=1.08 avg=0.94\n",
            "[496959 | 10538.06] loss=0.85 avg=0.94\n",
            "[496960 | 10538.94] loss=0.95 avg=0.94\n",
            "[496961 | 10539.81] loss=0.92 avg=0.94\n",
            "[496962 | 10540.68] loss=0.83 avg=0.94\n",
            "[496963 | 10541.55] loss=1.04 avg=0.94\n",
            "[496964 | 10542.43] loss=0.80 avg=0.94\n",
            "[496965 | 10543.31] loss=0.85 avg=0.94\n",
            "[496966 | 10544.18] loss=0.86 avg=0.94\n",
            "[496967 | 10545.05] loss=0.91 avg=0.94\n",
            "[496968 | 10545.93] loss=0.93 avg=0.94\n",
            "[496969 | 10546.81] loss=1.02 avg=0.94\n",
            "[496970 | 10547.68] loss=0.87 avg=0.94\n",
            "[496971 | 10548.56] loss=0.91 avg=0.94\n",
            "[496972 | 10549.43] loss=0.94 avg=0.94\n",
            "[496973 | 10550.31] loss=0.92 avg=0.94\n",
            "[496974 | 10551.19] loss=1.08 avg=0.94\n",
            "[496975 | 10552.06] loss=1.08 avg=0.94\n",
            "[496976 | 10552.94] loss=0.83 avg=0.94\n",
            "[496977 | 10553.81] loss=0.94 avg=0.94\n",
            "[496978 | 10554.69] loss=0.97 avg=0.94\n",
            "[496979 | 10555.56] loss=0.71 avg=0.94\n",
            "[496980 | 10556.44] loss=0.75 avg=0.93\n",
            "[496981 | 10557.31] loss=0.85 avg=0.93\n",
            "[496982 | 10558.19] loss=0.94 avg=0.93\n",
            "[496983 | 10559.06] loss=1.10 avg=0.93\n",
            "[496984 | 10559.93] loss=0.96 avg=0.93\n",
            "[496985 | 10560.81] loss=0.97 avg=0.94\n",
            "[496986 | 10561.68] loss=0.82 avg=0.93\n",
            "[496987 | 10562.56] loss=1.03 avg=0.94\n",
            "[496988 | 10563.43] loss=0.96 avg=0.94\n",
            "[496989 | 10564.30] loss=0.94 avg=0.94\n",
            "[496990 | 10565.18] loss=2.36 avg=0.95\n",
            "[496991 | 10566.06] loss=1.40 avg=0.95\n",
            "[496992 | 10566.92] loss=1.05 avg=0.96\n",
            "[496993 | 10567.81] loss=0.96 avg=0.96\n",
            "[496994 | 10568.68] loss=1.02 avg=0.96\n",
            "[496995 | 10569.56] loss=0.87 avg=0.95\n",
            "[496996 | 10570.43] loss=0.87 avg=0.95\n",
            "[496997 | 10571.31] loss=0.81 avg=0.95\n",
            "[496998 | 10572.18] loss=0.83 avg=0.95\n",
            "[496999 | 10573.06] loss=1.04 avg=0.95\n",
            "Saving checkpoint/run1/model-497000\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "ле Воллес Васильович скельні за своїх сухих умовах, головний брат Скіфмартін у Павловському родовищі комплектував з метою створення кісток на території Азербайджану. Заснована при Москві в XV столітті.\n",
            "До 1595-1627 р. мед відбулося перебудрити і взяти у Струдненській Черкаській династії внаслідок дрова і три дні поступово родов. До 1703-1736 р. низку працездатних коней українських і німецьких колон, водяного заповідника (Олексія, Семківці, Самгосяни), пермових, Перманор, можуть скликати книгу�ський розвиток криниць Івасюка. Вміння коней родовищ води належать до конем однойменного книгозбірні, із зачісками довгий час первосвяті в оточуючихових продарах інфекції і водяного живо- та ротових повітряних потреб. Так, до 1728-42 р. на місці навколишнього середовища інфекція входила так звана «Ґундустімська [трощина]».\n",
            "Відомо про п'ять основних праць: Вишневецького району; Нижньоводні падоми; Формлянківський район Фе\n",
            "\n",
            "[497000 | 10595.19] loss=0.89 avg=0.95\n",
            "[497001 | 10596.07] loss=0.92 avg=0.95\n",
            "[497002 | 10596.94] loss=0.22 avg=0.94\n",
            "[497003 | 10597.81] loss=0.97 avg=0.94\n",
            "[497004 | 10598.69] loss=0.91 avg=0.94\n",
            "[497005 | 10599.56] loss=0.84 avg=0.94\n",
            "[497006 | 10600.44] loss=1.13 avg=0.94\n",
            "[497007 | 10601.31] loss=0.86 avg=0.94\n",
            "[497008 | 10602.18] loss=0.73 avg=0.94\n",
            "[497009 | 10603.06] loss=0.89 avg=0.94\n",
            "[497010 | 10603.94] loss=0.63 avg=0.94\n",
            "[497011 | 10604.82] loss=0.78 avg=0.94\n",
            "[497012 | 10605.68] loss=1.01 avg=0.94\n",
            "[497013 | 10606.56] loss=0.91 avg=0.94\n",
            "[497014 | 10607.44] loss=0.80 avg=0.94\n",
            "[497015 | 10608.31] loss=0.85 avg=0.93\n",
            "[497016 | 10609.18] loss=0.89 avg=0.93\n",
            "[497017 | 10610.06] loss=1.28 avg=0.94\n",
            "[497018 | 10610.94] loss=1.03 avg=0.94\n",
            "[497019 | 10611.81] loss=0.94 avg=0.94\n",
            "[497020 | 10612.68] loss=0.82 avg=0.94\n",
            "[497021 | 10613.56] loss=1.19 avg=0.94\n",
            "[497022 | 10614.43] loss=0.93 avg=0.94\n",
            "[497023 | 10615.30] loss=0.91 avg=0.94\n",
            "[497024 | 10616.18] loss=0.88 avg=0.94\n",
            "[497025 | 10617.05] loss=1.02 avg=0.94\n",
            "[497026 | 10617.93] loss=0.80 avg=0.94\n",
            "[497027 | 10618.80] loss=0.86 avg=0.94\n",
            "[497028 | 10619.68] loss=1.03 avg=0.94\n",
            "[497029 | 10620.55] loss=1.00 avg=0.94\n",
            "[497030 | 10621.43] loss=0.79 avg=0.94\n",
            "[497031 | 10622.30] loss=1.03 avg=0.94\n",
            "[497032 | 10623.17] loss=1.04 avg=0.94\n",
            "[497033 | 10624.05] loss=0.93 avg=0.94\n",
            "[497034 | 10624.92] loss=0.83 avg=0.94\n",
            "[497035 | 10625.80] loss=0.96 avg=0.94\n",
            "[497036 | 10626.68] loss=1.02 avg=0.94\n",
            "[497037 | 10627.55] loss=0.92 avg=0.94\n",
            "[497038 | 10628.43] loss=0.90 avg=0.94\n",
            "[497039 | 10629.30] loss=0.88 avg=0.94\n",
            "[497040 | 10630.19] loss=1.14 avg=0.94\n",
            "[497041 | 10631.05] loss=1.03 avg=0.94\n",
            "[497042 | 10631.93] loss=0.91 avg=0.94\n",
            "[497043 | 10632.80] loss=0.76 avg=0.94\n",
            "[497044 | 10633.68] loss=0.77 avg=0.94\n",
            "[497045 | 10634.56] loss=0.85 avg=0.94\n",
            "[497046 | 10635.43] loss=1.02 avg=0.94\n",
            "[497047 | 10636.30] loss=0.90 avg=0.94\n",
            "[497048 | 10637.18] loss=0.89 avg=0.94\n",
            "[497049 | 10638.06] loss=0.81 avg=0.94\n",
            "[497050 | 10638.93] loss=1.02 avg=0.94\n",
            "[497051 | 10639.80] loss=1.12 avg=0.94\n",
            "[497052 | 10640.68] loss=0.81 avg=0.94\n",
            "[497053 | 10641.55] loss=0.94 avg=0.94\n",
            "[497054 | 10642.43] loss=0.87 avg=0.94\n",
            "[497055 | 10643.30] loss=0.82 avg=0.93\n",
            "[497056 | 10644.17] loss=0.81 avg=0.93\n",
            "[497057 | 10645.05] loss=0.72 avg=0.93\n",
            "[497058 | 10645.92] loss=1.03 avg=0.93\n",
            "[497059 | 10646.79] loss=1.04 avg=0.93\n",
            "[497060 | 10647.67] loss=0.93 avg=0.93\n",
            "[497061 | 10648.54] loss=1.09 avg=0.94\n",
            "[497062 | 10649.42] loss=1.03 avg=0.94\n",
            "[497063 | 10650.28] loss=1.73 avg=0.94\n",
            "[497064 | 10651.17] loss=0.90 avg=0.94\n",
            "[497065 | 10652.04] loss=0.94 avg=0.94\n",
            "[497066 | 10652.92] loss=1.08 avg=0.94\n",
            "[497067 | 10653.79] loss=1.09 avg=0.95\n",
            "[497068 | 10654.66] loss=1.03 avg=0.95\n",
            "[497069 | 10655.54] loss=0.90 avg=0.95\n",
            "[497070 | 10656.41] loss=1.06 avg=0.95\n",
            "[497071 | 10657.29] loss=0.83 avg=0.95\n",
            "[497072 | 10658.16] loss=0.94 avg=0.95\n",
            "[497073 | 10659.03] loss=0.95 avg=0.95\n",
            "[497074 | 10659.91] loss=1.01 avg=0.95\n",
            "[497075 | 10660.78] loss=1.10 avg=0.95\n",
            "[497076 | 10661.66] loss=1.08 avg=0.95\n",
            "[497077 | 10662.53] loss=1.03 avg=0.95\n",
            "[497078 | 10663.41] loss=0.88 avg=0.95\n",
            "[497079 | 10664.28] loss=0.88 avg=0.95\n",
            "[497080 | 10665.15] loss=0.94 avg=0.95\n",
            "[497081 | 10666.03] loss=1.06 avg=0.95\n",
            "[497082 | 10666.90] loss=0.85 avg=0.95\n",
            "[497083 | 10667.77] loss=1.01 avg=0.95\n",
            "[497084 | 10668.65] loss=0.81 avg=0.95\n",
            "[497085 | 10669.53] loss=0.85 avg=0.95\n",
            "[497086 | 10670.41] loss=1.22 avg=0.95\n",
            "[497087 | 10671.28] loss=0.92 avg=0.95\n",
            "[497088 | 10672.16] loss=1.00 avg=0.95\n",
            "[497089 | 10673.03] loss=0.78 avg=0.95\n",
            "[497090 | 10673.90] loss=0.91 avg=0.95\n",
            "[497091 | 10674.78] loss=0.87 avg=0.95\n",
            "[497092 | 10675.66] loss=0.99 avg=0.95\n",
            "[497093 | 10676.54] loss=0.84 avg=0.95\n",
            "[497094 | 10677.41] loss=0.86 avg=0.95\n",
            "[497095 | 10678.29] loss=1.08 avg=0.95\n",
            "[497096 | 10679.16] loss=0.86 avg=0.95\n",
            "[497097 | 10680.03] loss=0.90 avg=0.95\n",
            "[497098 | 10680.90] loss=0.95 avg=0.95\n",
            "[497099 | 10681.78] loss=1.04 avg=0.95\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "алежало до розширення них також протеріяних і шляхетських архітекторах Ауком Бруухом та Східному монастиріалу історично концентрованими монастирями політологами. Політичні погляди використовувалися навіть кубинці, кустень, плєкучившись спочинку, некспорт і будування мусульман. Дана смуга постоїть відео, форпостовку, фарватовку і прикрашати спальні смуги вантажі чи пшениці її оренду, допоміжно і їжу. Поради, а також російська провінція переходила в народився на конкурс історії, університет, експозиції, енциклопедії і розширення й дата життя провінції Політбюссельської.\n",
            "У вересні 1944 в Криму виробили місцеві, збільшився конвульський, продукція на 80 відсотків. Всі лінії жовтограми були порівняно з продажу коміксів Талейварського моста, прикраси для адміністративного музею впродовж 1944—1946 років, але згодом з'явився на шість курганів-бронзових гітарних стихій, син меланхолів, головна пам'ятка п\n",
            "\n",
            "[497100 | 10699.75] loss=0.77 avg=0.95\n",
            "[497101 | 10700.63] loss=1.01 avg=0.95\n",
            "[497102 | 10701.50] loss=0.76 avg=0.94\n",
            "[497103 | 10702.38] loss=1.03 avg=0.94\n",
            "[497104 | 10703.25] loss=0.89 avg=0.94\n",
            "[497105 | 10704.12] loss=0.88 avg=0.94\n",
            "[497106 | 10705.00] loss=0.95 avg=0.94\n",
            "[497107 | 10705.87] loss=0.90 avg=0.94\n",
            "[497108 | 10706.75] loss=1.03 avg=0.94\n",
            "[497109 | 10707.62] loss=0.96 avg=0.94\n",
            "[497110 | 10708.50] loss=1.16 avg=0.95\n",
            "[497111 | 10709.37] loss=0.89 avg=0.95\n",
            "[497112 | 10710.24] loss=0.83 avg=0.94\n",
            "[497113 | 10711.12] loss=0.87 avg=0.94\n",
            "[497114 | 10711.99] loss=0.85 avg=0.94\n",
            "[497115 | 10712.86] loss=0.75 avg=0.94\n",
            "[497116 | 10713.74] loss=1.06 avg=0.94\n",
            "[497117 | 10714.61] loss=0.89 avg=0.94\n",
            "[497118 | 10715.49] loss=1.02 avg=0.94\n",
            "[497119 | 10716.36] loss=1.16 avg=0.94\n",
            "[497120 | 10717.24] loss=0.89 avg=0.94\n",
            "[497121 | 10718.11] loss=0.78 avg=0.94\n",
            "[497122 | 10718.98] loss=0.91 avg=0.94\n",
            "[497123 | 10719.86] loss=1.16 avg=0.94\n",
            "[497124 | 10720.73] loss=0.86 avg=0.94\n",
            "[497125 | 10721.60] loss=0.81 avg=0.94\n",
            "[497126 | 10722.48] loss=0.88 avg=0.94\n",
            "[497127 | 10723.36] loss=0.99 avg=0.94\n",
            "[497128 | 10724.23] loss=1.09 avg=0.94\n",
            "[497129 | 10725.10] loss=0.88 avg=0.94\n",
            "[497130 | 10725.98] loss=1.20 avg=0.95\n",
            "[497131 | 10726.86] loss=1.22 avg=0.95\n",
            "[497132 | 10727.73] loss=0.90 avg=0.95\n",
            "[497133 | 10728.60] loss=0.90 avg=0.95\n",
            "[497134 | 10729.48] loss=1.02 avg=0.95\n",
            "[497135 | 10730.35] loss=0.87 avg=0.95\n",
            "[497136 | 10731.22] loss=0.88 avg=0.95\n",
            "[497137 | 10732.10] loss=0.76 avg=0.94\n",
            "[497138 | 10732.97] loss=0.88 avg=0.94\n",
            "[497139 | 10733.84] loss=0.89 avg=0.94\n",
            "[497140 | 10734.71] loss=0.80 avg=0.94\n",
            "[497141 | 10735.59] loss=1.19 avg=0.94\n",
            "[497142 | 10736.47] loss=0.88 avg=0.94\n",
            "[497143 | 10737.34] loss=1.12 avg=0.95\n",
            "[497144 | 10738.22] loss=0.76 avg=0.94\n",
            "[497145 | 10739.09] loss=0.88 avg=0.94\n",
            "[497146 | 10739.97] loss=1.18 avg=0.95\n",
            "[497147 | 10740.85] loss=0.78 avg=0.94\n",
            "[497148 | 10741.72] loss=0.81 avg=0.94\n",
            "[497149 | 10742.59] loss=0.97 avg=0.94\n",
            "[497150 | 10743.47] loss=0.87 avg=0.94\n",
            "[497151 | 10744.35] loss=0.71 avg=0.94\n",
            "[497152 | 10745.23] loss=0.80 avg=0.94\n",
            "[497153 | 10746.10] loss=0.85 avg=0.94\n",
            "[497154 | 10746.97] loss=0.87 avg=0.94\n",
            "[497155 | 10747.85] loss=1.06 avg=0.94\n",
            "[497156 | 10748.72] loss=1.00 avg=0.94\n",
            "[497157 | 10749.59] loss=0.89 avg=0.94\n",
            "[497158 | 10750.47] loss=0.98 avg=0.94\n",
            "[497159 | 10751.35] loss=1.18 avg=0.94\n",
            "[497160 | 10752.22] loss=1.18 avg=0.94\n",
            "[497161 | 10753.09] loss=0.84 avg=0.94\n",
            "[497162 | 10753.97] loss=0.94 avg=0.94\n",
            "[497163 | 10754.84] loss=1.20 avg=0.94\n",
            "[497164 | 10755.72] loss=1.03 avg=0.95\n",
            "[497165 | 10756.59] loss=1.04 avg=0.95\n",
            "[497166 | 10757.47] loss=0.80 avg=0.95\n",
            "[497167 | 10758.34] loss=0.90 avg=0.94\n",
            "[497168 | 10759.21] loss=0.84 avg=0.94\n",
            "[497169 | 10760.09] loss=0.89 avg=0.94\n",
            "[497170 | 10760.96] loss=0.99 avg=0.94\n",
            "[497171 | 10761.84] loss=0.96 avg=0.94\n",
            "[497172 | 10762.71] loss=0.82 avg=0.94\n",
            "[497173 | 10763.58] loss=1.03 avg=0.94\n",
            "[497174 | 10764.46] loss=0.85 avg=0.94\n",
            "[497175 | 10765.33] loss=1.02 avg=0.94\n",
            "[497176 | 10766.20] loss=0.85 avg=0.94\n",
            "[497177 | 10767.08] loss=0.90 avg=0.94\n",
            "[497178 | 10767.95] loss=1.02 avg=0.94\n",
            "[497179 | 10768.83] loss=1.12 avg=0.94\n",
            "[497180 | 10769.70] loss=1.04 avg=0.95\n",
            "[497181 | 10770.58] loss=0.89 avg=0.94\n",
            "[497182 | 10771.45] loss=1.28 avg=0.95\n",
            "[497183 | 10772.33] loss=0.97 avg=0.95\n",
            "[497184 | 10773.20] loss=0.97 avg=0.95\n",
            "[497185 | 10774.08] loss=0.94 avg=0.95\n",
            "[497186 | 10774.96] loss=0.86 avg=0.95\n",
            "[497187 | 10775.83] loss=0.86 avg=0.95\n",
            "[497188 | 10776.71] loss=0.84 avg=0.95\n",
            "[497189 | 10777.58] loss=0.76 avg=0.94\n",
            "[497190 | 10778.45] loss=0.93 avg=0.94\n",
            "[497191 | 10779.33] loss=0.78 avg=0.94\n",
            "[497192 | 10780.20] loss=0.89 avg=0.94\n",
            "[497193 | 10781.08] loss=0.93 avg=0.94\n",
            "[497194 | 10781.95] loss=0.93 avg=0.94\n",
            "[497195 | 10782.82] loss=1.07 avg=0.94\n",
            "[497196 | 10783.70] loss=1.04 avg=0.94\n",
            "[497197 | 10784.57] loss=0.78 avg=0.94\n",
            "[497198 | 10785.45] loss=1.05 avg=0.94\n",
            "[497199 | 10786.33] loss=0.95 avg=0.94\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "судочинства, що підтримує дотримання цієї ідеї.\n",
            "Доцент вищих інфекцій, розроблений разом з Кауфманом під загостренням застосування виробництва осіб означає, що пов'язане з тим, не діє змінного типу, через них поява слова має поширення будь-яке попитомовне чи політичне рівняння шляхом впровадження з інфекційного колумбійського мовлення . Оскільки з блискучих плодових, ЦОС можуть бути спрямовані (всі інші) зв'язки лише з початком поширення інфекцій як рівні пов'язаної з плутаниною 1 травня ще на початку Ради має намір мати відтоді універсал.\n",
            "При поширенні інфекційного колумбійського мовлення і натурних С. Джонського, його необхідної експертизи обмеженої державної інфраструктури доступні інфекційні експедиції, розроблені у сферу масової інформації, можливо, що жоден рівень між державною інформаційною інформацією статусу імперської мови визначає те, що перет\n",
            "\n",
            "[497200 | 10804.33] loss=0.98 avg=0.94\n",
            "[497201 | 10805.21] loss=0.89 avg=0.94\n",
            "[497202 | 10806.09] loss=0.89 avg=0.94\n",
            "[497203 | 10806.96] loss=1.03 avg=0.94\n",
            "[497204 | 10807.84] loss=1.03 avg=0.94\n",
            "[497205 | 10808.72] loss=1.02 avg=0.94\n",
            "[497206 | 10809.60] loss=0.96 avg=0.94\n",
            "[497207 | 10810.47] loss=0.74 avg=0.94\n",
            "[497208 | 10811.35] loss=0.94 avg=0.94\n",
            "[497209 | 10812.22] loss=0.91 avg=0.94\n",
            "[497210 | 10813.10] loss=0.74 avg=0.94\n",
            "[497211 | 10813.97] loss=0.81 avg=0.94\n",
            "[497212 | 10814.85] loss=1.18 avg=0.94\n",
            "[497213 | 10815.72] loss=0.83 avg=0.94\n",
            "[497214 | 10816.60] loss=0.90 avg=0.94\n",
            "[497215 | 10817.47] loss=0.98 avg=0.94\n",
            "[497216 | 10818.34] loss=0.92 avg=0.94\n",
            "[497217 | 10819.21] loss=1.02 avg=0.94\n",
            "[497218 | 10820.08] loss=0.76 avg=0.94\n",
            "[497219 | 10820.96] loss=1.04 avg=0.94\n",
            "[497220 | 10821.84] loss=0.96 avg=0.94\n",
            "[497221 | 10822.72] loss=0.78 avg=0.94\n",
            "[497222 | 10823.60] loss=1.00 avg=0.94\n",
            "[497223 | 10824.47] loss=0.94 avg=0.94\n",
            "[497224 | 10825.35] loss=1.06 avg=0.94\n",
            "[497225 | 10826.22] loss=1.03 avg=0.94\n",
            "[497226 | 10827.10] loss=1.03 avg=0.94\n",
            "[497227 | 10827.98] loss=0.90 avg=0.94\n",
            "[497228 | 10828.85] loss=0.73 avg=0.94\n",
            "[497229 | 10829.73] loss=0.95 avg=0.94\n",
            "[497230 | 10830.60] loss=0.82 avg=0.94\n",
            "[497231 | 10831.47] loss=0.96 avg=0.94\n",
            "[497232 | 10832.35] loss=0.98 avg=0.94\n",
            "[497233 | 10833.22] loss=0.81 avg=0.94\n",
            "[497234 | 10834.10] loss=0.94 avg=0.94\n",
            "[497235 | 10834.97] loss=0.78 avg=0.94\n",
            "[497236 | 10835.85] loss=0.91 avg=0.94\n",
            "[497237 | 10836.73] loss=0.77 avg=0.93\n",
            "[497238 | 10837.60] loss=1.01 avg=0.94\n",
            "[497239 | 10838.47] loss=0.98 avg=0.94\n",
            "[497240 | 10839.35] loss=0.79 avg=0.93\n",
            "[497241 | 10840.22] loss=0.87 avg=0.93\n",
            "[497242 | 10841.09] loss=0.85 avg=0.93\n",
            "[497243 | 10841.97] loss=0.79 avg=0.93\n",
            "[497244 | 10842.85] loss=1.20 avg=0.93\n",
            "[497245 | 10843.72] loss=1.08 avg=0.94\n",
            "[497246 | 10844.60] loss=0.81 avg=0.93\n",
            "[497247 | 10845.47] loss=0.87 avg=0.93\n",
            "[497248 | 10846.35] loss=0.93 avg=0.93\n",
            "[497249 | 10847.22] loss=0.88 avg=0.93\n",
            "[497250 | 10848.09] loss=0.97 avg=0.93\n",
            "[497251 | 10848.97] loss=0.76 avg=0.93\n",
            "[497252 | 10849.84] loss=0.93 avg=0.93\n",
            "[497253 | 10850.71] loss=0.82 avg=0.93\n",
            "[497254 | 10851.59] loss=0.83 avg=0.93\n",
            "[497255 | 10852.46] loss=0.89 avg=0.93\n",
            "[497256 | 10853.33] loss=0.86 avg=0.93\n",
            "[497257 | 10854.20] loss=0.75 avg=0.93\n",
            "[497258 | 10855.07] loss=0.86 avg=0.93\n",
            "[497259 | 10855.95] loss=0.95 avg=0.93\n",
            "[497260 | 10856.83] loss=0.70 avg=0.92\n",
            "[497261 | 10857.71] loss=0.97 avg=0.92\n",
            "[497262 | 10858.58] loss=1.03 avg=0.93\n",
            "[497263 | 10859.45] loss=0.84 avg=0.92\n",
            "[497264 | 10860.33] loss=0.91 avg=0.92\n",
            "[497265 | 10861.20] loss=1.03 avg=0.93\n",
            "[497266 | 10862.08] loss=0.78 avg=0.92\n",
            "[497267 | 10862.95] loss=1.06 avg=0.93\n",
            "[497268 | 10863.82] loss=0.88 avg=0.93\n",
            "[497269 | 10864.70] loss=0.81 avg=0.92\n",
            "[497270 | 10865.57] loss=1.02 avg=0.92\n",
            "[497271 | 10866.45] loss=1.05 avg=0.93\n",
            "[497272 | 10867.32] loss=1.20 avg=0.93\n",
            "[497273 | 10868.20] loss=1.24 avg=0.93\n",
            "[497274 | 10869.07] loss=1.03 avg=0.93\n",
            "[497275 | 10869.94] loss=1.11 avg=0.93\n",
            "[497276 | 10870.82] loss=0.67 avg=0.93\n",
            "[497277 | 10871.70] loss=0.87 avg=0.93\n",
            "[497278 | 10872.57] loss=0.88 avg=0.93\n",
            "[497279 | 10873.44] loss=0.66 avg=0.93\n",
            "[497280 | 10874.32] loss=0.83 avg=0.93\n",
            "[497281 | 10875.20] loss=0.91 avg=0.93\n",
            "[497282 | 10876.07] loss=1.18 avg=0.93\n",
            "[497283 | 10876.95] loss=0.70 avg=0.93\n",
            "[497284 | 10877.83] loss=0.96 avg=0.93\n",
            "[497285 | 10878.70] loss=0.65 avg=0.93\n",
            "[497286 | 10879.57] loss=1.02 avg=0.93\n",
            "[497287 | 10880.45] loss=0.82 avg=0.93\n",
            "[497288 | 10881.33] loss=0.94 avg=0.93\n",
            "[497289 | 10882.20] loss=0.84 avg=0.92\n",
            "[497290 | 10883.08] loss=0.97 avg=0.92\n",
            "[497291 | 10883.95] loss=0.90 avg=0.92\n",
            "[497292 | 10884.83] loss=1.03 avg=0.93\n",
            "[497293 | 10885.70] loss=0.93 avg=0.93\n",
            "[497294 | 10886.58] loss=0.83 avg=0.92\n",
            "[497295 | 10887.45] loss=1.00 avg=0.93\n",
            "[497296 | 10888.32] loss=0.93 avg=0.93\n",
            "[497297 | 10889.20] loss=0.82 avg=0.92\n",
            "[497298 | 10890.08] loss=0.82 avg=0.92\n",
            "[497299 | 10890.96] loss=0.88 avg=0.92\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "ргації» вет. Голова Патріаршого, брата Літтла, Кварта у своїй українській семінарії.\n",
            "Близько 18 травня призначено митрополитом (Боярча) Олетана Кузьмовича, Ольга Дмитровича, Богослова Петра І., Митрополита Семенєва Оглядача у своїй законі редакційної влади.\n",
            "Землі в світі закінчують Немирі Богутним православним Нобинським хіміко-визвольним рухом та землі вікон і почати поводження богослужінь за зменшену долю позашляхічного роду жінок (не. Богутні сина російського рисунків). Землі обох нарахного шляху були прикриті точки в Нобині, а зі степової сім'ї весь історію вікна — до її розколу і т. д. — хіміко-визвольним змаганням та народом за зменшеного роду. Деякі освітні медичні інтелектуали проводять у цьому районі.\n",
            "Поширеній військово-спортивний — поширеній ветеринарний сільськогосподарський гуртожитель, заснований на пневматичному повітрі. Матеріально виявилася невдала територія ветеринарної та по\n",
            "\n",
            "[497300 | 10908.94] loss=0.82 avg=0.92\n",
            "[497301 | 10909.81] loss=0.93 avg=0.92\n",
            "[497302 | 10910.69] loss=0.99 avg=0.92\n",
            "[497303 | 10911.56] loss=0.74 avg=0.92\n",
            "[497304 | 10912.44] loss=0.82 avg=0.92\n",
            "[497305 | 10913.31] loss=0.89 avg=0.92\n",
            "[497306 | 10914.19] loss=0.95 avg=0.92\n",
            "[497307 | 10915.06] loss=0.98 avg=0.92\n",
            "[497308 | 10915.93] loss=0.78 avg=0.92\n",
            "[497309 | 10916.81] loss=0.93 avg=0.92\n",
            "[497310 | 10917.68] loss=0.76 avg=0.92\n",
            "[497311 | 10918.56] loss=0.96 avg=0.92\n",
            "[497312 | 10919.43] loss=1.06 avg=0.92\n",
            "[497313 | 10920.31] loss=0.88 avg=0.92\n",
            "[497314 | 10921.19] loss=0.87 avg=0.92\n",
            "[497315 | 10922.06] loss=0.87 avg=0.92\n",
            "[497316 | 10922.94] loss=0.91 avg=0.92\n",
            "[497317 | 10923.81] loss=0.95 avg=0.92\n",
            "[497318 | 10924.69] loss=0.88 avg=0.92\n",
            "[497319 | 10925.56] loss=1.00 avg=0.92\n",
            "[497320 | 10926.44] loss=1.10 avg=0.92\n",
            "[497321 | 10927.32] loss=0.84 avg=0.92\n",
            "[497322 | 10928.20] loss=0.92 avg=0.92\n",
            "[497323 | 10929.07] loss=1.06 avg=0.92\n",
            "[497324 | 10929.95] loss=0.91 avg=0.92\n",
            "[497325 | 10930.82] loss=0.98 avg=0.92\n",
            "[497326 | 10931.69] loss=0.84 avg=0.92\n",
            "[497327 | 10932.56] loss=1.11 avg=0.92\n",
            "[497328 | 10933.44] loss=0.90 avg=0.92\n",
            "[497329 | 10934.30] loss=1.03 avg=0.92\n",
            "[497330 | 10935.19] loss=0.89 avg=0.92\n",
            "[497331 | 10936.06] loss=0.80 avg=0.92\n",
            "[497332 | 10936.93] loss=1.06 avg=0.92\n",
            "[497333 | 10937.81] loss=0.94 avg=0.92\n",
            "[497334 | 10938.69] loss=0.84 avg=0.92\n",
            "[497335 | 10939.57] loss=1.05 avg=0.92\n",
            "[497336 | 10940.43] loss=0.83 avg=0.92\n",
            "[497337 | 10941.32] loss=0.95 avg=0.92\n",
            "[497338 | 10942.20] loss=0.87 avg=0.92\n",
            "[497339 | 10943.07] loss=0.91 avg=0.92\n",
            "[497340 | 10943.94] loss=0.93 avg=0.92\n",
            "[497341 | 10944.82] loss=0.89 avg=0.92\n",
            "[497342 | 10945.70] loss=0.86 avg=0.92\n",
            "[497343 | 10946.57] loss=0.69 avg=0.92\n",
            "[497344 | 10947.45] loss=1.09 avg=0.92\n",
            "[497345 | 10948.32] loss=0.85 avg=0.92\n",
            "[497346 | 10949.20] loss=1.05 avg=0.92\n",
            "[497347 | 10950.07] loss=0.81 avg=0.92\n",
            "[497348 | 10950.94] loss=1.34 avg=0.92\n",
            "[497349 | 10951.82] loss=0.94 avg=0.92\n",
            "[497350 | 10952.69] loss=0.77 avg=0.92\n",
            "[497351 | 10953.57] loss=1.09 avg=0.92\n",
            "[497352 | 10954.44] loss=0.76 avg=0.92\n",
            "[497353 | 10955.31] loss=0.81 avg=0.92\n",
            "[497354 | 10956.19] loss=0.66 avg=0.92\n",
            "[497355 | 10957.06] loss=0.94 avg=0.92\n",
            "[497356 | 10957.94] loss=0.95 avg=0.92\n",
            "[497357 | 10958.81] loss=1.18 avg=0.92\n",
            "[497358 | 10959.69] loss=0.97 avg=0.92\n",
            "[497359 | 10960.56] loss=0.85 avg=0.92\n",
            "[497360 | 10961.43] loss=1.00 avg=0.92\n",
            "[497361 | 10962.31] loss=0.85 avg=0.92\n",
            "[497362 | 10963.18] loss=1.07 avg=0.92\n",
            "[497363 | 10964.06] loss=0.93 avg=0.92\n",
            "[497364 | 10964.93] loss=0.83 avg=0.92\n",
            "[497365 | 10965.80] loss=0.74 avg=0.92\n",
            "[497366 | 10966.68] loss=1.00 avg=0.92\n",
            "[497367 | 10967.55] loss=1.14 avg=0.92\n",
            "[497368 | 10968.42] loss=0.91 avg=0.92\n",
            "[497369 | 10969.30] loss=0.93 avg=0.92\n",
            "[497370 | 10970.16] loss=0.85 avg=0.92\n",
            "[497371 | 10971.04] loss=1.11 avg=0.92\n",
            "[497372 | 10971.91] loss=0.83 avg=0.92\n",
            "[497373 | 10972.78] loss=0.96 avg=0.92\n",
            "[497374 | 10973.66] loss=0.92 avg=0.92\n",
            "[497375 | 10974.53] loss=0.90 avg=0.92\n",
            "[497376 | 10975.40] loss=0.96 avg=0.92\n",
            "[497377 | 10976.28] loss=0.98 avg=0.92\n",
            "[497378 | 10977.16] loss=0.84 avg=0.92\n",
            "[497379 | 10978.03] loss=0.94 avg=0.92\n",
            "[497380 | 10978.90] loss=0.79 avg=0.92\n",
            "[497381 | 10979.77] loss=0.71 avg=0.92\n",
            "[497382 | 10980.65] loss=1.21 avg=0.92\n",
            "[497383 | 10981.52] loss=0.90 avg=0.92\n",
            "[497384 | 10982.40] loss=0.75 avg=0.92\n",
            "[497385 | 10983.27] loss=0.84 avg=0.92\n",
            "[497386 | 10984.15] loss=0.91 avg=0.92\n",
            "[497387 | 10985.02] loss=0.78 avg=0.92\n",
            "[497388 | 10985.90] loss=0.82 avg=0.92\n",
            "[497389 | 10986.77] loss=0.96 avg=0.92\n",
            "[497390 | 10987.64] loss=0.92 avg=0.92\n",
            "[497391 | 10988.52] loss=0.94 avg=0.92\n",
            "[497392 | 10989.39] loss=0.86 avg=0.92\n",
            "[497393 | 10990.26] loss=0.97 avg=0.92\n",
            "[497394 | 10991.14] loss=1.09 avg=0.92\n",
            "[497395 | 10992.02] loss=0.71 avg=0.92\n",
            "[497396 | 10992.89] loss=0.79 avg=0.92\n",
            "[497397 | 10993.77] loss=0.93 avg=0.92\n",
            "[497398 | 10994.64] loss=0.85 avg=0.92\n",
            "[497399 | 10995.52] loss=0.83 avg=0.92\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "а нарешті. «Ексцентричний простір» говорить про своє інтереси: можна переклав Елізабет украй написав, що Елізабет використовує ранні обрізи вад світу. Світ сьогодні використовував літературний творчий літературний текст Пікантси Розенента; ці обрізи використовують при веденні поліфейсько-інновацій гідних елементів. Для обряду Елізабета подібного гарматою подібного текста, також обрізом синдрому, співзвучать численні майстерно сцени, мутагдиські фізики, виконання індивідів арійського одягу, а також украй.\n",
            "Вони зауважували ці пірамід тим, що слід докласти з тим, щоб не захистити гідних впливів і обмежити наслідки пряних. 19 листопада Неаполітанська Народна Республіка була захоплена відродженням експертів на світовий ринок або підтвердженою відродженою фауністською мантією. Через те, що в цьому роді всі елементи кар'єряні захоплені фауни, які перевозили синдром Анторова — це осно�\n",
            "\n",
            "[497400 | 11013.60] loss=0.89 avg=0.92\n",
            "[497401 | 11014.48] loss=1.04 avg=0.92\n",
            "[497402 | 11015.35] loss=0.80 avg=0.92\n",
            "[497403 | 11016.22] loss=0.91 avg=0.92\n",
            "[497404 | 11017.10] loss=0.99 avg=0.92\n",
            "[497405 | 11017.97] loss=0.98 avg=0.92\n",
            "[497406 | 11018.85] loss=1.15 avg=0.92\n",
            "[497407 | 11019.72] loss=0.88 avg=0.92\n",
            "[497408 | 11020.59] loss=0.89 avg=0.92\n",
            "[497409 | 11021.47] loss=1.10 avg=0.92\n",
            "[497410 | 11022.34] loss=1.09 avg=0.92\n",
            "[497411 | 11023.22] loss=0.92 avg=0.92\n",
            "[497412 | 11024.09] loss=0.83 avg=0.92\n",
            "[497413 | 11024.97] loss=0.97 avg=0.92\n",
            "[497414 | 11025.84] loss=0.85 avg=0.92\n",
            "[497415 | 11026.71] loss=0.88 avg=0.92\n",
            "[497416 | 11027.58] loss=0.88 avg=0.92\n",
            "[497417 | 11028.45] loss=0.75 avg=0.92\n",
            "[497418 | 11029.32] loss=0.85 avg=0.92\n",
            "[497419 | 11030.20] loss=1.06 avg=0.92\n",
            "[497420 | 11031.08] loss=0.88 avg=0.92\n",
            "[497421 | 11031.96] loss=0.72 avg=0.92\n",
            "[497422 | 11032.83] loss=0.88 avg=0.92\n",
            "[497423 | 11033.71] loss=0.80 avg=0.92\n",
            "[497424 | 11034.59] loss=0.82 avg=0.91\n",
            "[497425 | 11035.46] loss=0.87 avg=0.91\n",
            "[497426 | 11036.34] loss=0.97 avg=0.91\n",
            "[497427 | 11037.21] loss=0.87 avg=0.91\n",
            "[497428 | 11038.08] loss=0.90 avg=0.91\n",
            "[497429 | 11038.96] loss=0.89 avg=0.91\n",
            "[497430 | 11039.83] loss=0.88 avg=0.91\n",
            "[497431 | 11040.71] loss=0.87 avg=0.91\n",
            "[497432 | 11041.58] loss=1.23 avg=0.92\n",
            "[497433 | 11042.46] loss=0.72 avg=0.91\n",
            "[497434 | 11043.33] loss=0.79 avg=0.91\n",
            "[497435 | 11044.21] loss=0.79 avg=0.91\n",
            "[497436 | 11045.08] loss=0.83 avg=0.91\n",
            "[497437 | 11045.96] loss=1.29 avg=0.91\n",
            "[497438 | 11046.83] loss=1.05 avg=0.92\n",
            "[497439 | 11047.70] loss=0.98 avg=0.92\n",
            "[497440 | 11048.58] loss=0.99 avg=0.92\n",
            "[497441 | 11049.46] loss=0.85 avg=0.92\n",
            "[497442 | 11050.33] loss=0.90 avg=0.92\n",
            "[497443 | 11051.20] loss=1.07 avg=0.92\n",
            "[497444 | 11052.08] loss=1.08 avg=0.92\n",
            "[497445 | 11052.96] loss=0.91 avg=0.92\n",
            "[497446 | 11053.83] loss=1.12 avg=0.92\n",
            "[497447 | 11054.70] loss=0.83 avg=0.92\n",
            "[497448 | 11055.58] loss=1.00 avg=0.92\n",
            "[497449 | 11056.45] loss=1.13 avg=0.92\n",
            "[497450 | 11057.32] loss=0.97 avg=0.92\n",
            "[497451 | 11058.19] loss=1.23 avg=0.93\n",
            "[497452 | 11059.07] loss=0.58 avg=0.92\n",
            "[497453 | 11059.94] loss=0.96 avg=0.92\n",
            "[497454 | 11060.82] loss=0.95 avg=0.92\n",
            "[497455 | 11061.70] loss=0.86 avg=0.92\n",
            "[497456 | 11062.57] loss=0.83 avg=0.92\n",
            "[497457 | 11063.45] loss=0.77 avg=0.92\n",
            "[497458 | 11064.32] loss=0.97 avg=0.92\n",
            "[497459 | 11065.19] loss=1.13 avg=0.92\n",
            "[497460 | 11066.07] loss=0.81 avg=0.92\n",
            "[497461 | 11066.94] loss=1.06 avg=0.92\n",
            "[497462 | 11067.82] loss=0.82 avg=0.92\n",
            "[497463 | 11068.69] loss=0.89 avg=0.92\n",
            "[497464 | 11069.57] loss=0.79 avg=0.92\n",
            "[497465 | 11070.44] loss=0.94 avg=0.92\n",
            "[497466 | 11071.31] loss=1.11 avg=0.92\n",
            "[497467 | 11072.18] loss=1.20 avg=0.93\n",
            "[497468 | 11073.05] loss=0.76 avg=0.92\n",
            "[497469 | 11073.93] loss=1.05 avg=0.93\n",
            "[497470 | 11074.81] loss=0.77 avg=0.92\n",
            "[497471 | 11075.68] loss=0.85 avg=0.92\n",
            "[497472 | 11076.56] loss=0.93 avg=0.92\n",
            "[497473 | 11077.43] loss=1.04 avg=0.92\n",
            "[497474 | 11078.30] loss=1.07 avg=0.93\n",
            "[497475 | 11079.17] loss=1.07 avg=0.93\n",
            "[497476 | 11080.04] loss=1.10 avg=0.93\n",
            "[497477 | 11080.93] loss=0.51 avg=0.93\n",
            "[497478 | 11081.80] loss=1.19 avg=0.93\n",
            "[497479 | 11082.68] loss=0.72 avg=0.93\n",
            "[497480 | 11083.55] loss=0.84 avg=0.92\n",
            "[497481 | 11084.43] loss=0.92 avg=0.92\n",
            "[497482 | 11085.31] loss=0.97 avg=0.93\n",
            "[497483 | 11086.18] loss=0.92 avg=0.93\n",
            "[497484 | 11087.06] loss=0.70 avg=0.92\n",
            "[497485 | 11087.94] loss=0.93 avg=0.92\n",
            "[497486 | 11088.81] loss=1.34 avg=0.93\n",
            "[497487 | 11089.68] loss=1.01 avg=0.93\n",
            "[497488 | 11090.56] loss=0.95 avg=0.93\n",
            "[497489 | 11091.43] loss=1.10 avg=0.93\n",
            "[497490 | 11092.31] loss=0.90 avg=0.93\n",
            "[497491 | 11093.19] loss=0.91 avg=0.93\n",
            "[497492 | 11094.07] loss=1.09 avg=0.93\n",
            "[497493 | 11094.94] loss=0.77 avg=0.93\n",
            "[497494 | 11095.81] loss=0.93 avg=0.93\n",
            "[497495 | 11096.69] loss=0.87 avg=0.93\n",
            "[497496 | 11097.57] loss=0.84 avg=0.93\n",
            "[497497 | 11098.45] loss=1.51 avg=0.93\n",
            "[497498 | 11099.32] loss=0.94 avg=0.93\n",
            "[497499 | 11100.20] loss=0.97 avg=0.93\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "�, Дону залишається професурами. Водночас провів рух у явищі палаців у Праксі. Можливо, перепродавання групово поєднувалося у гіроскопічно і по собі, можливо, так само, як Дрейфаме». іноді Бог, вода Добролюбійна гіркотинопрофілінгів набирала від Лазнефянь, деякою живою поєднавшись з продюсерською доброю поєднаною в одному дереві. Ці речі водойми можна здобути у національних подорожах. У Праксі «Суперконтины» набирали давньосередньо і великого шпиталю фосфатів. Справжні архітектурні елементи — Відмідат Тепсі Боратден (Тепсі звернення до колишнього подорожі Тирининської козаки, псевдонаукова глибина Башкортостанової експедиції, зверху Тепсі Бор в м. Богодові, у Гінсько-Татрен-Татар) набирались з його початку Богодухінатів. Серед його населених облаштувались. р. С. І. Шухевича (\"Павло Т. Мишевець\", 1887, Р.Циклонською, Цельсдейшвою). Водойми в Інниць — Лесі Відпус (., 1898), Інниць.\n",
            "Дослідження гіропрофі\n",
            "\n",
            "[497500 | 11118.42] loss=0.84 avg=0.93\n",
            "[497501 | 11119.30] loss=1.14 avg=0.94\n",
            "[497502 | 11120.18] loss=1.01 avg=0.94\n",
            "[497503 | 11121.05] loss=0.91 avg=0.94\n",
            "[497504 | 11121.92] loss=0.86 avg=0.94\n",
            "[497505 | 11122.80] loss=0.92 avg=0.93\n",
            "[497506 | 11123.68] loss=1.03 avg=0.94\n",
            "[497507 | 11124.56] loss=0.83 avg=0.93\n",
            "[497508 | 11125.43] loss=1.03 avg=0.94\n",
            "[497509 | 11126.30] loss=0.68 avg=0.93\n",
            "[497510 | 11127.19] loss=0.81 avg=0.93\n",
            "[497511 | 11128.07] loss=1.10 avg=0.93\n",
            "[497512 | 11128.94] loss=0.93 avg=0.93\n",
            "[497513 | 11129.82] loss=0.90 avg=0.93\n",
            "[497514 | 11130.70] loss=0.91 avg=0.93\n",
            "[497515 | 11131.58] loss=0.87 avg=0.93\n",
            "[497516 | 11132.45] loss=0.97 avg=0.93\n",
            "[497517 | 11133.32] loss=0.82 avg=0.93\n",
            "[497518 | 11134.20] loss=0.92 avg=0.93\n",
            "[497519 | 11135.07] loss=0.98 avg=0.93\n",
            "[497520 | 11135.96] loss=1.00 avg=0.93\n",
            "[497521 | 11136.84] loss=1.28 avg=0.94\n",
            "[497522 | 11137.71] loss=0.84 avg=0.94\n",
            "[497523 | 11138.58] loss=0.80 avg=0.93\n",
            "[497524 | 11139.46] loss=0.91 avg=0.93\n",
            "[497525 | 11140.34] loss=0.79 avg=0.93\n",
            "[497526 | 11141.20] loss=1.01 avg=0.93\n",
            "[497527 | 11142.08] loss=0.89 avg=0.93\n",
            "[497528 | 11142.95] loss=1.02 avg=0.93\n",
            "[497529 | 11143.83] loss=0.98 avg=0.93\n",
            "[497530 | 11144.71] loss=0.89 avg=0.93\n",
            "[497531 | 11145.58] loss=0.95 avg=0.93\n",
            "[497532 | 11146.46] loss=0.97 avg=0.93\n",
            "[497533 | 11147.33] loss=0.94 avg=0.93\n",
            "[497534 | 11148.21] loss=0.71 avg=0.93\n",
            "[497535 | 11149.09] loss=0.89 avg=0.93\n",
            "[497536 | 11149.97] loss=0.78 avg=0.93\n",
            "[497537 | 11150.84] loss=0.74 avg=0.93\n",
            "[497538 | 11151.72] loss=0.90 avg=0.93\n",
            "[497539 | 11152.59] loss=0.96 avg=0.93\n",
            "[497540 | 11153.47] loss=0.71 avg=0.93\n",
            "[497541 | 11154.35] loss=0.90 avg=0.93\n",
            "[497542 | 11155.22] loss=0.85 avg=0.92\n",
            "[497543 | 11156.10] loss=0.93 avg=0.92\n",
            "[497544 | 11156.97] loss=0.78 avg=0.92\n",
            "[497545 | 11157.85] loss=0.79 avg=0.92\n",
            "[497546 | 11158.72] loss=1.04 avg=0.92\n",
            "[497547 | 11159.60] loss=0.90 avg=0.92\n",
            "[497548 | 11160.47] loss=0.78 avg=0.92\n",
            "[497549 | 11161.35] loss=0.78 avg=0.92\n",
            "[497550 | 11162.23] loss=0.95 avg=0.92\n",
            "[497551 | 11163.10] loss=1.01 avg=0.92\n",
            "[497552 | 11163.98] loss=0.96 avg=0.92\n",
            "[497553 | 11164.85] loss=0.76 avg=0.92\n",
            "[497554 | 11165.72] loss=1.00 avg=0.92\n",
            "[497555 | 11166.60] loss=1.01 avg=0.92\n",
            "[497556 | 11167.47] loss=1.03 avg=0.92\n",
            "[497557 | 11168.35] loss=1.02 avg=0.92\n",
            "[497558 | 11169.22] loss=0.91 avg=0.92\n",
            "[497559 | 11170.10] loss=0.97 avg=0.92\n",
            "[497560 | 11170.98] loss=0.92 avg=0.92\n",
            "[497561 | 11171.86] loss=1.20 avg=0.93\n",
            "[497562 | 11172.73] loss=1.06 avg=0.93\n",
            "[497563 | 11173.60] loss=0.94 avg=0.93\n",
            "[497564 | 11174.48] loss=1.06 avg=0.93\n",
            "[497565 | 11175.35] loss=0.86 avg=0.93\n",
            "[497566 | 11176.23] loss=1.00 avg=0.93\n",
            "[497567 | 11177.10] loss=1.10 avg=0.93\n",
            "[497568 | 11177.98] loss=1.15 avg=0.93\n",
            "[497569 | 11178.85] loss=0.99 avg=0.93\n",
            "[497570 | 11179.73] loss=0.79 avg=0.93\n",
            "[497571 | 11180.61] loss=0.90 avg=0.93\n",
            "[497572 | 11181.48] loss=1.02 avg=0.93\n",
            "[497573 | 11182.35] loss=0.92 avg=0.93\n",
            "[497574 | 11183.22] loss=0.90 avg=0.93\n",
            "[497575 | 11184.10] loss=1.02 avg=0.93\n",
            "[497576 | 11184.97] loss=0.89 avg=0.93\n",
            "[497577 | 11185.85] loss=0.79 avg=0.93\n",
            "[497578 | 11186.72] loss=0.81 avg=0.93\n",
            "[497579 | 11187.59] loss=0.82 avg=0.93\n",
            "[497580 | 11188.47] loss=0.84 avg=0.93\n",
            "[497581 | 11189.35] loss=0.97 avg=0.93\n",
            "[497582 | 11190.22] loss=0.87 avg=0.93\n",
            "[497583 | 11191.09] loss=1.06 avg=0.93\n",
            "[497584 | 11191.96] loss=0.95 avg=0.93\n",
            "[497585 | 11192.83] loss=0.98 avg=0.93\n",
            "[497586 | 11193.72] loss=1.08 avg=0.93\n",
            "[497587 | 11194.58] loss=0.98 avg=0.93\n",
            "[497588 | 11195.46] loss=0.91 avg=0.93\n",
            "[497589 | 11196.34] loss=0.79 avg=0.93\n",
            "[497590 | 11197.21] loss=0.91 avg=0.93\n",
            "[497591 | 11198.08] loss=1.09 avg=0.93\n",
            "[497592 | 11198.95] loss=1.11 avg=0.93\n",
            "[497593 | 11199.84] loss=1.06 avg=0.93\n",
            "[497594 | 11200.72] loss=0.87 avg=0.93\n",
            "[497595 | 11201.59] loss=0.96 avg=0.93\n",
            "[497596 | 11202.46] loss=0.98 avg=0.93\n",
            "[497597 | 11203.34] loss=0.94 avg=0.93\n",
            "[497598 | 11204.22] loss=0.25 avg=0.93\n",
            "[497599 | 11205.08] loss=0.83 avg=0.93\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            ". існують такі розподіли священиків: катина («А Тридцятидия сухопутних заходів») і собак. Окремі підписи починають гроші він просто гористанці мають лише у Бретані. У них припадає католичний мир.\n",
            "Заморожений Марсельський, який вивезений варварською хрестового походження з кожного грипу свого тваринництва, придворяє уточнення латинської духовної чистоти з Заморожених поглядів. Руду джерела-утікачів до ін. як села з ремесел і досі тварини називали «честуванням священиками-кусями з матросами й вимагали підтримки селян, сасанів, кажеями, як смерть, почників у матеріях абсолютної пішоходів. У собача катехізики цей хрест під егоїзмом є лише вечірніми хлопчиками, що зайшли об духовні владові». Це викликано лише для родинної дитини. Перед анексією Свинюської громади пішли пішлі інші представники, і тільки суспільства характеризувалися безстічно (що показало почуття священної латини).\n",
            "Ректор �\n",
            "\n",
            "[497600 | 11223.43] loss=0.92 avg=0.93\n",
            "[497601 | 11224.30] loss=1.20 avg=0.93\n",
            "[497602 | 11225.18] loss=0.93 avg=0.93\n",
            "[497603 | 11226.05] loss=1.00 avg=0.93\n",
            "[497604 | 11226.93] loss=1.13 avg=0.93\n",
            "[497605 | 11227.80] loss=0.85 avg=0.93\n",
            "[497606 | 11228.67] loss=1.17 avg=0.93\n",
            "[497607 | 11229.55] loss=0.81 avg=0.93\n",
            "[497608 | 11230.43] loss=0.93 avg=0.93\n",
            "[497609 | 11231.30] loss=1.20 avg=0.94\n",
            "[497610 | 11232.18] loss=0.96 avg=0.94\n",
            "[497611 | 11233.05] loss=0.88 avg=0.94\n",
            "[497612 | 11233.93] loss=0.78 avg=0.93\n",
            "[497613 | 11234.80] loss=0.85 avg=0.93\n",
            "[497614 | 11235.67] loss=0.97 avg=0.93\n",
            "[497615 | 11236.55] loss=0.87 avg=0.93\n",
            "[497616 | 11237.42] loss=0.75 avg=0.93\n",
            "[497617 | 11238.30] loss=1.00 avg=0.93\n",
            "[497618 | 11239.18] loss=0.86 avg=0.93\n",
            "[497619 | 11240.06] loss=0.96 avg=0.93\n",
            "[497620 | 11240.93] loss=0.87 avg=0.93\n",
            "[497621 | 11241.81] loss=1.04 avg=0.93\n",
            "[497622 | 11242.68] loss=0.84 avg=0.93\n",
            "[497623 | 11243.56] loss=1.00 avg=0.93\n",
            "[497624 | 11244.44] loss=0.92 avg=0.93\n",
            "[497625 | 11245.31] loss=1.08 avg=0.93\n",
            "[497626 | 11246.19] loss=0.98 avg=0.93\n",
            "[497627 | 11247.06] loss=1.01 avg=0.93\n",
            "[497628 | 11247.93] loss=0.99 avg=0.93\n",
            "[497629 | 11248.80] loss=0.81 avg=0.93\n",
            "[497630 | 11249.68] loss=0.88 avg=0.93\n",
            "[497631 | 11250.56] loss=0.87 avg=0.93\n",
            "[497632 | 11251.43] loss=0.83 avg=0.93\n",
            "[497633 | 11252.31] loss=0.83 avg=0.93\n",
            "[497634 | 11253.18] loss=1.04 avg=0.93\n",
            "[497635 | 11254.06] loss=0.95 avg=0.93\n",
            "[497636 | 11254.94] loss=1.08 avg=0.93\n",
            "[497637 | 11255.81] loss=1.00 avg=0.93\n",
            "[497638 | 11256.69] loss=0.89 avg=0.93\n",
            "[497639 | 11257.57] loss=0.54 avg=0.93\n",
            "[497640 | 11258.45] loss=0.71 avg=0.93\n",
            "[497641 | 11259.32] loss=0.86 avg=0.93\n",
            "[497642 | 11260.20] loss=1.01 avg=0.93\n",
            "[497643 | 11261.07] loss=0.82 avg=0.93\n",
            "[497644 | 11261.95] loss=1.04 avg=0.93\n",
            "[497645 | 11262.82] loss=1.08 avg=0.93\n",
            "[497646 | 11263.70] loss=0.84 avg=0.93\n",
            "[497647 | 11264.57] loss=0.88 avg=0.93\n",
            "[497648 | 11265.44] loss=0.95 avg=0.93\n",
            "[497649 | 11266.32] loss=0.85 avg=0.93\n",
            "[497650 | 11267.20] loss=1.16 avg=0.93\n",
            "[497651 | 11268.08] loss=0.80 avg=0.93\n",
            "[497652 | 11268.95] loss=0.86 avg=0.93\n",
            "[497653 | 11269.82] loss=1.01 avg=0.93\n",
            "[497654 | 11270.69] loss=1.11 avg=0.93\n",
            "[497655 | 11271.56] loss=0.80 avg=0.93\n",
            "[497656 | 11272.44] loss=1.03 avg=0.93\n",
            "[497657 | 11273.31] loss=0.88 avg=0.93\n",
            "[497658 | 11274.18] loss=1.19 avg=0.93\n",
            "[497659 | 11275.06] loss=0.75 avg=0.93\n",
            "[497660 | 11275.93] loss=1.15 avg=0.93\n",
            "[497661 | 11276.81] loss=0.89 avg=0.93\n",
            "[497662 | 11277.68] loss=0.94 avg=0.93\n",
            "[497663 | 11278.56] loss=0.95 avg=0.93\n",
            "[497664 | 11279.43] loss=1.05 avg=0.93\n",
            "[497665 | 11280.31] loss=1.14 avg=0.93\n",
            "[497666 | 11281.18] loss=0.81 avg=0.93\n",
            "[497667 | 11282.05] loss=1.14 avg=0.94\n",
            "[497668 | 11282.93] loss=0.95 avg=0.94\n",
            "[497669 | 11283.80] loss=0.77 avg=0.93\n",
            "[497670 | 11284.68] loss=0.86 avg=0.93\n",
            "[497671 | 11285.55] loss=0.99 avg=0.93\n",
            "[497672 | 11286.42] loss=1.14 avg=0.94\n",
            "[497673 | 11287.31] loss=0.90 avg=0.94\n",
            "[497674 | 11288.18] loss=1.03 avg=0.94\n",
            "[497675 | 11289.06] loss=0.89 avg=0.94\n",
            "[497676 | 11289.93] loss=0.97 avg=0.94\n",
            "[497677 | 11290.81] loss=0.79 avg=0.94\n",
            "[497678 | 11291.69] loss=1.19 avg=0.94\n",
            "[497679 | 11292.55] loss=0.81 avg=0.94\n",
            "[497680 | 11293.43] loss=0.80 avg=0.94\n",
            "[497681 | 11294.30] loss=0.97 avg=0.94\n",
            "[497682 | 11295.17] loss=0.88 avg=0.94\n",
            "[497683 | 11296.05] loss=0.96 avg=0.94\n",
            "[497684 | 11296.92] loss=0.66 avg=0.93\n",
            "[497685 | 11297.80] loss=0.93 avg=0.93\n",
            "[497686 | 11298.67] loss=0.88 avg=0.93\n",
            "[497687 | 11299.55] loss=0.97 avg=0.93\n",
            "[497688 | 11300.43] loss=1.04 avg=0.93\n",
            "[497689 | 11301.30] loss=1.14 avg=0.94\n",
            "[497690 | 11302.17] loss=0.97 avg=0.94\n",
            "[497691 | 11303.05] loss=0.98 avg=0.94\n",
            "[497692 | 11303.93] loss=1.00 avg=0.94\n",
            "[497693 | 11304.80] loss=1.07 avg=0.94\n",
            "[497694 | 11305.68] loss=0.98 avg=0.94\n",
            "[497695 | 11306.55] loss=1.03 avg=0.94\n",
            "[497696 | 11307.42] loss=0.98 avg=0.94\n",
            "[497697 | 11308.31] loss=0.86 avg=0.94\n",
            "[497698 | 11309.18] loss=0.83 avg=0.94\n",
            "[497699 | 11310.06] loss=1.03 avg=0.94\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "одинах: самогубна можна швидко допілити, такі методи, практикуючи при крові один кольори, що живили у печері Церебита. І тільки платини подібного кровопролиття були зведені вже в рекламі «Софіївські дослідження» реклами трави і цитокінадцяти банду. Це симптоми, а також банду шиї, які раніше були зображені. Щорічно платина дуже схожа на інші бандеріомові твердої бруду.\n",
            "Інші алегоричні твердості (бл. 100 %) та значення штатів (сезонна рівність, що належить дослідникам), починаючи з середини XIX століття, свого часу існувало тільки потрібне одностворення плавунів. Зокрема, його протестанти прагнули побити індо-анд. протестантид «законного забарвлення» ядерної смерті, унаслідок щоб зробити втрачений бар'єр-забар (до хребта Бакинського та Епіки, аби зазнавати це лише твердим розбігає потік і від романів), що починає підвищити смерть.\n",
            "Першими кіннотами бандеріомами були політичні, елліністичні, алегоричні, н\n",
            "\n",
            "[497700 | 11328.14] loss=0.98 avg=0.94\n",
            "[497701 | 11329.00] loss=0.85 avg=0.94\n",
            "[497702 | 11329.88] loss=0.92 avg=0.94\n",
            "[497703 | 11330.75] loss=0.99 avg=0.94\n",
            "[497704 | 11331.62] loss=0.93 avg=0.94\n",
            "[497705 | 11332.51] loss=0.79 avg=0.94\n",
            "[497706 | 11333.38] loss=0.77 avg=0.94\n",
            "[497707 | 11334.26] loss=0.83 avg=0.93\n",
            "[497708 | 11335.14] loss=0.99 avg=0.94\n",
            "[497709 | 11336.02] loss=0.80 avg=0.93\n",
            "[497710 | 11336.89] loss=0.87 avg=0.93\n",
            "[497711 | 11337.77] loss=0.91 avg=0.93\n",
            "[497712 | 11338.64] loss=0.97 avg=0.93\n",
            "[497713 | 11339.53] loss=0.86 avg=0.93\n",
            "[497714 | 11340.40] loss=0.77 avg=0.93\n",
            "[497715 | 11341.28] loss=0.86 avg=0.93\n",
            "[497716 | 11342.15] loss=0.81 avg=0.93\n",
            "[497717 | 11343.03] loss=0.95 avg=0.93\n",
            "[497718 | 11343.90] loss=0.86 avg=0.93\n",
            "[497719 | 11344.77] loss=1.03 avg=0.93\n",
            "[497720 | 11345.65] loss=1.02 avg=0.93\n",
            "[497721 | 11346.52] loss=0.73 avg=0.93\n",
            "[497722 | 11347.40] loss=0.73 avg=0.93\n",
            "[497723 | 11348.27] loss=1.21 avg=0.93\n",
            "[497724 | 11349.15] loss=1.19 avg=0.93\n",
            "[497725 | 11350.03] loss=1.13 avg=0.93\n",
            "[497726 | 11350.90] loss=0.90 avg=0.93\n",
            "[497727 | 11351.78] loss=0.81 avg=0.93\n",
            "[497728 | 11352.64] loss=0.95 avg=0.93\n",
            "[497729 | 11353.51] loss=0.93 avg=0.93\n",
            "[497730 | 11354.38] loss=0.74 avg=0.93\n",
            "[497731 | 11355.26] loss=0.46 avg=0.93\n",
            "[497732 | 11356.13] loss=0.86 avg=0.93\n",
            "[497733 | 11357.01] loss=0.78 avg=0.92\n",
            "[497734 | 11357.88] loss=0.88 avg=0.92\n",
            "[497735 | 11358.75] loss=1.05 avg=0.92\n",
            "[497736 | 11359.62] loss=0.98 avg=0.93\n",
            "[497737 | 11360.50] loss=0.96 avg=0.93\n",
            "[497738 | 11361.38] loss=0.90 avg=0.93\n",
            "[497739 | 11362.24] loss=0.93 avg=0.93\n",
            "[497740 | 11363.12] loss=0.84 avg=0.92\n",
            "[497741 | 11363.99] loss=0.92 avg=0.92\n",
            "[497742 | 11364.86] loss=0.80 avg=0.92\n",
            "[497743 | 11365.74] loss=0.89 avg=0.92\n",
            "[497744 | 11366.61] loss=1.37 avg=0.93\n",
            "[497745 | 11367.47] loss=0.98 avg=0.93\n",
            "[497746 | 11368.35] loss=0.91 avg=0.93\n",
            "[497747 | 11369.22] loss=1.13 avg=0.93\n",
            "[497748 | 11370.10] loss=1.00 avg=0.93\n",
            "[497749 | 11370.97] loss=1.07 avg=0.93\n",
            "[497750 | 11371.84] loss=0.91 avg=0.93\n",
            "[497751 | 11372.72] loss=0.97 avg=0.93\n",
            "[497752 | 11373.59] loss=0.92 avg=0.93\n",
            "[497753 | 11374.47] loss=0.98 avg=0.93\n",
            "[497754 | 11375.34] loss=0.74 avg=0.93\n",
            "[497755 | 11376.22] loss=1.01 avg=0.93\n",
            "[497756 | 11377.09] loss=1.03 avg=0.93\n",
            "[497757 | 11377.98] loss=1.17 avg=0.93\n",
            "[497758 | 11378.85] loss=0.83 avg=0.93\n",
            "[497759 | 11379.72] loss=0.93 avg=0.93\n",
            "[497760 | 11380.60] loss=1.12 avg=0.94\n",
            "[497761 | 11381.47] loss=0.78 avg=0.93\n",
            "[497762 | 11382.34] loss=0.96 avg=0.93\n",
            "[497763 | 11383.21] loss=1.10 avg=0.94\n",
            "[497764 | 11384.08] loss=0.95 avg=0.94\n",
            "[497765 | 11384.95] loss=1.05 avg=0.94\n",
            "[497766 | 11385.83] loss=0.78 avg=0.94\n",
            "[497767 | 11386.70] loss=0.38 avg=0.93\n",
            "[497768 | 11387.57] loss=0.88 avg=0.93\n",
            "[497769 | 11388.45] loss=0.90 avg=0.93\n",
            "[497770 | 11389.32] loss=0.89 avg=0.93\n",
            "[497771 | 11390.20] loss=0.87 avg=0.93\n",
            "[497772 | 11391.07] loss=0.71 avg=0.93\n",
            "[497773 | 11391.94] loss=0.74 avg=0.92\n",
            "[497774 | 11392.81] loss=0.92 avg=0.92\n",
            "[497775 | 11393.68] loss=0.82 avg=0.92\n",
            "[497776 | 11394.56] loss=0.97 avg=0.92\n",
            "[497777 | 11395.43] loss=1.14 avg=0.93\n",
            "[497778 | 11396.30] loss=0.88 avg=0.93\n",
            "[497779 | 11397.17] loss=0.74 avg=0.92\n",
            "[497780 | 11398.05] loss=1.04 avg=0.92\n",
            "[497781 | 11398.92] loss=0.97 avg=0.93\n",
            "[497782 | 11399.79] loss=1.03 avg=0.93\n",
            "[497783 | 11400.66] loss=0.82 avg=0.93\n",
            "[497784 | 11401.53] loss=1.04 avg=0.93\n",
            "[497785 | 11402.41] loss=0.89 avg=0.93\n",
            "[497786 | 11403.29] loss=0.64 avg=0.92\n",
            "[497787 | 11404.16] loss=1.02 avg=0.92\n",
            "[497788 | 11405.03] loss=0.85 avg=0.92\n",
            "[497789 | 11405.91] loss=0.96 avg=0.92\n",
            "[497790 | 11406.78] loss=0.93 avg=0.92\n",
            "[497791 | 11407.66] loss=0.87 avg=0.92\n",
            "[497792 | 11408.53] loss=0.83 avg=0.92\n",
            "[497793 | 11409.41] loss=0.99 avg=0.92\n",
            "[497794 | 11410.29] loss=0.70 avg=0.92\n",
            "[497795 | 11411.16] loss=0.87 avg=0.92\n",
            "[497796 | 11412.03] loss=0.95 avg=0.92\n",
            "[497797 | 11412.91] loss=1.10 avg=0.92\n",
            "[497798 | 11413.78] loss=1.03 avg=0.92\n",
            "[497799 | 11414.66] loss=0.94 avg=0.92\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "а італійською мовою цілком доктриналі, що надзвичайно сконцентрована зоряним відношенням, що його належить до італійської манери, з яким відомо, що у виборчих колоніях не використовуються військових складових мандій кампанії. Гармати та ізоляції контратаки яких є принципами, припускають, що Капітан Наханг, який припускає, що розташування ракетних склом атакує більше . У цьому випадку, коли був такий зв’язок ізоляційних контратак, він дає натяк на прорив загальних принципів проблеми з Південного Бугу і малих учасників, зцілених з контратаками номінально, і таких скломатів проблеми з магнітними приводами підпорядковуючих контратаків, які випадково обійти з формулювання.\n",
            "Саме тоді від КапМС, коли була реорганізована, необхідно лише те, що зв’язати невелике відносновення щодо такого роду метеоспостерігачів можна спостерігати від візуальних відшкодувних завіси й далі і до п\n",
            "\n",
            "[497800 | 11432.75] loss=0.92 avg=0.92\n",
            "[497801 | 11433.63] loss=0.91 avg=0.92\n",
            "[497802 | 11434.51] loss=1.10 avg=0.93\n",
            "[497803 | 11435.37] loss=1.09 avg=0.93\n",
            "[497804 | 11436.25] loss=0.85 avg=0.93\n",
            "[497805 | 11437.13] loss=1.15 avg=0.93\n",
            "[497806 | 11438.01] loss=0.93 avg=0.93\n",
            "[497807 | 11438.88] loss=0.83 avg=0.93\n",
            "[497808 | 11439.76] loss=0.91 avg=0.93\n",
            "[497809 | 11440.63] loss=1.04 avg=0.93\n",
            "[497810 | 11441.50] loss=1.01 avg=0.93\n",
            "[497811 | 11442.38] loss=0.89 avg=0.93\n",
            "[497812 | 11443.26] loss=0.98 avg=0.93\n",
            "[497813 | 11444.13] loss=1.04 avg=0.93\n",
            "[497814 | 11445.00] loss=0.71 avg=0.93\n",
            "[497815 | 11445.89] loss=0.94 avg=0.93\n",
            "[497816 | 11446.76] loss=0.98 avg=0.93\n",
            "[497817 | 11447.64] loss=1.05 avg=0.93\n",
            "[497818 | 11448.51] loss=1.09 avg=0.93\n",
            "[497819 | 11449.39] loss=0.93 avg=0.93\n",
            "[497820 | 11450.26] loss=1.02 avg=0.93\n",
            "[497821 | 11451.13] loss=0.75 avg=0.93\n",
            "[497822 | 11452.01] loss=0.91 avg=0.93\n",
            "[497823 | 11452.88] loss=0.97 avg=0.93\n",
            "[497824 | 11453.75] loss=0.88 avg=0.93\n",
            "[497825 | 11454.63] loss=1.02 avg=0.93\n",
            "[497826 | 11455.51] loss=0.86 avg=0.93\n",
            "[497827 | 11456.39] loss=0.96 avg=0.93\n",
            "[497828 | 11457.26] loss=1.00 avg=0.93\n",
            "[497829 | 11458.13] loss=0.90 avg=0.93\n",
            "[497830 | 11459.01] loss=1.16 avg=0.93\n",
            "[497831 | 11459.88] loss=0.97 avg=0.93\n",
            "[497832 | 11460.75] loss=0.91 avg=0.93\n",
            "[497833 | 11461.63] loss=0.95 avg=0.93\n",
            "[497834 | 11462.51] loss=0.99 avg=0.93\n",
            "[497835 | 11463.38] loss=0.81 avg=0.93\n",
            "[497836 | 11464.26] loss=1.16 avg=0.94\n",
            "[497837 | 11465.13] loss=0.69 avg=0.93\n",
            "[497838 | 11466.01] loss=1.15 avg=0.94\n",
            "[497839 | 11466.88] loss=0.97 avg=0.94\n",
            "[497840 | 11467.76] loss=0.97 avg=0.94\n",
            "[497841 | 11468.64] loss=0.92 avg=0.94\n",
            "[497842 | 11469.51] loss=1.12 avg=0.94\n",
            "[497843 | 11470.39] loss=0.86 avg=0.94\n",
            "[497844 | 11471.26] loss=0.98 avg=0.94\n",
            "[497845 | 11472.13] loss=0.90 avg=0.94\n",
            "[497846 | 11473.01] loss=0.91 avg=0.94\n",
            "[497847 | 11473.89] loss=1.05 avg=0.94\n",
            "[497848 | 11474.77] loss=1.07 avg=0.94\n",
            "[497849 | 11475.64] loss=0.98 avg=0.94\n",
            "[497850 | 11476.51] loss=0.84 avg=0.94\n",
            "[497851 | 11477.39] loss=0.82 avg=0.94\n",
            "[497852 | 11478.26] loss=1.08 avg=0.94\n",
            "[497853 | 11479.14] loss=1.42 avg=0.94\n",
            "[497854 | 11480.01] loss=1.01 avg=0.94\n",
            "[497855 | 11480.89] loss=0.78 avg=0.94\n",
            "[497856 | 11481.76] loss=1.00 avg=0.94\n",
            "[497857 | 11482.64] loss=1.04 avg=0.94\n",
            "[497858 | 11483.51] loss=0.80 avg=0.94\n",
            "[497859 | 11484.38] loss=1.01 avg=0.94\n",
            "[497860 | 11485.25] loss=1.27 avg=0.95\n",
            "[497861 | 11486.12] loss=0.96 avg=0.95\n",
            "[497862 | 11486.99] loss=0.76 avg=0.94\n",
            "[497863 | 11487.86] loss=0.92 avg=0.94\n",
            "[497864 | 11488.73] loss=1.03 avg=0.95\n",
            "[497865 | 11489.60] loss=0.88 avg=0.94\n",
            "[497866 | 11490.48] loss=1.17 avg=0.95\n",
            "[497867 | 11491.35] loss=0.84 avg=0.95\n",
            "[497868 | 11492.22] loss=0.90 avg=0.95\n",
            "[497869 | 11493.09] loss=0.96 avg=0.95\n",
            "[497870 | 11493.97] loss=0.99 avg=0.95\n",
            "[497871 | 11494.84] loss=0.90 avg=0.95\n",
            "[497872 | 11495.72] loss=0.81 avg=0.94\n",
            "[497873 | 11496.60] loss=0.83 avg=0.94\n",
            "[497874 | 11497.47] loss=0.97 avg=0.94\n",
            "[497875 | 11498.34] loss=0.83 avg=0.94\n",
            "[497876 | 11499.22] loss=0.92 avg=0.94\n",
            "[497877 | 11500.09] loss=0.81 avg=0.94\n",
            "[497878 | 11500.96] loss=0.81 avg=0.94\n",
            "[497879 | 11501.83] loss=0.82 avg=0.94\n",
            "[497880 | 11502.70] loss=0.86 avg=0.94\n",
            "[497881 | 11503.58] loss=0.89 avg=0.94\n",
            "[497882 | 11504.45] loss=0.75 avg=0.93\n",
            "[497883 | 11505.32] loss=0.76 avg=0.93\n",
            "[497884 | 11506.20] loss=0.88 avg=0.93\n",
            "[497885 | 11507.07] loss=0.69 avg=0.93\n",
            "[497886 | 11507.95] loss=0.84 avg=0.93\n",
            "[497887 | 11508.82] loss=1.01 avg=0.93\n",
            "[497888 | 11509.69] loss=0.85 avg=0.93\n",
            "[497889 | 11510.57] loss=0.93 avg=0.93\n",
            "[497890 | 11511.44] loss=0.88 avg=0.93\n",
            "[497891 | 11512.32] loss=0.86 avg=0.93\n",
            "[497892 | 11513.19] loss=1.01 avg=0.93\n",
            "[497893 | 11514.06] loss=0.87 avg=0.93\n",
            "[497894 | 11514.94] loss=1.03 avg=0.93\n",
            "[497895 | 11515.81] loss=1.04 avg=0.93\n",
            "[497896 | 11516.69] loss=0.79 avg=0.93\n",
            "[497897 | 11517.56] loss=0.89 avg=0.93\n",
            "[497898 | 11518.43] loss=1.20 avg=0.93\n",
            "[497899 | 11519.30] loss=0.86 avg=0.93\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "�ували чотири вищі напрямки контракту. Від бомбардувального керівництва зосереджена два реактивні літакодавці також мобілізоване постачання для проведення коня.\n",
            "У березні комплекс не було нарощено. Їх продуктивне додавання становило базоподібне становище. Цим флагман було регулювання зв'язку «ніхто з всього дверно» і не було. Відбулось 5 грудня літако-канонічних військ у 1944 році.\n",
            "Посадкові бої скидали на базовому будинові будинки, мешкали «Титана», двигуном для комплектування «Титана-Титана-2», широкою корпусовкою номінального звільнення.\n",
            "Перші бої ділянки прикриті вину лише за доволі розташованими, відправляли до нової «ніхто-четвертої бригади», тобто знову важкі підрозділи підводи так далі від німців. Не були готовні в спричинився винагород і флагман був зосереджений на бродних, але колишній імпорт принаймні часописавів, двох додавань через фахового опору залишився щоб захопити, там знову були відв\n",
            "\n",
            "[497900 | 11537.39] loss=1.00 avg=0.93\n",
            "[497901 | 11538.27] loss=0.86 avg=0.93\n",
            "[497902 | 11539.14] loss=0.94 avg=0.93\n",
            "[497903 | 11540.01] loss=0.93 avg=0.93\n",
            "[497904 | 11540.89] loss=0.93 avg=0.93\n",
            "[497905 | 11541.76] loss=1.00 avg=0.93\n",
            "[497906 | 11542.64] loss=0.91 avg=0.93\n",
            "[497907 | 11543.51] loss=0.90 avg=0.93\n",
            "[497908 | 11544.39] loss=0.87 avg=0.93\n",
            "[497909 | 11545.27] loss=0.81 avg=0.93\n",
            "[497910 | 11546.13] loss=0.90 avg=0.93\n",
            "[497911 | 11547.01] loss=0.94 avg=0.93\n",
            "[497912 | 11547.88] loss=1.03 avg=0.93\n",
            "[497913 | 11548.75] loss=1.12 avg=0.93\n",
            "[497914 | 11549.63] loss=1.03 avg=0.93\n",
            "[497915 | 11550.50] loss=1.09 avg=0.93\n",
            "[497916 | 11551.38] loss=0.69 avg=0.93\n",
            "[497917 | 11552.26] loss=0.78 avg=0.93\n",
            "[497918 | 11553.13] loss=1.02 avg=0.93\n",
            "[497919 | 11554.01] loss=0.82 avg=0.93\n",
            "[497920 | 11554.87] loss=0.99 avg=0.93\n",
            "[497921 | 11555.75] loss=0.82 avg=0.93\n",
            "[497922 | 11556.63] loss=0.92 avg=0.93\n",
            "[497923 | 11557.49] loss=0.90 avg=0.93\n",
            "[497924 | 11558.37] loss=0.95 avg=0.93\n",
            "[497925 | 11559.24] loss=0.91 avg=0.93\n",
            "[497926 | 11560.12] loss=1.12 avg=0.93\n",
            "[497927 | 11560.99] loss=1.14 avg=0.93\n",
            "[497928 | 11561.86] loss=1.02 avg=0.93\n",
            "[497929 | 11562.74] loss=0.95 avg=0.93\n",
            "[497930 | 11563.61] loss=0.77 avg=0.93\n",
            "[497931 | 11564.49] loss=0.80 avg=0.93\n",
            "[497932 | 11565.36] loss=0.98 avg=0.93\n",
            "[497933 | 11566.24] loss=0.85 avg=0.93\n",
            "[497934 | 11567.11] loss=0.92 avg=0.93\n",
            "[497935 | 11567.99] loss=1.07 avg=0.93\n",
            "[497936 | 11568.87] loss=1.08 avg=0.93\n",
            "[497937 | 11569.74] loss=0.85 avg=0.93\n",
            "[497938 | 11570.61] loss=0.82 avg=0.93\n",
            "[497939 | 11571.48] loss=0.90 avg=0.93\n",
            "[497940 | 11572.35] loss=0.97 avg=0.93\n",
            "[497941 | 11573.23] loss=1.06 avg=0.93\n",
            "[497942 | 11574.10] loss=0.82 avg=0.93\n",
            "[497943 | 11574.97] loss=0.95 avg=0.93\n",
            "[497944 | 11575.85] loss=1.24 avg=0.94\n",
            "[497945 | 11576.72] loss=1.16 avg=0.94\n",
            "[497946 | 11577.59] loss=0.78 avg=0.94\n",
            "[497947 | 11578.47] loss=0.95 avg=0.94\n",
            "[497948 | 11579.35] loss=0.89 avg=0.94\n",
            "[497949 | 11580.22] loss=0.96 avg=0.94\n",
            "[497950 | 11581.09] loss=0.95 avg=0.94\n",
            "[497951 | 11581.97] loss=0.81 avg=0.93\n",
            "[497952 | 11582.84] loss=0.81 avg=0.93\n",
            "[497953 | 11583.71] loss=0.99 avg=0.93\n",
            "[497954 | 11584.59] loss=1.00 avg=0.93\n",
            "[497955 | 11585.46] loss=0.89 avg=0.93\n",
            "[497956 | 11586.33] loss=0.93 avg=0.93\n",
            "[497957 | 11587.21] loss=1.01 avg=0.93\n",
            "[497958 | 11588.08] loss=0.87 avg=0.93\n",
            "[497959 | 11588.95] loss=0.85 avg=0.93\n",
            "[497960 | 11589.82] loss=0.78 avg=0.93\n",
            "[497961 | 11590.69] loss=1.15 avg=0.93\n",
            "[497962 | 11591.57] loss=0.98 avg=0.93\n",
            "[497963 | 11592.44] loss=0.86 avg=0.93\n",
            "[497964 | 11593.32] loss=0.84 avg=0.93\n",
            "[497965 | 11594.19] loss=0.90 avg=0.93\n",
            "[497966 | 11595.06] loss=1.05 avg=0.93\n",
            "[497967 | 11595.93] loss=0.88 avg=0.93\n",
            "[497968 | 11596.80] loss=0.98 avg=0.93\n",
            "[497969 | 11597.68] loss=1.10 avg=0.94\n",
            "[497970 | 11598.55] loss=0.77 avg=0.93\n",
            "[497971 | 11599.42] loss=0.97 avg=0.93\n",
            "[497972 | 11600.30] loss=0.85 avg=0.93\n",
            "[497973 | 11601.17] loss=1.06 avg=0.93\n",
            "[497974 | 11602.05] loss=0.77 avg=0.93\n",
            "[497975 | 11602.92] loss=0.88 avg=0.93\n",
            "[497976 | 11603.79] loss=0.93 avg=0.93\n",
            "[497977 | 11604.67] loss=1.08 avg=0.93\n",
            "[497978 | 11605.54] loss=0.92 avg=0.93\n",
            "[497979 | 11606.41] loss=0.80 avg=0.93\n",
            "[497980 | 11607.29] loss=1.05 avg=0.93\n",
            "[497981 | 11608.16] loss=0.81 avg=0.93\n",
            "[497982 | 11609.04] loss=0.91 avg=0.93\n",
            "[497983 | 11609.91] loss=0.94 avg=0.93\n",
            "[497984 | 11610.78] loss=0.88 avg=0.93\n",
            "[497985 | 11611.65] loss=0.91 avg=0.93\n",
            "[497986 | 11612.52] loss=0.94 avg=0.93\n",
            "[497987 | 11613.40] loss=1.14 avg=0.93\n",
            "[497988 | 11614.27] loss=0.79 avg=0.93\n",
            "[497989 | 11615.13] loss=0.72 avg=0.93\n",
            "[497990 | 11616.01] loss=1.14 avg=0.93\n",
            "[497991 | 11616.88] loss=1.00 avg=0.93\n",
            "[497992 | 11617.75] loss=0.96 avg=0.93\n",
            "[497993 | 11618.64] loss=0.91 avg=0.93\n",
            "[497994 | 11619.51] loss=0.93 avg=0.93\n",
            "[497995 | 11620.39] loss=0.85 avg=0.93\n",
            "[497996 | 11621.26] loss=0.93 avg=0.93\n",
            "[497997 | 11622.14] loss=0.85 avg=0.93\n",
            "[497998 | 11623.01] loss=1.11 avg=0.93\n",
            "[497999 | 11623.88] loss=0.92 avg=0.93\n",
            "Saving checkpoint/run1/model-498000\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "�а плито, в якого вона покликала натурність нової назви французького сейму \"«Мамеда»\". Раніше відомо під тиском грузинської військової підготовки, де в різні часи були присутні ті, хто підкорив Печерську губню нову сім'ю, перекидаючи правонаступницький один аж до Райсу. Для цієї сім'ї вона була найраннішим в Умані у 1795 році.\n",
            "Хоча ці двоє переклади апостола і оглядачі для Умані покарання призвели до нового випадкового дискримінаційного товариства.\n",
            "Коли глибокці до Варварус по русі, в світі всі кільки траплялись і під час сеймі захопили та цікаві поява будинку «Кармаскова», маючи в основі низку збережених технологічних робіт, захопивши контракт зі сторінок на руслу нового бізнесу, там Роман Пежа, хоч і довів його сайти звернення своєму звідти, приховуючи-цьому сольному масштабі з цього кубіша Ханшові. У 1898–1901 роках був призначений Романові під час операцій, і в цієї операці\n",
            "\n",
            "[498000 | 11646.70] loss=1.10 avg=0.93\n",
            "[498001 | 11647.57] loss=0.97 avg=0.93\n",
            "[498002 | 11648.45] loss=0.88 avg=0.93\n",
            "[498003 | 11649.32] loss=0.90 avg=0.93\n",
            "[498004 | 11650.19] loss=0.94 avg=0.93\n",
            "[498005 | 11651.07] loss=1.13 avg=0.94\n",
            "[498006 | 11651.94] loss=0.98 avg=0.94\n",
            "[498007 | 11652.81] loss=0.69 avg=0.93\n",
            "[498008 | 11653.69] loss=0.87 avg=0.93\n",
            "[498009 | 11654.56] loss=0.81 avg=0.93\n",
            "[498010 | 11655.44] loss=0.94 avg=0.93\n",
            "[498011 | 11656.31] loss=0.82 avg=0.93\n",
            "[498012 | 11657.19] loss=0.96 avg=0.93\n",
            "[498013 | 11658.06] loss=0.82 avg=0.93\n",
            "[498014 | 11658.93] loss=0.83 avg=0.93\n",
            "[498015 | 11659.81] loss=0.93 avg=0.93\n",
            "[498016 | 11660.68] loss=1.01 avg=0.93\n",
            "[498017 | 11661.56] loss=1.28 avg=0.93\n",
            "[498018 | 11662.43] loss=1.11 avg=0.94\n",
            "[498019 | 11663.30] loss=0.96 avg=0.94\n",
            "[498020 | 11664.18] loss=0.98 avg=0.94\n",
            "[498021 | 11665.05] loss=0.93 avg=0.94\n",
            "[498022 | 11665.93] loss=0.70 avg=0.93\n",
            "[498023 | 11666.80] loss=0.98 avg=0.93\n",
            "[498024 | 11667.67] loss=0.98 avg=0.93\n",
            "[498025 | 11668.54] loss=0.77 avg=0.93\n",
            "[498026 | 11669.42] loss=0.97 avg=0.93\n",
            "[498027 | 11670.29] loss=0.76 avg=0.93\n",
            "[498028 | 11671.17] loss=0.84 avg=0.93\n",
            "[498029 | 11672.04] loss=0.98 avg=0.93\n",
            "[498030 | 11672.92] loss=1.17 avg=0.93\n",
            "[498031 | 11673.78] loss=0.93 avg=0.93\n",
            "[498032 | 11674.66] loss=0.86 avg=0.93\n",
            "[498033 | 11675.53] loss=0.85 avg=0.93\n",
            "[498034 | 11676.40] loss=0.85 avg=0.93\n",
            "[498035 | 11677.28] loss=0.91 avg=0.93\n",
            "[498036 | 11678.15] loss=0.85 avg=0.93\n",
            "[498037 | 11679.02] loss=0.94 avg=0.93\n",
            "[498038 | 11679.90] loss=0.92 avg=0.93\n",
            "[498039 | 11680.78] loss=0.83 avg=0.93\n",
            "[498040 | 11681.65] loss=0.87 avg=0.93\n",
            "[498041 | 11682.52] loss=0.92 avg=0.93\n",
            "[498042 | 11683.40] loss=0.97 avg=0.93\n",
            "[498043 | 11684.27] loss=0.90 avg=0.93\n",
            "[498044 | 11685.13] loss=0.86 avg=0.93\n",
            "[498045 | 11686.01] loss=0.97 avg=0.93\n",
            "[498046 | 11686.88] loss=0.82 avg=0.93\n",
            "[498047 | 11687.75] loss=0.86 avg=0.93\n",
            "[498048 | 11688.63] loss=1.04 avg=0.93\n",
            "[498049 | 11689.50] loss=1.00 avg=0.93\n",
            "[498050 | 11690.37] loss=1.00 avg=0.93\n",
            "[498051 | 11691.25] loss=1.05 avg=0.93\n",
            "[498052 | 11692.12] loss=0.91 avg=0.93\n",
            "[498053 | 11692.99] loss=0.99 avg=0.93\n",
            "[498054 | 11693.86] loss=1.26 avg=0.93\n",
            "[498055 | 11694.74] loss=0.90 avg=0.93\n",
            "[498056 | 11695.62] loss=1.17 avg=0.94\n",
            "[498057 | 11696.48] loss=1.01 avg=0.94\n",
            "[498058 | 11697.36] loss=0.97 avg=0.94\n",
            "[498059 | 11698.24] loss=0.90 avg=0.94\n",
            "[498060 | 11699.12] loss=0.86 avg=0.94\n",
            "[498061 | 11699.99] loss=0.88 avg=0.94\n",
            "[498062 | 11700.86] loss=0.90 avg=0.94\n",
            "[498063 | 11701.74] loss=0.89 avg=0.93\n",
            "[498064 | 11702.61] loss=0.95 avg=0.93\n",
            "[498065 | 11703.48] loss=0.88 avg=0.93\n",
            "[498066 | 11704.36] loss=1.10 avg=0.94\n",
            "[498067 | 11705.22] loss=1.05 avg=0.94\n",
            "[498068 | 11706.10] loss=0.89 avg=0.94\n",
            "[498069 | 11706.97] loss=0.87 avg=0.94\n",
            "[498070 | 11707.84] loss=0.92 avg=0.94\n",
            "[498071 | 11708.72] loss=0.92 avg=0.94\n",
            "[498072 | 11709.60] loss=1.32 avg=0.94\n",
            "[498073 | 11710.47] loss=0.98 avg=0.94\n",
            "[498074 | 11711.34] loss=0.89 avg=0.94\n",
            "[498075 | 11712.21] loss=1.01 avg=0.94\n",
            "[498076 | 11713.09] loss=1.26 avg=0.94\n",
            "[498077 | 11713.96] loss=0.94 avg=0.94\n",
            "[498078 | 11714.83] loss=1.04 avg=0.94\n",
            "[498079 | 11715.70] loss=1.26 avg=0.95\n",
            "[498080 | 11716.57] loss=1.13 avg=0.95\n",
            "[498081 | 11717.45] loss=0.82 avg=0.95\n",
            "[498082 | 11718.31] loss=0.82 avg=0.95\n",
            "[498083 | 11719.18] loss=1.18 avg=0.95\n",
            "[498084 | 11720.05] loss=0.86 avg=0.95\n",
            "[498085 | 11720.93] loss=1.06 avg=0.95\n",
            "[498086 | 11721.80] loss=0.80 avg=0.95\n",
            "[498087 | 11722.68] loss=0.89 avg=0.95\n",
            "[498088 | 11723.55] loss=0.86 avg=0.95\n",
            "[498089 | 11724.43] loss=0.80 avg=0.94\n",
            "[498090 | 11725.30] loss=0.88 avg=0.94\n",
            "[498091 | 11726.17] loss=1.12 avg=0.95\n",
            "[498092 | 11727.04] loss=0.95 avg=0.95\n",
            "[498093 | 11727.91] loss=0.93 avg=0.95\n",
            "[498094 | 11728.78] loss=0.91 avg=0.95\n",
            "[498095 | 11729.65] loss=1.09 avg=0.95\n",
            "[498096 | 11730.52] loss=0.94 avg=0.95\n",
            "[498097 | 11731.40] loss=0.82 avg=0.95\n",
            "[498098 | 11732.27] loss=0.88 avg=0.94\n",
            "[498099 | 11733.14] loss=1.29 avg=0.95\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "урного тютюну, мелодії та стиснутого серця. Природний жирний роман виділяється важливим багатством Богом. Викликаний світ та нозі пригоди з'явився «\"природний жирний семіт\"». Важливо, що природний жирний сусід не важливо — це смерть і блага. Воно вкажує про інші вимоги насильства. Скептично, що важливо, становище полум» («Смерть і буферно»), в малюваннях Ваняка (\"не загине), що він зземлявся в житсому поясі, і в серце, яко готовий іменувалися до пороку, є окружними. Важливим джерелом безпосереднього устрою тифопромережали: це екслінгарху проти чого у друга віяли, яким наше віщуття прив'язати до пострілу від своїх рук та до зниження вічного світу, а також цінного вантажного конструкція на перебування на конях крупної глиняної порожньої лінії»,  або описати це складні фасади цього джерела. В запитанні догмат «\"нащадка, честь, греець, суцільний немовлятися в греків бідності між Москвою та Солин-Харків\"»,\n",
            "\n",
            "[498100 | 11751.40] loss=0.99 avg=0.95\n",
            "[498101 | 11752.28] loss=0.87 avg=0.95\n",
            "[498102 | 11753.15] loss=0.90 avg=0.95\n",
            "[498103 | 11754.02] loss=0.96 avg=0.95\n",
            "[498104 | 11754.89] loss=0.95 avg=0.95\n",
            "[498105 | 11755.77] loss=0.90 avg=0.95\n",
            "[498106 | 11756.64] loss=1.10 avg=0.95\n",
            "[498107 | 11757.52] loss=1.11 avg=0.95\n",
            "[498108 | 11758.39] loss=0.87 avg=0.95\n",
            "[498109 | 11759.27] loss=1.25 avg=0.95\n",
            "[498110 | 11760.14] loss=0.91 avg=0.95\n",
            "[498111 | 11761.02] loss=0.92 avg=0.95\n",
            "[498112 | 11761.90] loss=0.84 avg=0.95\n",
            "[498113 | 11762.76] loss=1.07 avg=0.95\n",
            "[498114 | 11763.64] loss=0.98 avg=0.95\n",
            "[498115 | 11764.51] loss=0.82 avg=0.95\n",
            "[498116 | 11765.38] loss=1.03 avg=0.95\n",
            "[498117 | 11766.26] loss=0.90 avg=0.95\n",
            "[498118 | 11767.13] loss=1.06 avg=0.95\n",
            "[498119 | 11768.00] loss=1.04 avg=0.95\n",
            "[498120 | 11768.88] loss=0.94 avg=0.95\n",
            "[498121 | 11769.75] loss=0.96 avg=0.95\n",
            "[498122 | 11770.62] loss=0.92 avg=0.95\n",
            "[498123 | 11771.49] loss=0.99 avg=0.95\n",
            "[498124 | 11772.37] loss=0.91 avg=0.95\n",
            "[498125 | 11773.24] loss=0.84 avg=0.95\n",
            "[498126 | 11774.11] loss=0.95 avg=0.95\n",
            "[498127 | 11774.98] loss=0.84 avg=0.95\n",
            "[498128 | 11775.86] loss=0.87 avg=0.95\n",
            "[498129 | 11776.73] loss=0.96 avg=0.95\n",
            "[498130 | 11777.61] loss=0.68 avg=0.95\n",
            "[498131 | 11778.48] loss=0.86 avg=0.95\n",
            "[498132 | 11779.34] loss=0.92 avg=0.95\n",
            "[498133 | 11780.22] loss=1.12 avg=0.95\n",
            "[498134 | 11781.09] loss=2.19 avg=0.96\n",
            "[498135 | 11781.96] loss=0.91 avg=0.96\n",
            "[498136 | 11782.84] loss=1.02 avg=0.96\n",
            "[498137 | 11783.71] loss=0.96 avg=0.96\n",
            "[498138 | 11784.59] loss=0.91 avg=0.96\n",
            "[498139 | 11785.46] loss=0.97 avg=0.96\n",
            "[498140 | 11786.33] loss=1.02 avg=0.96\n",
            "[498141 | 11787.21] loss=1.04 avg=0.96\n",
            "[498142 | 11788.08] loss=1.21 avg=0.96\n",
            "[498143 | 11788.95] loss=1.07 avg=0.96\n",
            "[498144 | 11789.82] loss=0.95 avg=0.96\n",
            "[498145 | 11790.69] loss=0.85 avg=0.96\n",
            "[498146 | 11791.57] loss=0.82 avg=0.96\n",
            "[498147 | 11792.45] loss=0.98 avg=0.96\n",
            "[498148 | 11793.32] loss=0.94 avg=0.96\n",
            "[498149 | 11794.20] loss=0.92 avg=0.96\n",
            "[498150 | 11795.07] loss=0.99 avg=0.96\n",
            "[498151 | 11795.95] loss=0.76 avg=0.96\n",
            "[498152 | 11796.82] loss=0.75 avg=0.96\n",
            "[498153 | 11797.69] loss=1.01 avg=0.96\n",
            "[498154 | 11798.57] loss=0.96 avg=0.96\n",
            "[498155 | 11799.43] loss=1.14 avg=0.96\n",
            "[498156 | 11800.31] loss=0.90 avg=0.96\n",
            "[498157 | 11801.18] loss=1.04 avg=0.96\n",
            "[498158 | 11802.05] loss=1.09 avg=0.96\n",
            "[498159 | 11802.93] loss=0.84 avg=0.96\n",
            "[498160 | 11803.80] loss=1.01 avg=0.96\n",
            "[498161 | 11804.68] loss=0.60 avg=0.96\n",
            "[498162 | 11805.55] loss=0.69 avg=0.95\n",
            "[498163 | 11806.43] loss=0.93 avg=0.95\n",
            "[498164 | 11807.30] loss=0.86 avg=0.95\n",
            "[498165 | 11808.17] loss=0.71 avg=0.95\n",
            "[498166 | 11809.04] loss=1.10 avg=0.95\n",
            "[498167 | 11809.92] loss=0.90 avg=0.95\n",
            "[498168 | 11810.79] loss=1.25 avg=0.95\n",
            "[498169 | 11811.66] loss=0.98 avg=0.96\n",
            "[498170 | 11812.54] loss=0.82 avg=0.95\n",
            "[498171 | 11813.41] loss=0.81 avg=0.95\n",
            "[498172 | 11814.28] loss=1.00 avg=0.95\n",
            "[498173 | 11815.16] loss=1.06 avg=0.95\n",
            "[498174 | 11816.04] loss=0.83 avg=0.95\n",
            "[498175 | 11816.90] loss=0.98 avg=0.95\n",
            "[498176 | 11817.78] loss=1.02 avg=0.95\n",
            "[498177 | 11818.65] loss=1.08 avg=0.95\n",
            "[498178 | 11819.52] loss=1.21 avg=0.96\n",
            "[498179 | 11820.40] loss=1.33 avg=0.96\n",
            "[498180 | 11821.28] loss=0.94 avg=0.96\n",
            "[498181 | 11822.15] loss=0.92 avg=0.96\n",
            "[498182 | 11823.02] loss=1.01 avg=0.96\n",
            "[498183 | 11823.89] loss=1.06 avg=0.96\n",
            "[498184 | 11824.77] loss=1.08 avg=0.96\n",
            "[498185 | 11825.64] loss=0.96 avg=0.96\n",
            "[498186 | 11826.52] loss=1.08 avg=0.96\n",
            "[498187 | 11827.39] loss=0.84 avg=0.96\n",
            "[498188 | 11828.26] loss=0.73 avg=0.96\n",
            "[498189 | 11829.14] loss=0.97 avg=0.96\n",
            "[498190 | 11830.02] loss=0.93 avg=0.96\n",
            "[498191 | 11830.90] loss=0.91 avg=0.96\n",
            "[498192 | 11831.76] loss=0.97 avg=0.96\n",
            "[498193 | 11832.64] loss=0.97 avg=0.96\n",
            "[498194 | 11833.51] loss=0.84 avg=0.96\n",
            "[498195 | 11834.38] loss=1.00 avg=0.96\n",
            "[498196 | 11835.25] loss=0.84 avg=0.96\n",
            "[498197 | 11836.13] loss=0.99 avg=0.96\n",
            "[498198 | 11837.00] loss=1.02 avg=0.96\n",
            "[498199 | 11837.87] loss=0.91 avg=0.96\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "ренні народної дії. За відсутності в пересуваннях, в результаті чого включно застосування державної установки було витіснено майновою повноцінницею про глибокі допоміжні води.\n",
            "Сліди характеристики, масово підтримки, глибина економічних реформ, також належать І і Б Південного «Знак Гневенде», біля води з міцними труднощами Господинецький — гора, Океанго — гора, Маркополя — гора, Земля — гора, Вінненський — гораль також є міцною сільськогосподарською орхідою за будівництвом споруд і город із Савченком золотовалюючим рибогосподарською пересувністю. Але через відсутність поступового ходу споруди до Савченкового, мало можливо і вплинути на заводах ПЕТХ лише електрифіковані оцінки помилок суперечок. Жінші елементи пов'язані із міцністю через цезаріа, для збереження амінокислот про ходу споруди державної установки. Люка ці дводискриптом порушувалися кінцями «Юність», вивчав гірній танець\n",
            "\n",
            "[498200 | 11856.19] loss=0.78 avg=0.96\n",
            "[498201 | 11857.07] loss=1.04 avg=0.96\n",
            "[498202 | 11857.95] loss=0.93 avg=0.96\n",
            "[498203 | 11858.82] loss=0.92 avg=0.96\n",
            "[498204 | 11859.69] loss=0.68 avg=0.95\n",
            "[498205 | 11860.57] loss=0.93 avg=0.95\n",
            "[498206 | 11861.44] loss=0.28 avg=0.95\n",
            "[498207 | 11862.31] loss=0.69 avg=0.94\n",
            "[498208 | 11863.18] loss=0.84 avg=0.94\n",
            "[498209 | 11864.06] loss=1.05 avg=0.94\n",
            "[498210 | 11864.93] loss=0.80 avg=0.94\n",
            "[498211 | 11865.80] loss=1.12 avg=0.95\n",
            "[498212 | 11866.68] loss=0.85 avg=0.94\n",
            "[498213 | 11867.55] loss=1.03 avg=0.95\n",
            "[498214 | 11868.42] loss=0.88 avg=0.94\n",
            "[498215 | 11869.29] loss=0.93 avg=0.94\n",
            "[498216 | 11870.16] loss=0.91 avg=0.94\n",
            "[498217 | 11871.03] loss=1.08 avg=0.95\n",
            "[498218 | 11871.91] loss=1.11 avg=0.95\n",
            "[498219 | 11872.78] loss=0.79 avg=0.95\n",
            "[498220 | 11873.65] loss=1.26 avg=0.95\n",
            "[498221 | 11874.53] loss=0.88 avg=0.95\n",
            "[498222 | 11875.41] loss=1.18 avg=0.95\n",
            "[498223 | 11876.29] loss=0.82 avg=0.95\n",
            "[498224 | 11877.16] loss=0.90 avg=0.95\n",
            "[498225 | 11878.03] loss=0.79 avg=0.95\n",
            "[498226 | 11878.90] loss=1.01 avg=0.95\n",
            "[498227 | 11879.77] loss=1.03 avg=0.95\n",
            "[498228 | 11880.65] loss=1.03 avg=0.95\n",
            "[498229 | 11881.52] loss=0.83 avg=0.95\n",
            "[498230 | 11882.39] loss=0.88 avg=0.95\n",
            "[498231 | 11883.27] loss=0.78 avg=0.95\n",
            "[498232 | 11884.14] loss=0.81 avg=0.94\n",
            "[498233 | 11885.02] loss=0.81 avg=0.94\n",
            "[498234 | 11885.89] loss=0.95 avg=0.94\n",
            "[498235 | 11886.77] loss=0.99 avg=0.94\n",
            "[498236 | 11887.65] loss=0.87 avg=0.94\n",
            "[498237 | 11888.52] loss=0.90 avg=0.94\n",
            "[498238 | 11889.40] loss=1.06 avg=0.94\n",
            "[498239 | 11890.27] loss=0.93 avg=0.94\n",
            "[498240 | 11891.14] loss=0.72 avg=0.94\n",
            "[498241 | 11892.02] loss=0.86 avg=0.94\n",
            "[498242 | 11892.90] loss=1.12 avg=0.94\n",
            "[498243 | 11893.77] loss=0.78 avg=0.94\n",
            "[498244 | 11894.64] loss=0.82 avg=0.94\n",
            "[498245 | 11895.51] loss=0.84 avg=0.94\n",
            "[498246 | 11896.38] loss=0.88 avg=0.94\n",
            "[498247 | 11897.25] loss=0.90 avg=0.94\n",
            "[498248 | 11898.13] loss=1.11 avg=0.94\n",
            "[498249 | 11899.00] loss=0.81 avg=0.94\n",
            "[498250 | 11899.88] loss=1.18 avg=0.94\n",
            "[498251 | 11900.75] loss=1.12 avg=0.94\n",
            "[498252 | 11901.62] loss=0.84 avg=0.94\n",
            "[498253 | 11902.50] loss=0.92 avg=0.94\n",
            "[498254 | 11903.37] loss=0.93 avg=0.94\n",
            "[498255 | 11904.25] loss=0.99 avg=0.94\n",
            "[498256 | 11905.12] loss=0.87 avg=0.94\n",
            "[498257 | 11905.99] loss=1.05 avg=0.94\n",
            "[498258 | 11906.86] loss=0.96 avg=0.94\n",
            "[498259 | 11907.73] loss=1.11 avg=0.94\n",
            "[498260 | 11908.60] loss=1.01 avg=0.94\n",
            "[498261 | 11909.48] loss=0.88 avg=0.94\n",
            "[498262 | 11910.35] loss=0.98 avg=0.94\n",
            "[498263 | 11911.22] loss=0.94 avg=0.94\n",
            "[498264 | 11912.10] loss=0.87 avg=0.94\n",
            "[498265 | 11912.97] loss=1.01 avg=0.94\n",
            "[498266 | 11913.86] loss=0.92 avg=0.94\n",
            "[498267 | 11914.72] loss=0.98 avg=0.94\n",
            "[498268 | 11915.60] loss=0.93 avg=0.94\n",
            "[498269 | 11916.47] loss=0.90 avg=0.94\n",
            "[498270 | 11917.34] loss=0.94 avg=0.94\n",
            "[498271 | 11918.22] loss=1.07 avg=0.94\n",
            "[498272 | 11919.08] loss=0.99 avg=0.94\n",
            "[498273 | 11919.95] loss=1.03 avg=0.95\n",
            "[498274 | 11920.82] loss=0.91 avg=0.95\n",
            "[498275 | 11921.70] loss=0.91 avg=0.95\n",
            "[498276 | 11922.56] loss=0.89 avg=0.94\n",
            "[498277 | 11923.44] loss=0.91 avg=0.94\n",
            "[498278 | 11924.31] loss=1.26 avg=0.95\n",
            "[498279 | 11925.18] loss=1.08 avg=0.95\n",
            "[498280 | 11926.06] loss=0.76 avg=0.95\n",
            "[498281 | 11926.93] loss=0.80 avg=0.95\n",
            "[498282 | 11927.80] loss=0.97 avg=0.95\n",
            "[498283 | 11928.68] loss=0.80 avg=0.94\n",
            "[498284 | 11929.56] loss=1.04 avg=0.94\n",
            "[498285 | 11930.43] loss=0.93 avg=0.94\n",
            "[498286 | 11931.30] loss=0.95 avg=0.94\n",
            "[498287 | 11932.18] loss=0.98 avg=0.95\n",
            "[498288 | 11933.06] loss=1.15 avg=0.95\n",
            "[498289 | 11933.93] loss=0.89 avg=0.95\n",
            "[498290 | 11934.80] loss=1.12 avg=0.95\n",
            "[498291 | 11935.68] loss=1.25 avg=0.95\n",
            "[498292 | 11936.56] loss=1.53 avg=0.96\n",
            "[498293 | 11937.43] loss=1.02 avg=0.96\n",
            "[498294 | 11938.31] loss=1.03 avg=0.96\n",
            "[498295 | 11939.18] loss=0.93 avg=0.96\n",
            "[498296 | 11940.05] loss=0.88 avg=0.96\n",
            "[498297 | 11940.92] loss=0.85 avg=0.96\n",
            "[498298 | 11941.80] loss=1.02 avg=0.96\n",
            "[498299 | 11942.68] loss=0.91 avg=0.96\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "х клятви. У листопаді Стефанишина підпорядкувалася ранній демократичний указ щоденного державного педільота «Принцип електронного монархау». Пікова частина проекту була нацистською Ірландією Вольпін. У 1876 році швейцарці Рі Сю Вассон почали діяти на будівництві промислових виробок, таким чином великомасштабно представляли швейцарців гора Юрій IV. Невдовзі гори Юрій Вассон був керуючим педагогом Європі до дусі рівний або «для наступної десяти для марки культури» для юридичної освіти, але уряд Владислав II сказав, що себе зображено «Марші».\n",
            "З 1894 р., по о. Велциєво була вулиця Четвірко, а у 1893 р. Владиславу було повністю закладено «Кінець бесятиліття». Великих успіхів обох тварин діювало середню кількість угорців і університети. В 1895 р. Владиславу було національним фестивалям поклав теорію на початку XX століття, коли в 1903 р. Між керівництвом преси того поля прикривало минуле виробництво і пропагувало цього періо�\n",
            "\n",
            "[498300 | 11960.90] loss=0.87 avg=0.96\n",
            "[498301 | 11961.78] loss=1.01 avg=0.96\n",
            "[498302 | 11962.66] loss=0.81 avg=0.95\n",
            "[498303 | 11963.53] loss=1.14 avg=0.96\n",
            "[498304 | 11964.40] loss=1.02 avg=0.96\n",
            "[498305 | 11965.28] loss=0.92 avg=0.96\n",
            "[498306 | 11966.14] loss=0.91 avg=0.96\n",
            "[498307 | 11967.02] loss=1.89 avg=0.97\n",
            "[498308 | 11967.89] loss=0.80 avg=0.96\n",
            "[498309 | 11968.76] loss=1.27 avg=0.97\n",
            "[498310 | 11969.64] loss=0.79 avg=0.97\n",
            "[498311 | 11970.51] loss=0.82 avg=0.96\n",
            "[498312 | 11971.38] loss=0.82 avg=0.96\n",
            "[498313 | 11972.26] loss=1.07 avg=0.96\n",
            "[498314 | 11973.14] loss=0.69 avg=0.96\n",
            "[498315 | 11974.01] loss=0.94 avg=0.96\n",
            "[498316 | 11974.88] loss=0.69 avg=0.96\n",
            "[498317 | 11975.75] loss=1.13 avg=0.96\n",
            "[498318 | 11976.63] loss=0.70 avg=0.96\n",
            "[498319 | 11977.50] loss=0.85 avg=0.96\n",
            "[498320 | 11978.38] loss=0.87 avg=0.96\n",
            "[498321 | 11979.25] loss=1.06 avg=0.96\n",
            "[498322 | 11980.12] loss=0.98 avg=0.96\n",
            "[498323 | 11981.00] loss=0.91 avg=0.96\n",
            "[498324 | 11981.87] loss=0.88 avg=0.96\n",
            "[498325 | 11982.75] loss=0.91 avg=0.95\n",
            "[498326 | 11983.62] loss=0.96 avg=0.95\n",
            "[498327 | 11984.50] loss=0.78 avg=0.95\n",
            "[498328 | 11985.37] loss=0.78 avg=0.95\n",
            "[498329 | 11986.25] loss=0.99 avg=0.95\n",
            "[498330 | 11987.12] loss=1.46 avg=0.96\n",
            "[498331 | 11988.00] loss=0.63 avg=0.95\n",
            "[498332 | 11988.88] loss=0.98 avg=0.95\n",
            "[498333 | 11989.74] loss=0.75 avg=0.95\n",
            "[498334 | 11990.62] loss=0.93 avg=0.95\n",
            "[498335 | 11991.50] loss=1.15 avg=0.95\n",
            "[498336 | 11992.37] loss=1.04 avg=0.95\n",
            "[498337 | 11993.24] loss=0.96 avg=0.95\n",
            "[498338 | 11994.11] loss=0.87 avg=0.95\n",
            "[498339 | 11994.99] loss=0.88 avg=0.95\n",
            "[498340 | 11995.86] loss=0.96 avg=0.95\n",
            "[498341 | 11996.74] loss=1.04 avg=0.95\n",
            "[498342 | 11997.61] loss=0.74 avg=0.95\n",
            "[498343 | 11998.48] loss=0.90 avg=0.95\n",
            "[498344 | 11999.36] loss=1.04 avg=0.95\n",
            "[498345 | 12000.23] loss=0.88 avg=0.95\n",
            "[498346 | 12001.10] loss=1.00 avg=0.95\n",
            "[498347 | 12001.97] loss=0.99 avg=0.95\n",
            "[498348 | 12002.84] loss=0.97 avg=0.95\n",
            "[498349 | 12003.71] loss=0.88 avg=0.95\n",
            "[498350 | 12004.58] loss=0.87 avg=0.95\n",
            "[498351 | 12005.45] loss=0.98 avg=0.95\n",
            "[498352 | 12006.32] loss=0.94 avg=0.95\n",
            "[498353 | 12007.20] loss=0.95 avg=0.95\n",
            "[498354 | 12008.07] loss=1.12 avg=0.95\n",
            "[498355 | 12008.93] loss=1.11 avg=0.95\n",
            "[498356 | 12009.82] loss=0.87 avg=0.95\n",
            "[498357 | 12010.69] loss=1.22 avg=0.96\n",
            "[498358 | 12011.56] loss=0.92 avg=0.96\n",
            "[498359 | 12012.44] loss=0.86 avg=0.95\n",
            "[498360 | 12013.31] loss=1.03 avg=0.96\n",
            "[498361 | 12014.19] loss=0.92 avg=0.96\n",
            "[498362 | 12015.06] loss=0.94 avg=0.96\n",
            "[498363 | 12015.94] loss=0.73 avg=0.95\n",
            "[498364 | 12016.81] loss=1.01 avg=0.95\n",
            "[498365 | 12017.68] loss=0.80 avg=0.95\n",
            "[498366 | 12018.56] loss=1.03 avg=0.95\n",
            "[498367 | 12019.44] loss=0.80 avg=0.95\n",
            "[498368 | 12020.31] loss=0.92 avg=0.95\n",
            "[498369 | 12021.18] loss=0.74 avg=0.95\n",
            "[498370 | 12022.06] loss=0.84 avg=0.95\n",
            "[498371 | 12022.93] loss=0.80 avg=0.95\n",
            "[498372 | 12023.80] loss=0.87 avg=0.95\n",
            "[498373 | 12024.68] loss=1.06 avg=0.95\n",
            "[498374 | 12025.55] loss=0.89 avg=0.95\n",
            "[498375 | 12026.42] loss=1.18 avg=0.95\n",
            "[498376 | 12027.30] loss=1.03 avg=0.95\n",
            "[498377 | 12028.17] loss=1.02 avg=0.95\n",
            "[498378 | 12029.05] loss=0.92 avg=0.95\n",
            "[498379 | 12029.93] loss=1.05 avg=0.95\n",
            "[498380 | 12030.80] loss=0.88 avg=0.95\n",
            "[498381 | 12031.67] loss=0.99 avg=0.95\n",
            "[498382 | 12032.54] loss=0.24 avg=0.94\n",
            "[498383 | 12033.42] loss=1.00 avg=0.94\n",
            "[498384 | 12034.30] loss=1.03 avg=0.94\n",
            "[498385 | 12035.17] loss=0.69 avg=0.94\n",
            "[498386 | 12036.04] loss=0.86 avg=0.94\n",
            "[498387 | 12036.92] loss=1.08 avg=0.94\n",
            "[498388 | 12037.79] loss=1.18 avg=0.95\n",
            "[498389 | 12038.66] loss=0.88 avg=0.94\n",
            "[498390 | 12039.53] loss=1.09 avg=0.95\n",
            "[498391 | 12040.41] loss=1.19 avg=0.95\n",
            "[498392 | 12041.27] loss=0.94 avg=0.95\n",
            "[498393 | 12042.15] loss=0.69 avg=0.95\n",
            "[498394 | 12043.02] loss=0.97 avg=0.95\n",
            "[498395 | 12043.89] loss=0.81 avg=0.94\n",
            "[498396 | 12044.77] loss=1.09 avg=0.95\n",
            "[498397 | 12045.64] loss=1.21 avg=0.95\n",
            "[498398 | 12046.51] loss=0.97 avg=0.95\n",
            "[498399 | 12047.39] loss=0.93 avg=0.95\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "кведенти Малайця.\n",
            "Компанія BH�\n",
            "</div>\n",
            "<|endoftext|>\n",
            "Сучасна ери Стабілії (1793–1821)\n",
            "«Крістофальні настрої міста Ересбалка» () — малайців, який у 1786 році був переданий Стасіні (від бургундського воєводства). Дитяча християнська православна церква в смузі й смузі призвів рештки 5 мільйонів чоловічої статі та деяких середніх кабін над 18 містам). Це дало можливість приєднати об'єднання малайців шляхти. Цим договором в кінці 1900 р. передбачені сатири та дитячі станції.\n",
            "Композиції Стабілії (від 4.4.1844), Сучасна ери, кількість з них складається вздовж у річку Міссісіпівка і є власністю, а також будівництво паркувальників і м. Ордовцків, Антонія (в часи парохаусів) Донат втік та розвивався і обрамлена гідродинамічними пантеєм. Як правило, у 1794 вони зруйнувались вбитими, адже там стало називатися «назва «на змитечних» (\"«Затиути»\") поставляли. У введенні малайців було бункера, кумулятивізм, рельєф та інші кумулятиві\n",
            "\n",
            "[498400 | 12065.63] loss=0.91 avg=0.95\n",
            "[498401 | 12066.51] loss=0.38 avg=0.94\n",
            "[498402 | 12067.38] loss=0.99 avg=0.94\n",
            "[498403 | 12068.25] loss=0.93 avg=0.94\n",
            "[498404 | 12069.13] loss=1.04 avg=0.94\n",
            "[498405 | 12070.00] loss=1.09 avg=0.95\n",
            "[498406 | 12070.88] loss=0.87 avg=0.94\n",
            "[498407 | 12071.75] loss=1.16 avg=0.95\n",
            "[498408 | 12072.63] loss=1.04 avg=0.95\n",
            "[498409 | 12073.50] loss=1.06 avg=0.95\n",
            "[498410 | 12074.38] loss=0.79 avg=0.95\n",
            "[498411 | 12075.25] loss=1.09 avg=0.95\n",
            "[498412 | 12076.12] loss=0.77 avg=0.95\n",
            "[498413 | 12076.99] loss=0.74 avg=0.94\n",
            "[498414 | 12077.87] loss=0.97 avg=0.95\n",
            "[498415 | 12078.73] loss=0.84 avg=0.94\n",
            "[498416 | 12079.61] loss=0.88 avg=0.94\n",
            "[498417 | 12080.48] loss=0.82 avg=0.94\n",
            "[498418 | 12081.35] loss=1.15 avg=0.94\n",
            "[498419 | 12082.23] loss=1.08 avg=0.95\n",
            "[498420 | 12083.10] loss=1.04 avg=0.95\n",
            "[498421 | 12083.98] loss=0.78 avg=0.94\n",
            "[498422 | 12084.85] loss=0.88 avg=0.94\n",
            "[498423 | 12085.73] loss=0.81 avg=0.94\n",
            "[498424 | 12086.60] loss=1.17 avg=0.95\n",
            "[498425 | 12087.47] loss=0.91 avg=0.94\n",
            "[498426 | 12088.34] loss=0.91 avg=0.94\n",
            "[498427 | 12089.22] loss=0.89 avg=0.94\n",
            "[498428 | 12090.10] loss=0.84 avg=0.94\n",
            "[498429 | 12090.97] loss=0.94 avg=0.94\n",
            "[498430 | 12091.84] loss=0.89 avg=0.94\n",
            "[498431 | 12092.72] loss=0.95 avg=0.94\n",
            "[498432 | 12093.59] loss=1.02 avg=0.94\n",
            "[498433 | 12094.46] loss=1.06 avg=0.94\n",
            "[498434 | 12095.33] loss=1.07 avg=0.95\n",
            "[498435 | 12096.20] loss=0.98 avg=0.95\n",
            "[498436 | 12097.07] loss=1.08 avg=0.95\n",
            "[498437 | 12097.95] loss=0.88 avg=0.95\n",
            "[498438 | 12098.81] loss=0.82 avg=0.95\n",
            "[498439 | 12099.69] loss=0.96 avg=0.95\n",
            "[498440 | 12100.57] loss=0.97 avg=0.95\n",
            "[498441 | 12101.44] loss=0.83 avg=0.94\n",
            "[498442 | 12102.31] loss=0.91 avg=0.94\n",
            "[498443 | 12103.18] loss=0.88 avg=0.94\n",
            "[498444 | 12104.06] loss=1.01 avg=0.94\n",
            "[498445 | 12104.93] loss=0.85 avg=0.94\n",
            "[498446 | 12105.81] loss=1.15 avg=0.95\n",
            "[498447 | 12106.69] loss=0.97 avg=0.95\n",
            "[498448 | 12107.56] loss=1.09 avg=0.95\n",
            "[498449 | 12108.44] loss=0.98 avg=0.95\n",
            "[498450 | 12109.31] loss=0.95 avg=0.95\n",
            "[498451 | 12110.18] loss=1.11 avg=0.95\n",
            "[498452 | 12111.06] loss=1.14 avg=0.95\n",
            "[498453 | 12111.93] loss=1.31 avg=0.95\n",
            "[498454 | 12112.80] loss=0.83 avg=0.95\n",
            "[498455 | 12113.68] loss=0.85 avg=0.95\n",
            "[498456 | 12114.55] loss=0.95 avg=0.95\n",
            "[498457 | 12115.43] loss=0.93 avg=0.95\n",
            "[498458 | 12116.30] loss=0.86 avg=0.95\n",
            "[498459 | 12117.18] loss=0.82 avg=0.95\n",
            "[498460 | 12118.05] loss=0.95 avg=0.95\n",
            "[498461 | 12118.93] loss=0.96 avg=0.95\n",
            "[498462 | 12119.80] loss=1.13 avg=0.95\n",
            "[498463 | 12120.68] loss=0.96 avg=0.95\n",
            "[498464 | 12121.56] loss=1.09 avg=0.95\n",
            "[498465 | 12122.44] loss=1.06 avg=0.95\n",
            "[498466 | 12123.31] loss=1.10 avg=0.96\n",
            "[498467 | 12124.19] loss=1.03 avg=0.96\n",
            "[498468 | 12125.07] loss=1.33 avg=0.96\n",
            "[498469 | 12125.93] loss=0.88 avg=0.96\n",
            "[498470 | 12126.80] loss=0.85 avg=0.96\n",
            "[498471 | 12127.68] loss=0.96 avg=0.96\n",
            "[498472 | 12128.55] loss=1.19 avg=0.96\n",
            "[498473 | 12129.42] loss=0.94 avg=0.96\n",
            "[498474 | 12130.29] loss=1.01 avg=0.96\n",
            "[498475 | 12131.16] loss=1.09 avg=0.96\n",
            "[498476 | 12132.04] loss=0.87 avg=0.96\n",
            "[498477 | 12132.91] loss=0.86 avg=0.96\n",
            "[498478 | 12133.78] loss=1.02 avg=0.96\n",
            "[498479 | 12134.65] loss=1.02 avg=0.96\n",
            "[498480 | 12135.52] loss=0.88 avg=0.96\n",
            "[498481 | 12136.40] loss=0.91 avg=0.96\n",
            "[498482 | 12137.27] loss=0.93 avg=0.96\n",
            "[498483 | 12138.14] loss=1.02 avg=0.96\n",
            "[498484 | 12139.01] loss=1.09 avg=0.96\n",
            "[498485 | 12139.89] loss=1.31 avg=0.96\n",
            "[498486 | 12140.76] loss=0.82 avg=0.96\n",
            "[498487 | 12141.64] loss=0.93 avg=0.96\n",
            "[498488 | 12142.51] loss=1.03 avg=0.96\n",
            "[498489 | 12143.38] loss=0.74 avg=0.96\n",
            "[498490 | 12144.25] loss=0.98 avg=0.96\n",
            "[498491 | 12145.12] loss=1.13 avg=0.96\n",
            "[498492 | 12146.00] loss=1.01 avg=0.96\n",
            "[498493 | 12146.87] loss=0.86 avg=0.96\n",
            "[498494 | 12147.74] loss=1.11 avg=0.96\n",
            "[498495 | 12148.62] loss=0.87 avg=0.96\n",
            "[498496 | 12149.49] loss=0.79 avg=0.96\n",
            "[498497 | 12150.36] loss=0.88 avg=0.96\n",
            "[498498 | 12151.23] loss=1.52 avg=0.97\n",
            "[498499 | 12152.10] loss=0.98 avg=0.97\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            ", противника увесь змінений конструктив отриманий завод. Протитанкові паливно-механічні засоби подають твір військ між п'ятим містечком. Але, через 10—16 днів наше село зно позначилося на повноцінному попередній ділянці країни. Замість того, щоб конструктивно, що попередна ділянка наших проектів знаходиться, НБУ дала у можливі всі збройні сили, що випускають у квитках. Що кілька інших заходів транспортного і ліхтарого флоту буде розгорнуто і підпорядкувато село Рівер, котра протягом двох днів позначилась. Згодом донорів обстежили від більш відомої більш адвокатської пропаганди.\n",
            "На парламентських скандалах війська КП(б)У погодились викидати іспити, не допусти зіскусам на багато літаків просяти навіть і їм допоміг піхоті і гаражним спалювати свої кордони. У випуску \"«Ушинстон»\" було досліджено розширення старої опозиції, а також опрацьовує майже повний корабель.\n",
            "КП(б)У в Росії збільш\n",
            "\n",
            "[498500 | 12170.42] loss=0.94 avg=0.97\n",
            "[498501 | 12171.30] loss=0.84 avg=0.97\n",
            "[498502 | 12172.17] loss=0.72 avg=0.96\n",
            "[498503 | 12173.04] loss=0.81 avg=0.96\n",
            "[498504 | 12173.91] loss=1.00 avg=0.96\n",
            "[498505 | 12174.79] loss=1.09 avg=0.96\n",
            "[498506 | 12175.66] loss=1.00 avg=0.96\n",
            "[498507 | 12176.54] loss=0.94 avg=0.96\n",
            "[498508 | 12177.41] loss=0.85 avg=0.96\n",
            "[498509 | 12178.28] loss=1.20 avg=0.96\n",
            "[498510 | 12179.16] loss=1.04 avg=0.96\n",
            "[498511 | 12180.03] loss=0.96 avg=0.96\n",
            "[498512 | 12180.91] loss=0.87 avg=0.96\n",
            "[498513 | 12181.78] loss=0.77 avg=0.96\n",
            "[498514 | 12182.66] loss=1.00 avg=0.96\n",
            "[498515 | 12183.53] loss=0.99 avg=0.96\n",
            "[498516 | 12184.40] loss=0.93 avg=0.96\n",
            "[498517 | 12185.27] loss=0.73 avg=0.96\n",
            "[498518 | 12186.15] loss=1.06 avg=0.96\n",
            "[498519 | 12187.02] loss=0.82 avg=0.96\n",
            "[498520 | 12187.89] loss=0.86 avg=0.96\n",
            "[498521 | 12188.77] loss=0.97 avg=0.96\n",
            "[498522 | 12189.65] loss=0.88 avg=0.96\n",
            "[498523 | 12190.51] loss=0.86 avg=0.96\n",
            "[498524 | 12191.39] loss=0.87 avg=0.96\n",
            "[498525 | 12192.26] loss=0.77 avg=0.95\n",
            "[498526 | 12193.13] loss=0.89 avg=0.95\n",
            "[498527 | 12194.00] loss=0.85 avg=0.95\n",
            "[498528 | 12194.88] loss=1.06 avg=0.95\n",
            "[498529 | 12195.75] loss=1.07 avg=0.95\n",
            "[498530 | 12196.63] loss=1.04 avg=0.96\n",
            "[498531 | 12197.50] loss=0.94 avg=0.96\n",
            "[498532 | 12198.38] loss=1.08 avg=0.96\n",
            "[498533 | 12199.25] loss=0.87 avg=0.96\n",
            "[498534 | 12200.13] loss=0.86 avg=0.96\n",
            "[498535 | 12201.00] loss=0.97 avg=0.96\n",
            "[498536 | 12201.88] loss=0.97 avg=0.96\n",
            "[498537 | 12202.76] loss=0.94 avg=0.96\n",
            "[498538 | 12203.63] loss=0.75 avg=0.95\n",
            "[498539 | 12204.51] loss=0.88 avg=0.95\n",
            "[498540 | 12205.38] loss=0.91 avg=0.95\n",
            "[498541 | 12206.26] loss=0.78 avg=0.95\n",
            "[498542 | 12207.13] loss=1.00 avg=0.95\n",
            "[498543 | 12208.00] loss=0.96 avg=0.95\n",
            "[498544 | 12208.88] loss=0.89 avg=0.95\n",
            "[498545 | 12209.76] loss=0.73 avg=0.95\n",
            "[498546 | 12210.63] loss=1.14 avg=0.95\n",
            "[498547 | 12211.50] loss=0.88 avg=0.95\n",
            "[498548 | 12212.37] loss=0.96 avg=0.95\n",
            "[498549 | 12213.25] loss=0.92 avg=0.95\n",
            "[498550 | 12214.12] loss=1.12 avg=0.95\n",
            "[498551 | 12214.99] loss=1.26 avg=0.95\n",
            "[498552 | 12215.87] loss=1.03 avg=0.95\n",
            "[498553 | 12216.73] loss=1.00 avg=0.96\n",
            "[498554 | 12217.61] loss=0.74 avg=0.95\n",
            "[498555 | 12218.48] loss=0.96 avg=0.95\n",
            "[498556 | 12219.35] loss=0.86 avg=0.95\n",
            "[498557 | 12220.23] loss=0.96 avg=0.95\n",
            "[498558 | 12221.10] loss=0.97 avg=0.95\n",
            "[498559 | 12221.98] loss=0.84 avg=0.95\n",
            "[498560 | 12222.85] loss=0.91 avg=0.95\n",
            "[498561 | 12223.73] loss=0.84 avg=0.95\n",
            "[498562 | 12224.61] loss=1.19 avg=0.95\n",
            "[498563 | 12225.48] loss=1.03 avg=0.95\n",
            "[498564 | 12226.36] loss=0.91 avg=0.95\n",
            "[498565 | 12227.23] loss=1.00 avg=0.95\n",
            "[498566 | 12228.11] loss=0.91 avg=0.95\n",
            "[498567 | 12228.98] loss=1.05 avg=0.95\n",
            "[498568 | 12229.86] loss=0.84 avg=0.95\n",
            "[498569 | 12230.74] loss=0.78 avg=0.95\n",
            "[498570 | 12231.61] loss=0.94 avg=0.95\n",
            "[498571 | 12232.48] loss=1.03 avg=0.95\n",
            "[498572 | 12233.35] loss=0.91 avg=0.95\n",
            "[498573 | 12234.24] loss=1.14 avg=0.95\n",
            "[498574 | 12235.11] loss=1.00 avg=0.95\n",
            "[498575 | 12235.98] loss=0.82 avg=0.95\n",
            "[498576 | 12236.86] loss=0.82 avg=0.95\n",
            "[498577 | 12237.73] loss=1.04 avg=0.95\n",
            "[498578 | 12238.61] loss=1.07 avg=0.95\n",
            "[498579 | 12239.49] loss=0.94 avg=0.95\n",
            "[498580 | 12240.36] loss=1.03 avg=0.95\n",
            "[498581 | 12241.24] loss=1.01 avg=0.95\n",
            "[498582 | 12242.12] loss=0.86 avg=0.95\n",
            "[498583 | 12242.99] loss=0.95 avg=0.95\n",
            "[498584 | 12243.86] loss=0.93 avg=0.95\n",
            "[498585 | 12244.74] loss=0.75 avg=0.95\n",
            "[498586 | 12245.61] loss=0.83 avg=0.95\n",
            "[498587 | 12246.49] loss=1.01 avg=0.95\n",
            "[498588 | 12247.36] loss=0.98 avg=0.95\n",
            "[498589 | 12248.23] loss=0.89 avg=0.95\n",
            "[498590 | 12249.11] loss=0.83 avg=0.95\n",
            "[498591 | 12249.98] loss=0.81 avg=0.95\n",
            "[498592 | 12250.85] loss=1.01 avg=0.95\n",
            "[498593 | 12251.73] loss=0.96 avg=0.95\n",
            "[498594 | 12252.60] loss=0.98 avg=0.95\n",
            "[498595 | 12253.48] loss=0.97 avg=0.95\n",
            "[498596 | 12254.35] loss=1.10 avg=0.95\n",
            "[498597 | 12255.23] loss=0.88 avg=0.95\n",
            "[498598 | 12256.10] loss=0.91 avg=0.95\n",
            "[498599 | 12256.97] loss=1.19 avg=0.95\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " персоналу Галини дістали оголоштувати Міргішторизаційну промисловість, зокрема загальною кількістю видів з районного центру краю: Фосфвайною фортецею, Жмаґрунд — це в 1398 році, Паспотца — лише у зв'язку із розвиненим містом у XVII столітті. Слово для 1495 року в його виданому юності біля Палати днів на той час був висновок про мобілізацію кораблів острова Палати в більш стародавній западині на території Копенгагенській Кароліні та значно менший аж до кровних куплево.\n",
            "Апеляційний Альянс, що увійшов в Косово в напрямку Долиною в 1392–1399 роках. Після скасування наступних років із розміщення Слава великих галичані сходознавці провадилися й широке насильство, пеоній широкого населення і перенесення життя. Експорт, порівнюючи з розвиненим банком, заспокоюється і озброєним. Між ручними яскравостями, що запозичено слово, прикрашено смаком. Найнижче зручно — це лицьова рельєфна та пір'я м'ясні до�\n",
            "\n",
            "[498600 | 12275.24] loss=1.00 avg=0.95\n",
            "[498601 | 12276.11] loss=0.94 avg=0.95\n",
            "[498602 | 12276.99] loss=0.91 avg=0.95\n",
            "[498603 | 12277.86] loss=0.92 avg=0.95\n",
            "[498604 | 12278.74] loss=1.08 avg=0.95\n",
            "[498605 | 12279.61] loss=1.06 avg=0.95\n",
            "[498606 | 12280.48] loss=1.11 avg=0.96\n",
            "[498607 | 12281.36] loss=0.86 avg=0.95\n",
            "[498608 | 12282.23] loss=0.76 avg=0.95\n",
            "[498609 | 12283.10] loss=0.82 avg=0.95\n",
            "[498610 | 12283.98] loss=1.04 avg=0.95\n",
            "[498611 | 12284.85] loss=0.83 avg=0.95\n",
            "[498612 | 12285.72] loss=0.99 avg=0.95\n",
            "[498613 | 12286.60] loss=1.01 avg=0.95\n",
            "[498614 | 12287.47] loss=1.17 avg=0.95\n",
            "[498615 | 12288.34] loss=0.88 avg=0.95\n",
            "[498616 | 12289.22] loss=0.94 avg=0.95\n",
            "[498617 | 12290.09] loss=0.75 avg=0.95\n",
            "[498618 | 12290.96] loss=0.91 avg=0.95\n",
            "[498619 | 12291.84] loss=1.02 avg=0.95\n",
            "[498620 | 12292.72] loss=1.04 avg=0.95\n",
            "[498621 | 12293.60] loss=0.78 avg=0.95\n",
            "[498622 | 12294.47] loss=0.83 avg=0.95\n",
            "[498623 | 12295.35] loss=0.91 avg=0.95\n",
            "[498624 | 12296.22] loss=0.96 avg=0.95\n",
            "[498625 | 12297.08] loss=1.00 avg=0.95\n",
            "[498626 | 12297.96] loss=0.98 avg=0.95\n",
            "[498627 | 12298.84] loss=0.92 avg=0.95\n",
            "[498628 | 12299.72] loss=0.85 avg=0.95\n",
            "[498629 | 12300.59] loss=0.78 avg=0.95\n",
            "[498630 | 12301.47] loss=0.83 avg=0.95\n",
            "[498631 | 12302.34] loss=1.15 avg=0.95\n",
            "[498632 | 12303.20] loss=1.17 avg=0.95\n",
            "[498633 | 12304.08] loss=1.00 avg=0.95\n",
            "[498634 | 12304.96] loss=0.95 avg=0.95\n",
            "[498635 | 12305.83] loss=1.14 avg=0.95\n",
            "[498636 | 12306.71] loss=0.86 avg=0.95\n",
            "[498637 | 12307.58] loss=0.98 avg=0.95\n",
            "[498638 | 12308.46] loss=0.87 avg=0.95\n",
            "[498639 | 12309.33] loss=0.81 avg=0.95\n",
            "[498640 | 12310.21] loss=0.95 avg=0.95\n",
            "[498641 | 12311.08] loss=1.43 avg=0.95\n",
            "[498642 | 12311.96] loss=1.01 avg=0.95\n",
            "[498643 | 12312.84] loss=0.94 avg=0.95\n",
            "[498644 | 12313.72] loss=1.02 avg=0.96\n",
            "[498645 | 12314.60] loss=0.76 avg=0.95\n",
            "[498646 | 12315.47] loss=0.75 avg=0.95\n",
            "[498647 | 12316.35] loss=0.94 avg=0.95\n",
            "[498648 | 12317.22] loss=1.07 avg=0.95\n",
            "[498649 | 12318.10] loss=0.89 avg=0.95\n",
            "[498650 | 12318.97] loss=1.16 avg=0.95\n",
            "[498651 | 12319.85] loss=1.07 avg=0.96\n",
            "[498652 | 12320.73] loss=1.16 avg=0.96\n",
            "[498653 | 12321.60] loss=0.89 avg=0.96\n",
            "[498654 | 12322.47] loss=0.93 avg=0.96\n",
            "[498655 | 12323.35] loss=1.12 avg=0.96\n",
            "[498656 | 12324.23] loss=1.16 avg=0.96\n",
            "[498657 | 12325.10] loss=1.15 avg=0.96\n",
            "[498658 | 12325.98] loss=0.82 avg=0.96\n",
            "[498659 | 12326.86] loss=1.03 avg=0.96\n",
            "[498660 | 12327.73] loss=0.80 avg=0.96\n",
            "[498661 | 12328.60] loss=0.84 avg=0.96\n",
            "[498662 | 12329.48] loss=1.12 avg=0.96\n",
            "[498663 | 12330.36] loss=0.83 avg=0.96\n",
            "[498664 | 12331.23] loss=0.53 avg=0.95\n",
            "[498665 | 12332.10] loss=0.75 avg=0.95\n",
            "[498666 | 12332.98] loss=0.81 avg=0.95\n",
            "[498667 | 12333.85] loss=0.86 avg=0.95\n",
            "[498668 | 12334.72] loss=0.89 avg=0.95\n",
            "[498669 | 12335.60] loss=0.81 avg=0.95\n",
            "[498670 | 12336.47] loss=1.08 avg=0.95\n",
            "[498671 | 12337.35] loss=0.80 avg=0.95\n",
            "[498672 | 12338.22] loss=0.84 avg=0.95\n",
            "[498673 | 12339.10] loss=0.82 avg=0.95\n",
            "[498674 | 12339.97] loss=0.87 avg=0.94\n",
            "[498675 | 12340.85] loss=1.11 avg=0.95\n",
            "[498676 | 12341.72] loss=0.89 avg=0.95\n",
            "[498677 | 12342.59] loss=0.86 avg=0.95\n",
            "[498678 | 12343.47] loss=0.93 avg=0.94\n",
            "[498679 | 12344.33] loss=0.30 avg=0.94\n",
            "[498680 | 12345.20] loss=0.89 avg=0.94\n",
            "[498681 | 12346.08] loss=0.92 avg=0.94\n",
            "[498682 | 12346.95] loss=1.00 avg=0.94\n",
            "[498683 | 12347.82] loss=0.83 avg=0.94\n",
            "[498684 | 12348.70] loss=0.73 avg=0.94\n",
            "[498685 | 12349.57] loss=1.04 avg=0.94\n",
            "[498686 | 12350.44] loss=0.74 avg=0.93\n",
            "[498687 | 12351.31] loss=0.79 avg=0.93\n",
            "[498688 | 12352.19] loss=0.84 avg=0.93\n",
            "[498689 | 12353.06] loss=1.08 avg=0.93\n",
            "[498690 | 12353.94] loss=0.84 avg=0.93\n",
            "[498691 | 12354.82] loss=0.77 avg=0.93\n",
            "[498692 | 12355.69] loss=1.05 avg=0.93\n",
            "[498693 | 12356.56] loss=1.01 avg=0.93\n",
            "[498694 | 12357.43] loss=1.02 avg=0.93\n",
            "[498695 | 12358.31] loss=0.72 avg=0.93\n",
            "[498696 | 12359.19] loss=0.84 avg=0.93\n",
            "[498697 | 12360.06] loss=0.92 avg=0.93\n",
            "[498698 | 12360.93] loss=0.85 avg=0.93\n",
            "[498699 | 12361.81] loss=0.83 avg=0.93\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " до згадки «ренаційного фінансунку». Його співвідношення збігається за прогнозуванням неінформам за прогнозами, з-поміж них кількість інфляціїв і санкцій по обох короткометними відкладах атомної електропривабливості і електропривабливості з метою ефекту виділення атомної електропривабливості, імовірно весь успішно захищати інфляцію ядерного синдрому зволожувачів населення. Також значною мірою положення співвідношення факторів силікатного математика, він збігся за , який містяться у післявоєнний час.\n",
            "Обставин, ґрунтується на значних типах \"n=2\", таких як на золотах співвідношення \"b=6\", стрімним з коливань заву межа з Гіфітом на південному фланзі Макавеллі, гори Південного Американського королівства. Так напівпровідні коронавірусні системи можуть використовуватися як ван здоровій до установленого \"ЕМ-ЕМ\". Застосовують 3,9-річний ґрунт на золотій атмосфері міста Кордоном на північно-схі\n",
            "\n",
            "[498700 | 12380.11] loss=0.87 avg=0.93\n",
            "[498701 | 12380.99] loss=0.73 avg=0.93\n",
            "[498702 | 12381.86] loss=0.87 avg=0.93\n",
            "[498703 | 12382.74] loss=0.97 avg=0.93\n",
            "[498704 | 12383.61] loss=1.01 avg=0.93\n",
            "[498705 | 12384.47] loss=0.92 avg=0.93\n",
            "[498706 | 12385.35] loss=0.91 avg=0.93\n",
            "[498707 | 12386.23] loss=1.10 avg=0.93\n",
            "[498708 | 12387.10] loss=0.89 avg=0.93\n",
            "[498709 | 12387.97] loss=0.89 avg=0.93\n",
            "[498710 | 12388.85] loss=0.85 avg=0.93\n",
            "[498711 | 12389.73] loss=1.03 avg=0.93\n",
            "[498712 | 12390.59] loss=0.98 avg=0.93\n",
            "[498713 | 12391.47] loss=1.09 avg=0.93\n",
            "[498714 | 12392.35] loss=0.97 avg=0.93\n",
            "[498715 | 12393.23] loss=0.99 avg=0.93\n",
            "[498716 | 12394.10] loss=0.90 avg=0.93\n",
            "[498717 | 12394.97] loss=0.86 avg=0.93\n",
            "[498718 | 12395.85] loss=1.00 avg=0.93\n",
            "[498719 | 12396.71] loss=0.96 avg=0.93\n",
            "[498720 | 12397.59] loss=0.91 avg=0.93\n",
            "[498721 | 12398.48] loss=1.00 avg=0.93\n",
            "[498722 | 12399.35] loss=0.82 avg=0.93\n",
            "[498723 | 12400.22] loss=1.02 avg=0.93\n",
            "[498724 | 12401.10] loss=0.86 avg=0.93\n",
            "[498725 | 12401.97] loss=0.86 avg=0.93\n",
            "[498726 | 12402.84] loss=0.99 avg=0.93\n",
            "[498727 | 12403.71] loss=1.07 avg=0.93\n",
            "[498728 | 12404.59] loss=0.84 avg=0.93\n",
            "[498729 | 12405.46] loss=0.94 avg=0.93\n",
            "[498730 | 12406.33] loss=0.84 avg=0.93\n",
            "[498731 | 12407.21] loss=0.81 avg=0.93\n",
            "[498732 | 12408.09] loss=0.81 avg=0.93\n",
            "[498733 | 12408.96] loss=0.97 avg=0.93\n",
            "[498734 | 12409.83] loss=0.84 avg=0.93\n",
            "[498735 | 12410.71] loss=1.01 avg=0.93\n",
            "[498736 | 12411.58] loss=0.93 avg=0.93\n",
            "[498737 | 12412.46] loss=0.69 avg=0.93\n",
            "[498738 | 12413.33] loss=1.25 avg=0.93\n",
            "[498739 | 12414.20] loss=1.12 avg=0.93\n",
            "[498740 | 12415.08] loss=0.98 avg=0.93\n",
            "[498741 | 12415.95] loss=0.89 avg=0.93\n",
            "[498742 | 12416.83] loss=1.00 avg=0.93\n",
            "[498743 | 12417.70] loss=0.99 avg=0.93\n",
            "[498744 | 12418.57] loss=1.05 avg=0.93\n",
            "[498745 | 12419.45] loss=1.04 avg=0.93\n",
            "[498746 | 12420.32] loss=0.80 avg=0.93\n",
            "[498747 | 12421.20] loss=1.08 avg=0.93\n",
            "[498748 | 12422.07] loss=1.05 avg=0.94\n",
            "[498749 | 12422.94] loss=1.05 avg=0.94\n",
            "[498750 | 12423.81] loss=1.13 avg=0.94\n",
            "[498751 | 12424.68] loss=1.06 avg=0.94\n",
            "[498752 | 12425.56] loss=0.94 avg=0.94\n",
            "[498753 | 12426.43] loss=0.91 avg=0.94\n",
            "[498754 | 12427.31] loss=0.84 avg=0.94\n",
            "[498755 | 12428.19] loss=1.05 avg=0.94\n",
            "[498756 | 12429.05] loss=0.78 avg=0.94\n",
            "[498757 | 12429.94] loss=0.99 avg=0.94\n",
            "[498758 | 12430.81] loss=0.79 avg=0.94\n",
            "[498759 | 12431.68] loss=0.93 avg=0.94\n",
            "[498760 | 12432.56] loss=1.15 avg=0.94\n",
            "[498761 | 12433.43] loss=0.99 avg=0.94\n",
            "[498762 | 12434.31] loss=1.08 avg=0.94\n",
            "[498763 | 12435.18] loss=0.97 avg=0.94\n",
            "[498764 | 12436.05] loss=0.75 avg=0.94\n",
            "[498765 | 12436.93] loss=0.76 avg=0.94\n",
            "[498766 | 12437.79] loss=0.98 avg=0.94\n",
            "[498767 | 12438.67] loss=0.77 avg=0.94\n",
            "[498768 | 12439.55] loss=1.01 avg=0.94\n",
            "[498769 | 12440.41] loss=1.05 avg=0.94\n",
            "[498770 | 12441.29] loss=1.12 avg=0.94\n",
            "[498771 | 12442.16] loss=0.84 avg=0.94\n",
            "[498772 | 12443.04] loss=1.02 avg=0.94\n",
            "[498773 | 12443.91] loss=0.95 avg=0.94\n",
            "[498774 | 12444.78] loss=0.92 avg=0.94\n",
            "[498775 | 12445.66] loss=0.78 avg=0.94\n",
            "[498776 | 12446.53] loss=0.87 avg=0.94\n",
            "[498777 | 12447.41] loss=1.01 avg=0.94\n",
            "[498778 | 12448.28] loss=0.84 avg=0.94\n",
            "[498779 | 12449.15] loss=0.76 avg=0.94\n",
            "[498780 | 12450.03] loss=1.01 avg=0.94\n",
            "[498781 | 12450.90] loss=0.85 avg=0.94\n",
            "[498782 | 12451.77] loss=0.89 avg=0.93\n",
            "[498783 | 12452.65] loss=0.86 avg=0.93\n",
            "[498784 | 12453.52] loss=0.88 avg=0.93\n",
            "[498785 | 12454.40] loss=0.95 avg=0.93\n",
            "[498786 | 12455.27] loss=0.67 avg=0.93\n",
            "[498787 | 12456.14] loss=0.77 avg=0.93\n",
            "[498788 | 12457.01] loss=1.11 avg=0.93\n",
            "[498789 | 12457.88] loss=0.83 avg=0.93\n",
            "[498790 | 12458.76] loss=1.21 avg=0.93\n",
            "[498791 | 12459.63] loss=0.75 avg=0.93\n",
            "[498792 | 12460.50] loss=0.97 avg=0.93\n",
            "[498793 | 12461.37] loss=0.80 avg=0.93\n",
            "[498794 | 12462.24] loss=0.92 avg=0.93\n",
            "[498795 | 12463.11] loss=0.84 avg=0.93\n",
            "[498796 | 12463.99] loss=0.91 avg=0.93\n",
            "[498797 | 12464.87] loss=0.85 avg=0.93\n",
            "[498798 | 12465.74] loss=1.02 avg=0.93\n",
            "[498799 | 12466.61] loss=1.03 avg=0.93\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " річок на початку лісів дає змогу трансформувати це тільки флігель-карлицький їжі, простір накладає більш як 11 поїзд до майбутнього. Тим часом державну ціль транспортних засобів зламано король Вейором Петресом і валу Дженкіним, із друзями під командуванням отамана Платтіса Тех, Траппаром Маркс застосував князівську китайську виборчу мініатюру, що командує Марксом в крайнім температурі тіла, а проходить лежало в Гюльму.\n",
            "24 листопада Турецький уже великий відсталості Татуїна почалася ще один практикою боротьби за свою багатогранну утиски, зокрема Карибською Спілкою. Тоді Татуїн почав відвідувати міжусобиць країни, але останній мав майже навіть позитивний король. Початковий іслам зазнав великого портретного втручання в навколишні сільськогосподарського підпілля — наземного шахтою Середземномор'я та Турецького третього боку, вирішивши повернутись до гітлерівського, — на той час це було поруч з ки\n",
            "\n",
            "[498800 | 12484.90] loss=0.91 avg=0.93\n",
            "[498801 | 12485.78] loss=1.05 avg=0.93\n",
            "[498802 | 12486.65] loss=0.85 avg=0.93\n",
            "[498803 | 12487.53] loss=0.97 avg=0.93\n",
            "[498804 | 12488.40] loss=0.66 avg=0.93\n",
            "[498805 | 12489.28] loss=0.85 avg=0.93\n",
            "[498806 | 12490.15] loss=1.00 avg=0.93\n",
            "[498807 | 12491.02] loss=0.99 avg=0.93\n",
            "[498808 | 12491.90] loss=0.88 avg=0.93\n",
            "[498809 | 12492.77] loss=0.87 avg=0.93\n",
            "[498810 | 12493.63] loss=0.92 avg=0.93\n",
            "[498811 | 12494.52] loss=0.95 avg=0.93\n",
            "[498812 | 12495.39] loss=0.98 avg=0.93\n",
            "[498813 | 12496.27] loss=0.89 avg=0.93\n",
            "[498814 | 12497.14] loss=1.04 avg=0.93\n",
            "[498815 | 12498.01] loss=0.87 avg=0.93\n",
            "[498816 | 12498.89] loss=0.59 avg=0.93\n",
            "[498817 | 12499.76] loss=0.77 avg=0.92\n",
            "[498818 | 12500.64] loss=0.79 avg=0.92\n",
            "[498819 | 12501.51] loss=0.81 avg=0.92\n",
            "[498820 | 12502.39] loss=0.94 avg=0.92\n",
            "[498821 | 12503.26] loss=0.77 avg=0.92\n",
            "[498822 | 12504.14] loss=0.82 avg=0.92\n",
            "[498823 | 12505.01] loss=0.91 avg=0.92\n",
            "[498824 | 12505.88] loss=0.82 avg=0.92\n",
            "[498825 | 12506.76] loss=1.10 avg=0.92\n",
            "[498826 | 12507.63] loss=0.76 avg=0.92\n",
            "[498827 | 12508.50] loss=1.14 avg=0.92\n",
            "[498828 | 12509.38] loss=0.82 avg=0.92\n",
            "[498829 | 12510.25] loss=1.09 avg=0.92\n",
            "[498830 | 12511.12] loss=0.92 avg=0.92\n",
            "[498831 | 12512.00] loss=0.94 avg=0.92\n",
            "[498832 | 12512.87] loss=1.06 avg=0.92\n",
            "[498833 | 12513.75] loss=1.04 avg=0.92\n",
            "[498834 | 12514.62] loss=0.85 avg=0.92\n",
            "[498835 | 12515.50] loss=0.87 avg=0.92\n",
            "[498836 | 12516.37] loss=0.91 avg=0.92\n",
            "[498837 | 12517.24] loss=0.85 avg=0.92\n",
            "[498838 | 12518.11] loss=0.87 avg=0.92\n",
            "[498839 | 12518.99] loss=1.06 avg=0.92\n",
            "[498840 | 12519.86] loss=1.02 avg=0.92\n",
            "[498841 | 12520.74] loss=0.90 avg=0.92\n",
            "[498842 | 12521.61] loss=0.95 avg=0.92\n",
            "[498843 | 12522.49] loss=0.83 avg=0.92\n",
            "[498844 | 12523.36] loss=0.92 avg=0.92\n",
            "[498845 | 12524.23] loss=0.89 avg=0.92\n",
            "[498846 | 12525.11] loss=0.97 avg=0.92\n",
            "[498847 | 12525.98] loss=1.07 avg=0.92\n",
            "[498848 | 12526.86] loss=0.98 avg=0.92\n",
            "[498849 | 12527.73] loss=0.96 avg=0.93\n",
            "[498850 | 12528.61] loss=0.98 avg=0.93\n",
            "[498851 | 12529.48] loss=0.86 avg=0.92\n",
            "[498852 | 12530.36] loss=0.81 avg=0.92\n",
            "[498853 | 12531.23] loss=0.85 avg=0.92\n",
            "[498854 | 12532.09] loss=0.91 avg=0.92\n",
            "[498855 | 12532.97] loss=0.86 avg=0.92\n",
            "[498856 | 12533.85] loss=0.98 avg=0.92\n",
            "[498857 | 12534.72] loss=1.03 avg=0.92\n",
            "[498858 | 12535.60] loss=1.01 avg=0.92\n",
            "[498859 | 12536.47] loss=0.87 avg=0.92\n",
            "[498860 | 12537.35] loss=1.00 avg=0.93\n",
            "[498861 | 12538.22] loss=0.97 avg=0.93\n",
            "[498862 | 12539.09] loss=0.86 avg=0.92\n",
            "[498863 | 12539.96] loss=0.87 avg=0.92\n",
            "[498864 | 12540.83] loss=0.81 avg=0.92\n",
            "[498865 | 12541.71] loss=1.24 avg=0.93\n",
            "[498866 | 12542.58] loss=0.81 avg=0.93\n",
            "[498867 | 12543.45] loss=1.16 avg=0.93\n",
            "[498868 | 12544.33] loss=0.96 avg=0.93\n",
            "[498869 | 12545.21] loss=0.77 avg=0.93\n",
            "[498870 | 12546.08] loss=1.33 avg=0.93\n",
            "[498871 | 12546.95] loss=1.12 avg=0.93\n",
            "[498872 | 12547.82] loss=0.96 avg=0.93\n",
            "[498873 | 12548.70] loss=0.89 avg=0.93\n",
            "[498874 | 12549.57] loss=0.87 avg=0.93\n",
            "[498875 | 12550.45] loss=0.78 avg=0.93\n",
            "[498876 | 12551.33] loss=1.22 avg=0.93\n",
            "[498877 | 12552.20] loss=0.99 avg=0.93\n",
            "[498878 | 12553.08] loss=1.01 avg=0.93\n",
            "[498879 | 12553.95] loss=0.93 avg=0.93\n",
            "[498880 | 12554.83] loss=1.02 avg=0.93\n",
            "[498881 | 12555.70] loss=0.77 avg=0.93\n",
            "[498882 | 12556.58] loss=1.00 avg=0.93\n",
            "[498883 | 12557.45] loss=0.91 avg=0.93\n",
            "[498884 | 12558.32] loss=0.96 avg=0.93\n",
            "[498885 | 12559.20] loss=0.82 avg=0.93\n",
            "[498886 | 12560.08] loss=1.18 avg=0.94\n",
            "[498887 | 12560.95] loss=0.87 avg=0.93\n",
            "[498888 | 12561.82] loss=1.03 avg=0.94\n",
            "[498889 | 12562.70] loss=0.64 avg=0.93\n",
            "[498890 | 12563.57] loss=0.92 avg=0.93\n",
            "[498891 | 12564.44] loss=1.20 avg=0.93\n",
            "[498892 | 12565.31] loss=1.03 avg=0.94\n",
            "[498893 | 12566.19] loss=0.93 avg=0.94\n",
            "[498894 | 12567.06] loss=0.89 avg=0.94\n",
            "[498895 | 12567.94] loss=0.57 avg=0.93\n",
            "[498896 | 12568.82] loss=0.74 avg=0.93\n",
            "[498897 | 12569.69] loss=0.86 avg=0.93\n",
            "[498898 | 12570.56] loss=1.06 avg=0.93\n",
            "[498899 | 12571.43] loss=0.90 avg=0.93\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "ивороди пізніше будуть дані. Митцю сказав, що під час відбиття братгаузевих дворян неможливі. В цей час довгий час («діяли споруди») все одно вистачало, до скептих виграли найбільш обережно поруч, який завдяки змінам панування в'язнів і вагомих знань. А в статті маломірики Едвард Кларк не поставив собі по собі віддавати своїм зраділям всупереч інтригуванню значно ширить людство. Однак не вдалося взяти участь у боротьбі з брати в Нюй-юй, що скасували насильно.\n",
            "У ревній галереї на півночі інтригування з Хатаба в ніч на 21 липня раніше, подано британською майбутньою президією Мастерс території, що займалася колективною ринковою смаковою інтригою, що призупинило дороги. Крім того, всі перспективи новоствореній наприкінці грудня Мастерс повсюдила, що її дізнається вроджене навернення. Для отримання місцевої військової траса (Тейша), можливий почуття всередині радіоактивної практики силья\n",
            "\n",
            "[498900 | 12589.75] loss=1.00 avg=0.93\n",
            "[498901 | 12590.63] loss=1.06 avg=0.93\n",
            "[498902 | 12591.50] loss=0.77 avg=0.93\n",
            "[498903 | 12592.37] loss=0.94 avg=0.93\n",
            "[498904 | 12593.25] loss=0.92 avg=0.93\n",
            "[498905 | 12594.12] loss=0.85 avg=0.93\n",
            "[498906 | 12595.00] loss=0.90 avg=0.93\n",
            "[498907 | 12595.87] loss=0.98 avg=0.93\n",
            "[498908 | 12596.75] loss=0.84 avg=0.93\n",
            "[498909 | 12597.62] loss=0.88 avg=0.93\n",
            "[498910 | 12598.49] loss=0.85 avg=0.93\n",
            "[498911 | 12599.37] loss=1.08 avg=0.93\n",
            "[498912 | 12600.24] loss=0.76 avg=0.93\n",
            "[498913 | 12601.11] loss=0.86 avg=0.93\n",
            "[498914 | 12601.99] loss=0.77 avg=0.93\n",
            "[498915 | 12602.87] loss=0.74 avg=0.92\n",
            "[498916 | 12603.74] loss=1.18 avg=0.93\n",
            "[498917 | 12604.61] loss=1.26 avg=0.93\n",
            "[498918 | 12605.49] loss=0.98 avg=0.93\n",
            "[498919 | 12606.36] loss=1.14 avg=0.93\n",
            "[498920 | 12607.23] loss=1.04 avg=0.93\n",
            "[498921 | 12608.10] loss=0.79 avg=0.93\n",
            "[498922 | 12608.98] loss=1.02 avg=0.93\n",
            "[498923 | 12609.85] loss=0.92 avg=0.93\n",
            "[498924 | 12610.73] loss=0.90 avg=0.93\n",
            "[498925 | 12611.60] loss=0.99 avg=0.93\n",
            "[498926 | 12612.48] loss=0.88 avg=0.93\n",
            "[498927 | 12613.35] loss=1.01 avg=0.93\n",
            "[498928 | 12614.22] loss=1.01 avg=0.93\n",
            "[498929 | 12615.09] loss=0.77 avg=0.93\n",
            "[498930 | 12615.96] loss=0.76 avg=0.93\n",
            "[498931 | 12616.84] loss=1.05 avg=0.93\n",
            "[498932 | 12617.71] loss=0.99 avg=0.93\n",
            "[498933 | 12618.59] loss=0.65 avg=0.93\n",
            "[498934 | 12619.46] loss=1.01 avg=0.93\n",
            "[498935 | 12620.34] loss=0.75 avg=0.93\n",
            "[498936 | 12621.21] loss=0.97 avg=0.93\n",
            "[498937 | 12622.08] loss=1.03 avg=0.93\n",
            "[498938 | 12622.96] loss=1.01 avg=0.93\n",
            "[498939 | 12623.83] loss=0.94 avg=0.93\n",
            "[498940 | 12624.70] loss=1.03 avg=0.93\n",
            "[498941 | 12625.58] loss=1.24 avg=0.93\n",
            "[498942 | 12626.45] loss=0.73 avg=0.93\n",
            "[498943 | 12627.31] loss=0.85 avg=0.93\n",
            "[498944 | 12628.19] loss=0.85 avg=0.93\n",
            "[498945 | 12629.07] loss=0.93 avg=0.93\n",
            "[498946 | 12629.95] loss=0.85 avg=0.93\n",
            "[498947 | 12630.82] loss=0.82 avg=0.93\n",
            "[498948 | 12631.69] loss=0.91 avg=0.93\n",
            "[498949 | 12632.56] loss=0.87 avg=0.93\n",
            "[498950 | 12633.43] loss=0.95 avg=0.93\n",
            "[498951 | 12634.30] loss=0.99 avg=0.93\n",
            "[498952 | 12635.18] loss=1.06 avg=0.93\n",
            "[498953 | 12636.05] loss=1.06 avg=0.93\n",
            "[498954 | 12636.93] loss=1.02 avg=0.93\n",
            "[498955 | 12637.80] loss=0.83 avg=0.93\n",
            "[498956 | 12638.68] loss=1.16 avg=0.93\n",
            "[498957 | 12639.56] loss=0.98 avg=0.93\n",
            "[498958 | 12640.43] loss=0.97 avg=0.93\n",
            "[498959 | 12641.31] loss=0.92 avg=0.93\n",
            "[498960 | 12642.17] loss=1.08 avg=0.94\n",
            "[498961 | 12643.05] loss=0.83 avg=0.93\n",
            "[498962 | 12643.92] loss=1.03 avg=0.94\n",
            "[498963 | 12644.79] loss=0.94 avg=0.94\n",
            "[498964 | 12645.67] loss=0.92 avg=0.94\n",
            "[498965 | 12646.54] loss=0.97 avg=0.94\n",
            "[498966 | 12647.42] loss=1.31 avg=0.94\n",
            "[498967 | 12648.29] loss=0.88 avg=0.94\n",
            "[498968 | 12649.16] loss=1.03 avg=0.94\n",
            "[498969 | 12650.04] loss=1.01 avg=0.94\n",
            "[498970 | 12650.91] loss=0.91 avg=0.94\n",
            "[498971 | 12651.78] loss=1.06 avg=0.94\n",
            "[498972 | 12652.65] loss=1.65 avg=0.95\n",
            "[498973 | 12653.52] loss=1.19 avg=0.95\n",
            "[498974 | 12654.40] loss=0.99 avg=0.95\n",
            "[498975 | 12655.27] loss=0.92 avg=0.95\n",
            "[498976 | 12656.14] loss=0.98 avg=0.95\n",
            "[498977 | 12657.02] loss=0.94 avg=0.95\n",
            "[498978 | 12657.89] loss=0.98 avg=0.95\n",
            "[498979 | 12658.77] loss=0.78 avg=0.95\n",
            "[498980 | 12659.64] loss=0.93 avg=0.95\n",
            "[498981 | 12660.52] loss=0.91 avg=0.95\n",
            "[498982 | 12661.39] loss=0.84 avg=0.95\n",
            "[498983 | 12662.25] loss=0.94 avg=0.95\n",
            "[498984 | 12663.13] loss=0.89 avg=0.95\n",
            "[498985 | 12664.00] loss=1.13 avg=0.95\n",
            "[498986 | 12664.87] loss=1.04 avg=0.95\n",
            "[498987 | 12665.75] loss=0.86 avg=0.95\n",
            "[498988 | 12666.63] loss=0.88 avg=0.95\n",
            "[498989 | 12667.49] loss=0.90 avg=0.95\n",
            "[498990 | 12668.37] loss=0.84 avg=0.95\n",
            "[498991 | 12669.25] loss=0.82 avg=0.95\n",
            "[498992 | 12670.12] loss=0.81 avg=0.94\n",
            "[498993 | 12670.99] loss=1.12 avg=0.95\n",
            "[498994 | 12671.87] loss=0.79 avg=0.94\n",
            "[498995 | 12672.74] loss=0.79 avg=0.94\n",
            "[498996 | 12673.62] loss=0.87 avg=0.94\n",
            "[498997 | 12674.50] loss=1.03 avg=0.94\n",
            "[498998 | 12675.37] loss=1.00 avg=0.94\n",
            "[498999 | 12676.23] loss=0.80 avg=0.94\n",
            "Saving checkpoint/run1/model-499000\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "�у намір використовувати для всіх цих пісень. В даний час спочатку залишалася винахідна діяльність та продовжувала розвивати зовнішню творчість іменем «Міського Прайм», вивантаженому близьковими самосвідомості. Його вже було неподільним: «Хрещатий Хрест доюлекторний», «ЧужКОВІК», у тому числі з таким розмаїттям — «Льоджераг», «Ковіс пираний» і «Пласт».\n",
            "Двозначною вагітністю є зображення «Просвіти». Шквіт зовнішніх технологій називають життєдіяльністю: «Страсбург міста Вільнощич», «Міського дня», «Евергокрани», «Чужі», «Каваш» та «Центру». «Просвіта», «Чужі» та «Сучасний» є одним з текстів програми «Баз». Деякі конкрети, що припиняють подібні діяльності, поряд або називають «Любель».\n",
            "Ім'я Зовнішніх Зовницьких імен сотері на всіх фактах вивчення просвітницьких фактів, такі як великі технології, служилі особистості та шкіл самосвідомості місцевого творіння. Їх вона створю�\n",
            "\n",
            "[499000 | 12698.90] loss=0.90 avg=0.94\n",
            "[499001 | 12699.77] loss=0.99 avg=0.94\n",
            "[499002 | 12700.65] loss=0.81 avg=0.94\n",
            "[499003 | 12701.52] loss=0.99 avg=0.94\n",
            "[499004 | 12702.40] loss=0.91 avg=0.94\n",
            "[499005 | 12703.27] loss=0.94 avg=0.94\n",
            "[499006 | 12704.14] loss=1.10 avg=0.94\n",
            "[499007 | 12705.02] loss=1.13 avg=0.94\n",
            "[499008 | 12705.89] loss=1.09 avg=0.95\n",
            "[499009 | 12706.77] loss=0.81 avg=0.94\n",
            "[499010 | 12707.64] loss=0.86 avg=0.94\n",
            "[499011 | 12708.52] loss=0.78 avg=0.94\n",
            "[499012 | 12709.40] loss=0.19 avg=0.93\n",
            "[499013 | 12710.27] loss=0.88 avg=0.93\n",
            "[499014 | 12711.15] loss=0.84 avg=0.93\n",
            "[499015 | 12712.02] loss=0.91 avg=0.93\n",
            "[499016 | 12712.90] loss=0.94 avg=0.93\n",
            "[499017 | 12713.77] loss=1.02 avg=0.93\n",
            "[499018 | 12714.64] loss=0.81 avg=0.93\n",
            "[499019 | 12715.52] loss=0.77 avg=0.93\n",
            "[499020 | 12716.39] loss=1.11 avg=0.93\n",
            "[499021 | 12717.26] loss=1.16 avg=0.94\n",
            "[499022 | 12718.13] loss=1.08 avg=0.94\n",
            "[499023 | 12719.00] loss=0.91 avg=0.94\n",
            "[499024 | 12719.88] loss=0.87 avg=0.94\n",
            "[499025 | 12720.74] loss=0.84 avg=0.93\n",
            "[499026 | 12721.61] loss=1.08 avg=0.94\n",
            "[499027 | 12722.49] loss=0.91 avg=0.94\n",
            "[499028 | 12723.37] loss=0.76 avg=0.93\n",
            "[499029 | 12724.25] loss=0.86 avg=0.93\n",
            "[499030 | 12725.12] loss=1.12 avg=0.94\n",
            "[499031 | 12725.99] loss=1.11 avg=0.94\n",
            "[499032 | 12726.87] loss=1.05 avg=0.94\n",
            "[499033 | 12727.74] loss=0.84 avg=0.94\n",
            "[499034 | 12728.61] loss=0.92 avg=0.94\n",
            "[499035 | 12729.49] loss=1.15 avg=0.94\n",
            "[499036 | 12730.36] loss=0.82 avg=0.94\n",
            "[499037 | 12731.23] loss=0.80 avg=0.94\n",
            "[499038 | 12732.10] loss=0.85 avg=0.94\n",
            "[499039 | 12732.97] loss=1.01 avg=0.94\n",
            "[499040 | 12733.85] loss=0.92 avg=0.94\n",
            "[499041 | 12734.73] loss=1.01 avg=0.94\n",
            "[499042 | 12735.60] loss=0.87 avg=0.94\n",
            "[499043 | 12736.47] loss=0.88 avg=0.94\n",
            "[499044 | 12737.35] loss=0.66 avg=0.93\n",
            "[499045 | 12738.22] loss=0.78 avg=0.93\n",
            "[499046 | 12739.09] loss=0.84 avg=0.93\n",
            "[499047 | 12739.97] loss=1.14 avg=0.93\n",
            "[499048 | 12740.84] loss=0.87 avg=0.93\n",
            "[499049 | 12741.72] loss=1.03 avg=0.93\n",
            "[499050 | 12742.60] loss=0.82 avg=0.93\n",
            "[499051 | 12743.47] loss=0.93 avg=0.93\n",
            "[499052 | 12744.34] loss=1.10 avg=0.93\n",
            "[499053 | 12745.21] loss=0.85 avg=0.93\n",
            "[499054 | 12746.09] loss=0.67 avg=0.93\n",
            "[499055 | 12746.96] loss=1.03 avg=0.93\n",
            "[499056 | 12747.83] loss=0.93 avg=0.93\n",
            "[499057 | 12748.72] loss=1.25 avg=0.93\n",
            "[499058 | 12749.59] loss=0.90 avg=0.93\n",
            "[499059 | 12750.47] loss=1.03 avg=0.93\n",
            "[499060 | 12751.34] loss=0.80 avg=0.93\n",
            "[499061 | 12752.21] loss=1.05 avg=0.93\n",
            "[499062 | 12753.08] loss=0.97 avg=0.94\n",
            "[499063 | 12753.95] loss=0.62 avg=0.93\n",
            "[499064 | 12754.83] loss=0.99 avg=0.93\n",
            "[499065 | 12755.70] loss=1.12 avg=0.93\n",
            "[499066 | 12756.57] loss=1.07 avg=0.94\n",
            "[499067 | 12757.46] loss=0.78 avg=0.93\n",
            "[499068 | 12758.33] loss=1.02 avg=0.94\n",
            "[499069 | 12759.21] loss=0.65 avg=0.93\n",
            "[499070 | 12760.07] loss=1.03 avg=0.93\n",
            "[499071 | 12760.95] loss=1.15 avg=0.94\n",
            "[499072 | 12761.82] loss=0.96 avg=0.94\n",
            "[499073 | 12762.69] loss=0.76 avg=0.93\n",
            "[499074 | 12763.56] loss=0.91 avg=0.93\n",
            "[499075 | 12764.44] loss=0.87 avg=0.93\n",
            "[499076 | 12765.31] loss=0.99 avg=0.93\n",
            "[499077 | 12766.19] loss=0.97 avg=0.93\n",
            "[499078 | 12767.06] loss=0.94 avg=0.93\n",
            "[499079 | 12767.94] loss=0.94 avg=0.93\n",
            "[499080 | 12768.81] loss=1.00 avg=0.93\n",
            "[499081 | 12769.69] loss=0.58 avg=0.93\n",
            "[499082 | 12770.56] loss=0.74 avg=0.93\n",
            "[499083 | 12771.43] loss=0.78 avg=0.93\n",
            "[499084 | 12772.32] loss=0.94 avg=0.93\n",
            "[499085 | 12773.19] loss=0.86 avg=0.93\n",
            "[499086 | 12774.07] loss=1.04 avg=0.93\n",
            "[499087 | 12774.94] loss=0.92 avg=0.93\n",
            "[499088 | 12775.82] loss=0.91 avg=0.93\n",
            "[499089 | 12776.69] loss=0.93 avg=0.93\n",
            "[499090 | 12777.56] loss=1.13 avg=0.93\n",
            "[499091 | 12778.44] loss=0.85 avg=0.93\n",
            "[499092 | 12779.31] loss=0.77 avg=0.93\n",
            "[499093 | 12780.18] loss=0.82 avg=0.93\n",
            "[499094 | 12781.06] loss=0.88 avg=0.93\n",
            "[499095 | 12781.94] loss=0.90 avg=0.93\n",
            "[499096 | 12782.82] loss=0.84 avg=0.93\n",
            "[499097 | 12783.68] loss=0.97 avg=0.93\n",
            "[499098 | 12784.56] loss=1.01 avg=0.93\n",
            "[499099 | 12785.43] loss=0.78 avg=0.93\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "ьники конференціями, як зображення на різних націях — пов'язаних наприкінці 1950-х років.\n",
            "Діяльність жінок дуже вибудована вже так, коли зможе пояснити, чому вона контролює заяву, що може. Ймовірно, у Інституті Іджі утвердилася число соціальної консерватизмової юрисдикції, як людство, конференції, декомунікації і її інформативної діяльності, воно створило своє нове духовне керівництво. Оскільки вся давньоруська діяльність була організована захисним духовним рейтингом наркотиків. Відважна династія. Оскільки жінки його відрізнялись від забороненого найвпливовішими правилами. Історик Бартолус цілком ділився не тільки тому, що кожен рішення дуже високого рівня сексуального як душевний стан централізації. Як було одним з організаторів, дуже легко модери проявляють їх на громадську, а рівність громадян що відбуваються в Росії, де любить її боротьба, як і результат дипломатії. У найвплив\n",
            "\n",
            "[499100 | 12803.85] loss=1.00 avg=0.93\n",
            "[499101 | 12804.72] loss=1.05 avg=0.93\n",
            "[499102 | 12805.60] loss=0.75 avg=0.93\n",
            "[499103 | 12806.47] loss=1.12 avg=0.93\n",
            "[499104 | 12807.35] loss=0.77 avg=0.93\n",
            "[499105 | 12808.22] loss=0.80 avg=0.92\n",
            "[499106 | 12809.09] loss=0.96 avg=0.92\n",
            "[499107 | 12809.97] loss=0.93 avg=0.92\n",
            "[499108 | 12810.84] loss=0.98 avg=0.93\n",
            "[499109 | 12811.72] loss=0.72 avg=0.92\n",
            "[499110 | 12812.59] loss=1.02 avg=0.92\n",
            "[499111 | 12813.47] loss=1.24 avg=0.93\n",
            "[499112 | 12814.34] loss=1.02 avg=0.93\n",
            "[499113 | 12815.21] loss=1.13 avg=0.93\n",
            "[499114 | 12816.09] loss=0.92 avg=0.93\n",
            "[499115 | 12816.96] loss=0.76 avg=0.93\n",
            "[499116 | 12817.83] loss=0.87 avg=0.93\n",
            "[499117 | 12818.71] loss=0.85 avg=0.93\n",
            "[499118 | 12819.58] loss=0.86 avg=0.93\n",
            "[499119 | 12820.46] loss=1.06 avg=0.93\n",
            "[499120 | 12821.33] loss=0.92 avg=0.93\n",
            "[499121 | 12822.21] loss=0.78 avg=0.93\n",
            "[499122 | 12823.09] loss=1.02 avg=0.93\n",
            "[499123 | 12823.96] loss=0.76 avg=0.93\n",
            "[499124 | 12824.84] loss=1.04 avg=0.93\n",
            "[499125 | 12825.72] loss=0.82 avg=0.93\n",
            "[499126 | 12826.60] loss=0.98 avg=0.93\n",
            "[499127 | 12827.47] loss=1.15 avg=0.93\n",
            "[499128 | 12828.35] loss=1.19 avg=0.93\n",
            "[499129 | 12829.22] loss=0.91 avg=0.93\n",
            "[499130 | 12830.09] loss=0.92 avg=0.93\n",
            "[499131 | 12830.97] loss=0.81 avg=0.93\n",
            "[499132 | 12831.84] loss=0.87 avg=0.93\n",
            "[499133 | 12832.72] loss=1.09 avg=0.93\n",
            "[499134 | 12833.59] loss=0.95 avg=0.93\n",
            "[499135 | 12834.47] loss=0.84 avg=0.93\n",
            "[499136 | 12835.35] loss=0.87 avg=0.93\n",
            "[499137 | 12836.21] loss=1.00 avg=0.93\n",
            "[499138 | 12837.09] loss=1.13 avg=0.93\n",
            "[499139 | 12837.97] loss=0.89 avg=0.93\n",
            "[499140 | 12838.85] loss=1.05 avg=0.93\n",
            "[499141 | 12839.72] loss=0.84 avg=0.93\n",
            "[499142 | 12840.60] loss=0.97 avg=0.93\n",
            "[499143 | 12841.47] loss=0.77 avg=0.93\n",
            "[499144 | 12842.34] loss=0.84 avg=0.93\n",
            "[499145 | 12843.22] loss=0.96 avg=0.93\n",
            "[499146 | 12844.09] loss=1.04 avg=0.93\n",
            "[499147 | 12844.97] loss=1.05 avg=0.93\n",
            "[499148 | 12845.84] loss=0.91 avg=0.93\n",
            "[499149 | 12846.73] loss=0.90 avg=0.93\n",
            "[499150 | 12847.60] loss=0.84 avg=0.93\n",
            "[499151 | 12848.47] loss=1.08 avg=0.93\n",
            "[499152 | 12849.35] loss=1.05 avg=0.93\n",
            "[499153 | 12850.23] loss=1.13 avg=0.94\n",
            "[499154 | 12851.10] loss=0.66 avg=0.93\n",
            "[499155 | 12851.97] loss=1.04 avg=0.93\n",
            "[499156 | 12852.85] loss=0.81 avg=0.93\n",
            "[499157 | 12853.72] loss=0.85 avg=0.93\n",
            "[499158 | 12854.59] loss=1.01 avg=0.93\n",
            "[499159 | 12855.46] loss=0.95 avg=0.93\n",
            "[499160 | 12856.34] loss=1.09 avg=0.93\n",
            "[499161 | 12857.21] loss=0.86 avg=0.93\n",
            "[499162 | 12858.09] loss=0.78 avg=0.93\n",
            "[499163 | 12858.96] loss=1.03 avg=0.93\n",
            "[499164 | 12859.84] loss=1.57 avg=0.94\n",
            "[499165 | 12860.71] loss=0.94 avg=0.94\n",
            "[499166 | 12861.59] loss=0.86 avg=0.94\n",
            "[499167 | 12862.47] loss=0.93 avg=0.94\n",
            "[499168 | 12863.34] loss=1.05 avg=0.94\n",
            "[499169 | 12864.21] loss=0.98 avg=0.94\n",
            "[499170 | 12865.09] loss=0.87 avg=0.94\n",
            "[499171 | 12865.97] loss=0.74 avg=0.94\n",
            "[499172 | 12866.84] loss=1.14 avg=0.94\n",
            "[499173 | 12867.72] loss=1.20 avg=0.94\n",
            "[499174 | 12868.59] loss=1.09 avg=0.94\n",
            "[499175 | 12869.46] loss=0.98 avg=0.94\n",
            "[499176 | 12870.34] loss=0.80 avg=0.94\n",
            "[499177 | 12871.21] loss=0.85 avg=0.94\n",
            "[499178 | 12872.09] loss=0.89 avg=0.94\n",
            "[499179 | 12872.97] loss=0.90 avg=0.94\n",
            "[499180 | 12873.84] loss=0.87 avg=0.94\n",
            "[499181 | 12874.71] loss=1.04 avg=0.94\n",
            "[499182 | 12875.58] loss=0.89 avg=0.94\n",
            "[499183 | 12876.46] loss=1.12 avg=0.94\n",
            "[499184 | 12877.33] loss=0.94 avg=0.94\n",
            "[499185 | 12878.20] loss=1.02 avg=0.94\n",
            "[499186 | 12879.07] loss=0.84 avg=0.94\n",
            "[499187 | 12879.95] loss=0.80 avg=0.94\n",
            "[499188 | 12880.82] loss=0.96 avg=0.94\n",
            "[499189 | 12881.70] loss=0.97 avg=0.94\n",
            "[499190 | 12882.57] loss=0.95 avg=0.94\n",
            "[499191 | 12883.44] loss=0.81 avg=0.94\n",
            "[499192 | 12884.31] loss=0.78 avg=0.94\n",
            "[499193 | 12885.19] loss=1.09 avg=0.94\n",
            "[499194 | 12886.06] loss=1.04 avg=0.94\n",
            "[499195 | 12886.93] loss=0.88 avg=0.94\n",
            "[499196 | 12887.80] loss=0.93 avg=0.94\n",
            "[499197 | 12888.67] loss=0.91 avg=0.94\n",
            "[499198 | 12889.55] loss=0.90 avg=0.94\n",
            "[499199 | 12890.43] loss=0.94 avg=0.94\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            ". Хліб опустельний знаходився впритул до себе у велику дерев'яну точку зору планету. Загиблими складами є скульптура «одуя», що зустрічаються на її межах, а високомолекулярні елементи кислоточного живлення не приводить до зарості з дублюючого. Хліб живуть до трафарету, що оточують до нього фруктових чи вепалуванні кислот та вираз на невизначений час. Цей стан дозволятий знаходити посухи вище, ніж за декілька кілограмів. Каракллан зводять до їх запеклих номер добре укритті, приділених для еліпсою на молекулярні густини, замість фар планети затоплюють скульптури, поставлені двері, вологі листівки предметів. Цей стрес показав, що характер знання мертва віддають ефір, помилко вважаючи \"розсіяними\" . Це атоми розсіяного \"всі живлення\" мертві можуть утримуватися від мармуру розсіяних горіхів платежів.\n",
            "Багато світлівних відмінностей, однорядних із світлових призи кислотою визнають деяким релігійним, на а\n",
            "\n",
            "[499200 | 12908.83] loss=1.00 avg=0.94\n",
            "[499201 | 12909.70] loss=0.81 avg=0.94\n",
            "[499202 | 12910.57] loss=0.85 avg=0.94\n",
            "[499203 | 12911.44] loss=0.83 avg=0.94\n",
            "[499204 | 12912.32] loss=1.18 avg=0.94\n",
            "[499205 | 12913.19] loss=1.25 avg=0.94\n",
            "[499206 | 12914.07] loss=0.93 avg=0.94\n",
            "[499207 | 12914.94] loss=1.02 avg=0.94\n",
            "[499208 | 12915.83] loss=0.87 avg=0.94\n",
            "[499209 | 12916.71] loss=1.12 avg=0.94\n",
            "[499210 | 12917.58] loss=1.06 avg=0.95\n",
            "[499211 | 12918.45] loss=0.73 avg=0.94\n",
            "[499212 | 12919.33] loss=1.23 avg=0.95\n",
            "[499213 | 12920.21] loss=1.17 avg=0.95\n",
            "[499214 | 12921.08] loss=0.75 avg=0.95\n",
            "[499215 | 12921.95] loss=0.75 avg=0.94\n",
            "[499216 | 12922.82] loss=0.87 avg=0.94\n",
            "[499217 | 12923.70] loss=1.09 avg=0.94\n",
            "[499218 | 12924.57] loss=0.95 avg=0.94\n",
            "[499219 | 12925.45] loss=0.78 avg=0.94\n",
            "[499220 | 12926.33] loss=0.91 avg=0.94\n",
            "[499221 | 12927.20] loss=0.84 avg=0.94\n",
            "[499222 | 12928.07] loss=1.01 avg=0.94\n",
            "[499223 | 12928.95] loss=1.30 avg=0.95\n",
            "[499224 | 12929.82] loss=0.86 avg=0.95\n",
            "[499225 | 12930.70] loss=0.77 avg=0.94\n",
            "[499226 | 12931.57] loss=0.98 avg=0.94\n",
            "[499227 | 12932.45] loss=0.97 avg=0.94\n",
            "[499228 | 12933.32] loss=1.15 avg=0.95\n",
            "[499229 | 12934.19] loss=0.81 avg=0.94\n",
            "[499230 | 12935.07] loss=0.85 avg=0.94\n",
            "[499231 | 12935.93] loss=0.75 avg=0.94\n",
            "[499232 | 12936.81] loss=0.97 avg=0.94\n",
            "[499233 | 12937.68] loss=1.21 avg=0.94\n",
            "[499234 | 12938.55] loss=0.75 avg=0.94\n",
            "[499235 | 12939.43] loss=0.79 avg=0.94\n",
            "[499236 | 12940.30] loss=1.10 avg=0.94\n",
            "[499237 | 12941.17] loss=0.82 avg=0.94\n",
            "[499238 | 12942.05] loss=0.97 avg=0.94\n",
            "[499239 | 12942.92] loss=0.93 avg=0.94\n",
            "[499240 | 12943.80] loss=1.32 avg=0.95\n",
            "[499241 | 12944.67] loss=0.97 avg=0.95\n",
            "[499242 | 12945.54] loss=1.19 avg=0.95\n",
            "[499243 | 12946.41] loss=0.83 avg=0.95\n",
            "[499244 | 12947.28] loss=1.08 avg=0.95\n",
            "[499245 | 12948.16] loss=0.90 avg=0.95\n",
            "[499246 | 12949.03] loss=0.76 avg=0.95\n",
            "[499247 | 12949.90] loss=0.85 avg=0.95\n",
            "[499248 | 12950.78] loss=0.75 avg=0.94\n",
            "[499249 | 12951.66] loss=0.84 avg=0.94\n",
            "[499250 | 12952.54] loss=0.85 avg=0.94\n",
            "[499251 | 12953.40] loss=1.01 avg=0.94\n",
            "[499252 | 12954.28] loss=0.90 avg=0.94\n",
            "[499253 | 12955.15] loss=0.96 avg=0.94\n",
            "[499254 | 12956.02] loss=0.96 avg=0.94\n",
            "[499255 | 12956.90] loss=0.73 avg=0.94\n",
            "[499256 | 12957.77] loss=0.96 avg=0.94\n",
            "[499257 | 12958.65] loss=0.88 avg=0.94\n",
            "[499258 | 12959.53] loss=0.99 avg=0.94\n",
            "[499259 | 12960.41] loss=0.90 avg=0.94\n",
            "[499260 | 12961.29] loss=1.06 avg=0.94\n",
            "[499261 | 12962.16] loss=1.01 avg=0.94\n",
            "[499262 | 12963.03] loss=0.82 avg=0.94\n",
            "[499263 | 12963.90] loss=1.04 avg=0.94\n",
            "[499264 | 12964.78] loss=0.91 avg=0.94\n",
            "[499265 | 12965.65] loss=0.73 avg=0.94\n",
            "[499266 | 12966.53] loss=0.88 avg=0.94\n",
            "[499267 | 12967.41] loss=0.99 avg=0.94\n",
            "[499268 | 12968.27] loss=0.98 avg=0.94\n",
            "[499269 | 12969.15] loss=0.87 avg=0.94\n",
            "[499270 | 12970.02] loss=1.02 avg=0.94\n",
            "[499271 | 12970.89] loss=0.99 avg=0.94\n",
            "[499272 | 12971.78] loss=1.00 avg=0.94\n",
            "[499273 | 12972.65] loss=0.74 avg=0.94\n",
            "[499274 | 12973.53] loss=1.17 avg=0.94\n",
            "[499275 | 12974.40] loss=0.92 avg=0.94\n",
            "[499276 | 12975.27] loss=1.17 avg=0.94\n",
            "[499277 | 12976.15] loss=1.03 avg=0.94\n",
            "[499278 | 12977.01] loss=1.02 avg=0.94\n",
            "[499279 | 12977.89] loss=1.02 avg=0.94\n",
            "[499280 | 12978.76] loss=0.86 avg=0.94\n",
            "[499281 | 12979.64] loss=0.85 avg=0.94\n",
            "[499282 | 12980.51] loss=1.10 avg=0.94\n",
            "[499283 | 12981.39] loss=0.95 avg=0.94\n",
            "[499284 | 12982.27] loss=1.14 avg=0.95\n",
            "[499285 | 12983.14] loss=0.75 avg=0.94\n",
            "[499286 | 12984.02] loss=0.78 avg=0.94\n",
            "[499287 | 12984.89] loss=0.66 avg=0.94\n",
            "[499288 | 12985.76] loss=0.84 avg=0.94\n",
            "[499289 | 12986.63] loss=1.23 avg=0.94\n",
            "[499290 | 12987.50] loss=0.79 avg=0.94\n",
            "[499291 | 12988.38] loss=0.83 avg=0.94\n",
            "[499292 | 12989.25] loss=0.97 avg=0.94\n",
            "[499293 | 12990.13] loss=0.92 avg=0.94\n",
            "[499294 | 12991.01] loss=1.05 avg=0.94\n",
            "[499295 | 12991.87] loss=0.79 avg=0.94\n",
            "[499296 | 12992.75] loss=0.93 avg=0.94\n",
            "[499297 | 12993.62] loss=0.99 avg=0.94\n",
            "[499298 | 12994.49] loss=0.83 avg=0.94\n",
            "[499299 | 12995.36] loss=0.93 avg=0.94\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "� що всі вони й знаходять по всій країні своєю людиною і пішли по Італії, Світовий Схід, Африку, Алжиру і Фесенат.\n",
            "6 червня самогубство закінчилось хворим штурмом і показом війні Африки. Перевагу жертвам щодня війни вирушили в Африку, Алжир, Ебен-Туонів і Лонобода. На їх території стало відомо, що же в Африці зовсім інший. Переважно провини до початку зони війни аж до тих пір, аби раніше із них поверталися на висот. Доки третім чоловікам виявилися подалі, в більшості випадків хвороби час вдалось демонструвати. Під час показу Фесенат рухався до того ж розвитку, обмежившись, на відміну від неживих тварин до Африки, або навіть якийсь їхні висот. Нагороди аби досягли великих вігнань, хоч і не були завойовані війною Тухаці і в інших країнах. Хоча абсолютна кількість території і хотіли видавати за візуальним магнітне поглинання. Цей магнітний поглинання може свідчити про ш\n",
            "\n",
            "[499300 | 13013.60] loss=1.07 avg=0.94\n",
            "[499301 | 13014.47] loss=0.97 avg=0.94\n",
            "[499302 | 13015.35] loss=0.86 avg=0.94\n",
            "[499303 | 13016.23] loss=0.88 avg=0.94\n",
            "[499304 | 13017.11] loss=0.86 avg=0.94\n",
            "[499305 | 13017.98] loss=1.13 avg=0.94\n",
            "[499306 | 13018.86] loss=1.01 avg=0.94\n",
            "[499307 | 13019.73] loss=0.99 avg=0.94\n",
            "[499308 | 13020.59] loss=1.11 avg=0.94\n",
            "[499309 | 13021.47] loss=0.98 avg=0.94\n",
            "[499310 | 13022.35] loss=0.17 avg=0.94\n",
            "[499311 | 13023.22] loss=0.93 avg=0.94\n",
            "[499312 | 13024.09] loss=0.47 avg=0.93\n",
            "[499313 | 13024.96] loss=0.87 avg=0.93\n",
            "[499314 | 13025.84] loss=1.13 avg=0.93\n",
            "[499315 | 13026.71] loss=0.93 avg=0.93\n",
            "[499316 | 13027.58] loss=0.97 avg=0.93\n",
            "[499317 | 13028.46] loss=0.80 avg=0.93\n",
            "[499318 | 13029.34] loss=1.15 avg=0.93\n",
            "[499319 | 13030.21] loss=1.08 avg=0.93\n",
            "[499320 | 13031.09] loss=0.92 avg=0.93\n",
            "[499321 | 13031.96] loss=0.89 avg=0.93\n",
            "[499322 | 13032.84] loss=0.82 avg=0.93\n",
            "[499323 | 13033.71] loss=1.00 avg=0.93\n",
            "[499324 | 13034.59] loss=0.85 avg=0.93\n",
            "[499325 | 13035.45] loss=1.07 avg=0.93\n",
            "[499326 | 13036.33] loss=0.77 avg=0.93\n",
            "[499327 | 13037.21] loss=1.00 avg=0.93\n",
            "[499328 | 13038.07] loss=1.06 avg=0.93\n",
            "[499329 | 13038.95] loss=0.90 avg=0.93\n",
            "[499330 | 13039.82] loss=0.98 avg=0.93\n",
            "[499331 | 13040.69] loss=0.79 avg=0.93\n",
            "[499332 | 13041.56] loss=0.99 avg=0.93\n",
            "[499333 | 13042.44] loss=0.87 avg=0.93\n",
            "[499334 | 13043.31] loss=0.91 avg=0.93\n",
            "[499335 | 13044.19] loss=0.85 avg=0.93\n",
            "[499336 | 13045.06] loss=0.89 avg=0.93\n",
            "[499337 | 13045.94] loss=1.07 avg=0.93\n",
            "[499338 | 13046.81] loss=1.06 avg=0.93\n",
            "[499339 | 13047.68] loss=0.93 avg=0.93\n",
            "[499340 | 13048.56] loss=0.93 avg=0.93\n",
            "[499341 | 13049.42] loss=0.79 avg=0.93\n",
            "[499342 | 13050.31] loss=1.10 avg=0.93\n",
            "[499343 | 13051.18] loss=0.86 avg=0.93\n",
            "[499344 | 13052.06] loss=0.89 avg=0.93\n",
            "[499345 | 13052.93] loss=1.03 avg=0.93\n",
            "[499346 | 13053.80] loss=0.80 avg=0.93\n",
            "[499347 | 13054.68] loss=0.78 avg=0.93\n",
            "[499348 | 13055.54] loss=0.98 avg=0.93\n",
            "[499349 | 13056.42] loss=0.95 avg=0.93\n",
            "[499350 | 13057.29] loss=1.00 avg=0.93\n",
            "[499351 | 13058.16] loss=0.95 avg=0.93\n",
            "[499352 | 13059.04] loss=0.97 avg=0.93\n",
            "[499353 | 13059.91] loss=0.89 avg=0.93\n",
            "[499354 | 13060.78] loss=1.01 avg=0.93\n",
            "[499355 | 13061.66] loss=0.89 avg=0.93\n",
            "[499356 | 13062.53] loss=0.73 avg=0.93\n",
            "[499357 | 13063.41] loss=0.95 avg=0.93\n",
            "[499358 | 13064.28] loss=0.96 avg=0.93\n",
            "[499359 | 13065.16] loss=0.74 avg=0.93\n",
            "[499360 | 13066.03] loss=0.94 avg=0.93\n",
            "[499361 | 13066.90] loss=0.84 avg=0.93\n",
            "[499362 | 13067.78] loss=1.00 avg=0.93\n",
            "[499363 | 13068.65] loss=0.76 avg=0.93\n",
            "[499364 | 13069.53] loss=0.87 avg=0.93\n",
            "[499365 | 13070.40] loss=1.03 avg=0.93\n",
            "[499366 | 13071.27] loss=1.09 avg=0.93\n",
            "[499367 | 13072.15] loss=0.97 avg=0.93\n",
            "[499368 | 13073.02] loss=1.08 avg=0.93\n",
            "[499369 | 13073.90] loss=0.75 avg=0.93\n",
            "[499370 | 13074.77] loss=0.39 avg=0.92\n",
            "[499371 | 13075.64] loss=0.82 avg=0.92\n",
            "[499372 | 13076.51] loss=0.94 avg=0.92\n",
            "[499373 | 13077.39] loss=1.06 avg=0.92\n",
            "[499374 | 13078.26] loss=1.02 avg=0.93\n",
            "[499375 | 13079.14] loss=0.83 avg=0.93\n",
            "[499376 | 13080.01] loss=0.89 avg=0.92\n",
            "[499377 | 13080.89] loss=0.90 avg=0.92\n",
            "[499378 | 13081.76] loss=0.94 avg=0.92\n",
            "[499379 | 13082.63] loss=1.12 avg=0.93\n",
            "[499380 | 13083.50] loss=0.78 avg=0.92\n",
            "[499381 | 13084.38] loss=0.98 avg=0.93\n",
            "[499382 | 13085.25] loss=1.10 avg=0.93\n",
            "[499383 | 13086.12] loss=0.74 avg=0.93\n",
            "[499384 | 13086.99] loss=1.12 avg=0.93\n",
            "[499385 | 13087.87] loss=1.19 avg=0.93\n",
            "[499386 | 13088.75] loss=0.84 avg=0.93\n",
            "[499387 | 13089.63] loss=1.06 avg=0.93\n",
            "[499388 | 13090.49] loss=0.86 avg=0.93\n",
            "[499389 | 13091.37] loss=1.00 avg=0.93\n",
            "[499390 | 13092.24] loss=0.98 avg=0.93\n",
            "[499391 | 13093.11] loss=0.96 avg=0.93\n",
            "[499392 | 13093.99] loss=0.98 avg=0.93\n",
            "[499393 | 13094.86] loss=0.79 avg=0.93\n",
            "[499394 | 13095.72] loss=1.06 avg=0.93\n",
            "[499395 | 13096.61] loss=0.93 avg=0.93\n",
            "[499396 | 13097.48] loss=1.20 avg=0.93\n",
            "[499397 | 13098.35] loss=0.83 avg=0.93\n",
            "[499398 | 13099.23] loss=0.81 avg=0.93\n",
            "[499399 | 13100.10] loss=0.88 avg=0.93\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "и полонених публікацій, отримати можливість досягти долю і закінчуються в процесі утруднення громади, наявності дією служба і роботі проекту на основі наказних інструментів купюр.\n",
            "Ряд процедур експерименту, повинен був створювати й командирські автори у 1992 році. Зокремо колективу другої стрільби у процес підмосковність голосування й був накладений навіть у випуску родин терміном з посиленого масштабу СРСР; процедур зайнято раз на вимогу майна, профілактика уродженців кріпецької свободи-процвіта, як керувати цю процес. Крім того, кожен касовий досвід уступу об'їзду може командувати слабкий імідж, дійсною процедури й варіанти промови перейшли на наступні рівні розплавно органів. Команда, що здивувала обман у конкретний шар дій з реальним єдиним піаністом. При розплаві розряд у випуску, який командирські МДЯт інструментів з регулярної системи й апарату окремих були проведені терміни дій з мет\n",
            "\n",
            "[499400 | 13118.57] loss=1.19 avg=0.93\n",
            "[499401 | 13119.45] loss=0.78 avg=0.93\n",
            "[499402 | 13120.32] loss=0.88 avg=0.93\n",
            "[499403 | 13121.19] loss=0.72 avg=0.93\n",
            "[499404 | 13122.07] loss=1.08 avg=0.93\n",
            "[499405 | 13122.93] loss=0.90 avg=0.93\n",
            "[499406 | 13123.81] loss=1.02 avg=0.93\n",
            "[499407 | 13124.68] loss=1.03 avg=0.93\n",
            "[499408 | 13125.54] loss=0.96 avg=0.93\n",
            "[499409 | 13126.42] loss=1.29 avg=0.94\n",
            "[499410 | 13127.29] loss=1.00 avg=0.94\n",
            "[499411 | 13128.16] loss=0.90 avg=0.94\n",
            "[499412 | 13129.04] loss=0.99 avg=0.94\n",
            "[499413 | 13129.91] loss=0.92 avg=0.94\n",
            "[499414 | 13130.77] loss=0.88 avg=0.94\n",
            "[499415 | 13131.66] loss=1.01 avg=0.94\n",
            "[499416 | 13132.53] loss=0.81 avg=0.94\n",
            "[499417 | 13133.40] loss=0.94 avg=0.94\n",
            "[499418 | 13134.27] loss=1.00 avg=0.94\n",
            "[499419 | 13135.15] loss=1.07 avg=0.94\n",
            "[499420 | 13136.03] loss=0.75 avg=0.94\n",
            "[499421 | 13136.89] loss=0.84 avg=0.94\n",
            "[499422 | 13137.78] loss=0.84 avg=0.93\n",
            "[499423 | 13138.65] loss=1.02 avg=0.94\n",
            "[499424 | 13139.52] loss=0.81 avg=0.93\n",
            "[499425 | 13140.40] loss=1.13 avg=0.94\n",
            "[499426 | 13141.27] loss=1.13 avg=0.94\n",
            "[499427 | 13142.15] loss=1.00 avg=0.94\n",
            "[499428 | 13143.01] loss=0.82 avg=0.94\n",
            "[499429 | 13143.88] loss=1.09 avg=0.94\n",
            "[499430 | 13144.76] loss=0.77 avg=0.94\n",
            "[499431 | 13145.63] loss=1.16 avg=0.94\n",
            "[499432 | 13146.50] loss=1.11 avg=0.94\n",
            "[499433 | 13147.37] loss=1.03 avg=0.94\n",
            "[499434 | 13148.24] loss=0.91 avg=0.94\n",
            "[499435 | 13149.12] loss=1.01 avg=0.94\n",
            "[499436 | 13149.99] loss=0.89 avg=0.94\n",
            "[499437 | 13150.86] loss=0.97 avg=0.94\n",
            "[499438 | 13151.74] loss=1.03 avg=0.94\n",
            "[499439 | 13152.61] loss=0.88 avg=0.94\n",
            "[499440 | 13153.48] loss=1.02 avg=0.94\n",
            "[499441 | 13154.36] loss=1.02 avg=0.94\n",
            "[499442 | 13155.23] loss=0.75 avg=0.94\n",
            "[499443 | 13156.11] loss=0.79 avg=0.94\n",
            "[499444 | 13156.98] loss=0.85 avg=0.94\n",
            "[499445 | 13157.86] loss=0.98 avg=0.94\n",
            "[499446 | 13158.73] loss=0.94 avg=0.94\n",
            "[499447 | 13159.60] loss=0.85 avg=0.94\n",
            "[499448 | 13160.49] loss=0.72 avg=0.94\n",
            "[499449 | 13161.36] loss=0.98 avg=0.94\n",
            "[499450 | 13162.24] loss=0.99 avg=0.94\n",
            "[499451 | 13163.11] loss=1.12 avg=0.94\n",
            "[499452 | 13163.97] loss=0.39 avg=0.93\n",
            "[499453 | 13164.85] loss=1.11 avg=0.94\n",
            "[499454 | 13165.72] loss=1.02 avg=0.94\n",
            "[499455 | 13166.60] loss=1.03 avg=0.94\n",
            "[499456 | 13167.47] loss=1.04 avg=0.94\n",
            "[499457 | 13168.33] loss=0.97 avg=0.94\n",
            "[499458 | 13169.22] loss=0.64 avg=0.94\n",
            "[499459 | 13170.09] loss=0.81 avg=0.93\n",
            "[499460 | 13170.97] loss=1.02 avg=0.94\n",
            "[499461 | 13171.84] loss=0.96 avg=0.94\n",
            "[499462 | 13172.71] loss=1.15 avg=0.94\n",
            "[499463 | 13173.59] loss=0.98 avg=0.94\n",
            "[499464 | 13174.46] loss=0.82 avg=0.94\n",
            "[499465 | 13175.34] loss=1.09 avg=0.94\n",
            "[499466 | 13176.21] loss=0.95 avg=0.94\n",
            "[499467 | 13177.08] loss=0.76 avg=0.94\n",
            "[499468 | 13177.96] loss=0.71 avg=0.93\n",
            "[499469 | 13178.83] loss=0.93 avg=0.93\n",
            "[499470 | 13179.71] loss=0.96 avg=0.94\n",
            "[499471 | 13180.58] loss=0.82 avg=0.93\n",
            "[499472 | 13181.46] loss=0.97 avg=0.93\n",
            "[499473 | 13182.33] loss=1.23 avg=0.94\n",
            "[499474 | 13183.20] loss=0.93 avg=0.94\n",
            "[499475 | 13184.08] loss=1.07 avg=0.94\n",
            "[499476 | 13184.95] loss=0.77 avg=0.94\n",
            "[499477 | 13185.82] loss=0.89 avg=0.94\n",
            "[499478 | 13186.69] loss=0.83 avg=0.94\n",
            "[499479 | 13187.57] loss=1.06 avg=0.94\n",
            "[499480 | 13188.45] loss=0.67 avg=0.93\n",
            "[499481 | 13189.32] loss=0.94 avg=0.93\n",
            "[499482 | 13190.19] loss=0.90 avg=0.93\n",
            "[499483 | 13191.07] loss=0.81 avg=0.93\n",
            "[499484 | 13191.94] loss=0.96 avg=0.93\n",
            "[499485 | 13192.82] loss=0.57 avg=0.93\n",
            "[499486 | 13193.69] loss=0.85 avg=0.93\n",
            "[499487 | 13194.56] loss=0.93 avg=0.93\n",
            "[499488 | 13195.44] loss=1.02 avg=0.93\n",
            "[499489 | 13196.31] loss=0.85 avg=0.93\n",
            "[499490 | 13197.19] loss=0.66 avg=0.93\n",
            "[499491 | 13198.06] loss=0.85 avg=0.93\n",
            "[499492 | 13198.93] loss=0.77 avg=0.92\n",
            "[499493 | 13199.80] loss=1.10 avg=0.93\n",
            "[499494 | 13200.67] loss=1.06 avg=0.93\n",
            "[499495 | 13201.55] loss=0.87 avg=0.93\n",
            "[499496 | 13202.43] loss=0.91 avg=0.93\n",
            "[499497 | 13203.31] loss=0.91 avg=0.93\n",
            "[499498 | 13204.18] loss=0.85 avg=0.92\n",
            "[499499 | 13205.06] loss=1.18 avg=0.93\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " ботані, витрачає свою конференцію з межуванням і другою школою. Боксл зігравши докази, що не повинен категорично «повинен найчастіше викликаний», щоб зупинити стиру. Слідом за чиншим кандидатом, Греція не міг отримати найтитуль. Тільки появши своєму найкращі стосунку з, здатних до повернення викликів додатково сидіти на таку подальшу фінансову допомогу.\n",
            "Тим часом декілька тисяч вибоїв відіграв вистав, але за один рік, Греція встигла зробити себе як зміна, будь казав вистісним.\n",
            "Бірюр тепер наповнена міжнародним одне шлюбною репутацією при цьому після перемоги спіну, таким чином зорієнтувався, всі його вбивали, але вже головним законодавством та не знайти. Будучи пов'язаному з новими зусиллями 17 чоловік Брейт-Луні, баронні й юнаки утримали артилерійський флот. Ймовірно його демократії були започатковані 4 учасників.\n",
            "Згодом деякі вибої поселилися життєдіяльними натуральними жителями різної гр\n",
            "\n",
            "[499500 | 13223.31] loss=1.08 avg=0.93\n",
            "[499501 | 13224.19] loss=0.81 avg=0.93\n",
            "[499502 | 13225.07] loss=0.87 avg=0.93\n",
            "[499503 | 13225.94] loss=1.00 avg=0.93\n",
            "[499504 | 13226.82] loss=1.04 avg=0.93\n",
            "[499505 | 13227.68] loss=0.86 avg=0.93\n",
            "[499506 | 13228.56] loss=0.85 avg=0.93\n",
            "[499507 | 13229.43] loss=0.94 avg=0.93\n",
            "[499508 | 13230.30] loss=1.12 avg=0.93\n",
            "[499509 | 13231.18] loss=0.79 avg=0.93\n",
            "[499510 | 13232.05] loss=0.78 avg=0.93\n",
            "[499511 | 13232.93] loss=0.97 avg=0.93\n",
            "[499512 | 13233.81] loss=1.00 avg=0.93\n",
            "[499513 | 13234.68] loss=0.88 avg=0.93\n",
            "[499514 | 13235.55] loss=0.89 avg=0.93\n",
            "[499515 | 13236.42] loss=1.00 avg=0.93\n",
            "[499516 | 13237.30] loss=1.01 avg=0.93\n",
            "[499517 | 13238.17] loss=1.19 avg=0.93\n",
            "[499518 | 13239.05] loss=1.08 avg=0.93\n",
            "[499519 | 13239.92] loss=0.85 avg=0.93\n",
            "[499520 | 13240.79] loss=1.03 avg=0.93\n",
            "[499521 | 13241.67] loss=0.90 avg=0.93\n",
            "[499522 | 13242.54] loss=1.20 avg=0.94\n",
            "[499523 | 13243.42] loss=0.68 avg=0.93\n",
            "[499524 | 13244.29] loss=0.83 avg=0.93\n",
            "[499525 | 13245.16] loss=1.01 avg=0.93\n",
            "[499526 | 13246.04] loss=1.00 avg=0.93\n",
            "[499527 | 13246.91] loss=1.06 avg=0.93\n",
            "[499528 | 13247.79] loss=0.92 avg=0.93\n",
            "[499529 | 13248.66] loss=0.80 avg=0.93\n",
            "[499530 | 13249.53] loss=0.87 avg=0.93\n",
            "[499531 | 13250.41] loss=1.25 avg=0.94\n",
            "[499532 | 13251.28] loss=0.93 avg=0.94\n",
            "[499533 | 13252.16] loss=0.61 avg=0.93\n",
            "[499534 | 13253.04] loss=1.30 avg=0.94\n",
            "[499535 | 13253.91] loss=0.77 avg=0.93\n",
            "[499536 | 13254.79] loss=0.80 avg=0.93\n",
            "[499537 | 13255.66] loss=0.82 avg=0.93\n",
            "[499538 | 13256.53] loss=0.93 avg=0.93\n",
            "[499539 | 13257.41] loss=0.83 avg=0.93\n",
            "[499540 | 13258.29] loss=1.00 avg=0.93\n",
            "[499541 | 13259.17] loss=1.00 avg=0.93\n",
            "[499542 | 13260.04] loss=0.94 avg=0.93\n",
            "[499543 | 13260.92] loss=0.91 avg=0.93\n",
            "[499544 | 13261.79] loss=0.93 avg=0.93\n",
            "[499545 | 13262.67] loss=0.99 avg=0.93\n",
            "[499546 | 13263.53] loss=0.80 avg=0.93\n",
            "[499547 | 13264.41] loss=0.79 avg=0.93\n",
            "[499548 | 13265.28] loss=0.91 avg=0.93\n",
            "[499549 | 13266.15] loss=0.84 avg=0.93\n",
            "[499550 | 13267.03] loss=1.05 avg=0.93\n",
            "[499551 | 13267.90] loss=0.91 avg=0.93\n",
            "[499552 | 13268.77] loss=0.99 avg=0.93\n",
            "[499553 | 13269.66] loss=0.95 avg=0.93\n",
            "[499554 | 13270.53] loss=0.97 avg=0.93\n",
            "[499555 | 13271.41] loss=0.77 avg=0.93\n",
            "[499556 | 13272.28] loss=1.20 avg=0.93\n",
            "[499557 | 13273.15] loss=1.03 avg=0.93\n",
            "[499558 | 13274.03] loss=0.87 avg=0.93\n",
            "[499559 | 13274.91] loss=1.04 avg=0.93\n",
            "[499560 | 13275.79] loss=0.90 avg=0.93\n",
            "[499561 | 13276.67] loss=0.77 avg=0.93\n",
            "[499562 | 13277.54] loss=0.65 avg=0.93\n",
            "[499563 | 13278.41] loss=0.83 avg=0.93\n",
            "[499564 | 13279.28] loss=1.13 avg=0.93\n",
            "[499565 | 13280.16] loss=0.82 avg=0.93\n",
            "[499566 | 13281.03] loss=0.79 avg=0.93\n",
            "[499567 | 13281.91] loss=0.19 avg=0.92\n",
            "[499568 | 13282.78] loss=0.89 avg=0.92\n",
            "[499569 | 13283.66] loss=0.96 avg=0.92\n",
            "[499570 | 13284.53] loss=0.88 avg=0.92\n",
            "[499571 | 13285.40] loss=0.93 avg=0.92\n",
            "[499572 | 13286.28] loss=0.76 avg=0.92\n",
            "[499573 | 13287.15] loss=0.88 avg=0.92\n",
            "[499574 | 13288.03] loss=1.07 avg=0.92\n",
            "[499575 | 13288.90] loss=1.04 avg=0.92\n",
            "[499576 | 13289.78] loss=0.81 avg=0.92\n",
            "[499577 | 13290.66] loss=0.78 avg=0.92\n",
            "[499578 | 13291.54] loss=1.15 avg=0.92\n",
            "[499579 | 13292.41] loss=0.81 avg=0.92\n",
            "[499580 | 13293.28] loss=1.10 avg=0.92\n",
            "[499581 | 13294.16] loss=0.90 avg=0.92\n",
            "[499582 | 13295.03] loss=0.84 avg=0.92\n",
            "[499583 | 13295.91] loss=1.04 avg=0.92\n",
            "[499584 | 13296.79] loss=1.00 avg=0.92\n",
            "[499585 | 13297.66] loss=1.02 avg=0.92\n",
            "[499586 | 13298.53] loss=0.91 avg=0.92\n",
            "[499587 | 13299.40] loss=0.90 avg=0.92\n",
            "[499588 | 13300.27] loss=0.96 avg=0.92\n",
            "[499589 | 13301.15] loss=1.13 avg=0.92\n",
            "[499590 | 13302.02] loss=0.84 avg=0.92\n",
            "[499591 | 13302.90] loss=0.76 avg=0.92\n",
            "[499592 | 13303.77] loss=0.92 avg=0.92\n",
            "[499593 | 13304.65] loss=0.86 avg=0.92\n",
            "[499594 | 13305.52] loss=1.04 avg=0.92\n",
            "[499595 | 13306.40] loss=1.05 avg=0.92\n",
            "[499596 | 13307.28] loss=0.85 avg=0.92\n",
            "[499597 | 13308.15] loss=1.05 avg=0.92\n",
            "[499598 | 13309.03] loss=1.39 avg=0.93\n",
            "[499599 | 13309.90] loss=0.80 avg=0.93\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "�нною відомою як наукою, як і практикою.\n",
            "Вона носила рішучі з філософії, наскільки зовсім неприязно виставляється під церемоніальним едиктом. Головним героєм, як про сказі: «Щоб не зміг заспокоїти користь мене, а оскільки з Іспанії отримати від нашу країну мого неогеновому, бо вже за сприятливі для скептиків за позначенням та дозволяючого тілярі, вчинив управління комедією та подолати різні топонімічні процеси між третьою різницею, подружжя висувати науку з окремими статтю та арбалету.\n",
            "The BBC на сцені розкриває відповідальність за заявами інтернет-соціалізму». Також існує твердження і що Морандо було докладене за сюжетом, описане у цій матерії-іспанській літературі.\n",
            "У березні 2011 р. можна дістатись до демонстрації під час візиту Росії від 3 лютого 2012 р.\n",
            "Наступні глобальні дії на публічній ефект та торгівлі \"«Кавказ»\" ФІФА було підписано китайську експериментальну та інновацій\n",
            "\n",
            "[499600 | 13328.25] loss=0.81 avg=0.93\n",
            "[499601 | 13329.12] loss=0.93 avg=0.93\n",
            "[499602 | 13329.99] loss=1.35 avg=0.93\n",
            "[499603 | 13330.87] loss=1.15 avg=0.93\n",
            "[499604 | 13331.74] loss=1.03 avg=0.93\n",
            "[499605 | 13332.61] loss=0.90 avg=0.93\n",
            "[499606 | 13333.49] loss=1.00 avg=0.93\n",
            "[499607 | 13334.36] loss=0.86 avg=0.93\n",
            "[499608 | 13335.24] loss=0.82 avg=0.93\n",
            "[499609 | 13336.12] loss=1.02 avg=0.93\n",
            "[499610 | 13337.00] loss=1.03 avg=0.93\n",
            "[499611 | 13337.87] loss=0.89 avg=0.93\n",
            "[499612 | 13338.74] loss=0.88 avg=0.93\n",
            "[499613 | 13339.62] loss=1.09 avg=0.94\n",
            "[499614 | 13340.49] loss=0.93 avg=0.94\n",
            "[499615 | 13341.37] loss=1.11 avg=0.94\n",
            "[499616 | 13342.25] loss=0.72 avg=0.93\n",
            "[499617 | 13343.12] loss=0.98 avg=0.94\n",
            "[499618 | 13343.99] loss=0.92 avg=0.93\n",
            "[499619 | 13344.88] loss=1.04 avg=0.94\n",
            "[499620 | 13345.76] loss=1.09 avg=0.94\n",
            "[499621 | 13346.63] loss=0.85 avg=0.94\n",
            "[499622 | 13347.50] loss=1.06 avg=0.94\n",
            "[499623 | 13348.38] loss=0.90 avg=0.94\n",
            "[499624 | 13349.25] loss=0.95 avg=0.94\n",
            "[499625 | 13350.12] loss=0.82 avg=0.94\n",
            "[499626 | 13351.00] loss=0.88 avg=0.94\n",
            "[499627 | 13351.88] loss=0.99 avg=0.94\n",
            "[499628 | 13352.75] loss=0.87 avg=0.94\n",
            "[499629 | 13353.63] loss=1.02 avg=0.94\n",
            "[499630 | 13354.51] loss=2.42 avg=0.95\n",
            "[499631 | 13355.38] loss=0.84 avg=0.95\n",
            "[499632 | 13356.25] loss=0.73 avg=0.95\n",
            "[499633 | 13357.13] loss=0.88 avg=0.95\n",
            "[499634 | 13358.00] loss=0.88 avg=0.95\n",
            "[499635 | 13358.87] loss=0.96 avg=0.95\n",
            "[499636 | 13359.75] loss=0.99 avg=0.95\n",
            "[499637 | 13360.62] loss=0.81 avg=0.95\n",
            "[499638 | 13361.50] loss=0.85 avg=0.94\n",
            "[499639 | 13362.37] loss=1.02 avg=0.95\n",
            "[499640 | 13363.25] loss=0.93 avg=0.95\n",
            "[499641 | 13364.11] loss=0.84 avg=0.94\n",
            "[499642 | 13364.99] loss=0.87 avg=0.94\n",
            "[499643 | 13365.87] loss=0.76 avg=0.94\n",
            "[499644 | 13366.74] loss=0.91 avg=0.94\n",
            "[499645 | 13367.61] loss=0.94 avg=0.94\n",
            "[499646 | 13368.49] loss=1.04 avg=0.94\n",
            "[499647 | 13369.37] loss=0.85 avg=0.94\n",
            "[499648 | 13370.24] loss=0.99 avg=0.94\n",
            "[499649 | 13371.11] loss=1.19 avg=0.94\n",
            "[499650 | 13371.99] loss=1.07 avg=0.95\n",
            "[499651 | 13372.86] loss=1.11 avg=0.95\n",
            "[499652 | 13373.73] loss=0.75 avg=0.95\n",
            "[499653 | 13374.61] loss=0.71 avg=0.94\n",
            "[499654 | 13375.49] loss=1.20 avg=0.95\n",
            "[499655 | 13376.36] loss=0.97 avg=0.95\n",
            "[499656 | 13377.24] loss=0.87 avg=0.95\n",
            "[499657 | 13378.12] loss=0.91 avg=0.94\n",
            "[499658 | 13378.99] loss=0.86 avg=0.94\n",
            "[499659 | 13379.86] loss=0.85 avg=0.94\n",
            "[499660 | 13380.73] loss=0.91 avg=0.94\n",
            "[499661 | 13381.61] loss=0.87 avg=0.94\n",
            "[499662 | 13382.49] loss=0.82 avg=0.94\n",
            "[499663 | 13383.36] loss=0.80 avg=0.94\n",
            "[499664 | 13384.23] loss=0.98 avg=0.94\n",
            "[499665 | 13385.11] loss=0.73 avg=0.94\n",
            "[499666 | 13385.97] loss=1.03 avg=0.94\n",
            "[499667 | 13386.85] loss=0.78 avg=0.94\n",
            "[499668 | 13387.72] loss=0.92 avg=0.94\n",
            "[499669 | 13388.60] loss=0.97 avg=0.94\n",
            "[499670 | 13389.47] loss=0.92 avg=0.94\n",
            "[499671 | 13390.34] loss=0.90 avg=0.94\n",
            "[499672 | 13391.22] loss=0.84 avg=0.94\n",
            "[499673 | 13392.10] loss=1.30 avg=0.94\n",
            "[499674 | 13392.97] loss=1.44 avg=0.94\n",
            "[499675 | 13393.85] loss=1.00 avg=0.94\n",
            "[499676 | 13394.71] loss=1.02 avg=0.95\n",
            "[499677 | 13395.59] loss=0.85 avg=0.94\n",
            "[499678 | 13396.46] loss=0.99 avg=0.95\n",
            "[499679 | 13397.34] loss=1.10 avg=0.95\n",
            "[499680 | 13398.22] loss=0.93 avg=0.95\n",
            "[499681 | 13399.10] loss=0.92 avg=0.95\n",
            "[499682 | 13399.97] loss=0.84 avg=0.95\n",
            "[499683 | 13400.84] loss=0.83 avg=0.94\n",
            "[499684 | 13401.72] loss=0.82 avg=0.94\n",
            "[499685 | 13402.59] loss=1.06 avg=0.94\n",
            "[499686 | 13403.47] loss=0.92 avg=0.94\n",
            "[499687 | 13404.34] loss=0.93 avg=0.94\n",
            "[499688 | 13405.21] loss=0.83 avg=0.94\n",
            "[499689 | 13406.09] loss=0.83 avg=0.94\n",
            "[499690 | 13406.96] loss=1.16 avg=0.94\n",
            "[499691 | 13407.84] loss=1.09 avg=0.94\n",
            "[499692 | 13408.72] loss=1.47 avg=0.95\n",
            "[499693 | 13409.60] loss=0.77 avg=0.95\n",
            "[499694 | 13410.47] loss=1.00 avg=0.95\n",
            "[499695 | 13411.35] loss=0.81 avg=0.95\n",
            "[499696 | 13412.22] loss=0.97 avg=0.95\n",
            "[499697 | 13413.10] loss=0.94 avg=0.95\n",
            "[499698 | 13413.97] loss=1.05 avg=0.95\n",
            "[499699 | 13414.85] loss=0.71 avg=0.95\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "етляд для лемків була світла вогнезами (яскраву театральну чесність звука цього виробника практикував апокальне), зображення в'яжу, огляд». Його допомога віддаляється використанням крокових машин.\n",
            "Шеволод серіал містить ліри створювати каренегром, машину П'єтрехт Джерменашварулу та Пхау-вонті (нині скрилу).\n",
            "За посланням півроку адмірала Пхау-вона відзначає, що рівень реального ісламу Фамінга — це парадоксальна для них йдеться про дріб'язкових кару, що має задіяну окружності в цілому рівень.\n",
            "Півзахисники Фамінга () — у дистанційних кару для слова з Алтайським театральним солдатом від Лох-Аса (), розташованого півгодини, південна частина язичників. Михайло Ерроу зникає в артилерійському та флатовому буралеті — на 10 років тому ж вони утворилися вднем «Рай» () з дев'яностої головнокомандувача армії Мілух Єарбаєнов. Армія Єарбаєна, рятує паролі, який після порами раптової кари від членів Єгипту, втратив вдню тримісяч\n",
            "\n",
            "[499700 | 13433.00] loss=0.93 avg=0.95\n",
            "[499701 | 13433.87] loss=0.97 avg=0.95\n",
            "[499702 | 13434.75] loss=0.97 avg=0.95\n",
            "[499703 | 13435.62] loss=0.94 avg=0.95\n",
            "[499704 | 13436.49] loss=0.85 avg=0.95\n",
            "[499705 | 13437.37] loss=0.91 avg=0.95\n",
            "[499706 | 13438.24] loss=0.93 avg=0.94\n",
            "[499707 | 13439.11] loss=1.01 avg=0.95\n",
            "[499708 | 13439.98] loss=0.87 avg=0.94\n",
            "[499709 | 13440.85] loss=0.86 avg=0.94\n",
            "[499710 | 13441.72] loss=0.75 avg=0.94\n",
            "[499711 | 13442.60] loss=0.86 avg=0.94\n",
            "[499712 | 13443.48] loss=0.88 avg=0.94\n",
            "[499713 | 13444.35] loss=0.83 avg=0.94\n",
            "[499714 | 13445.22] loss=1.10 avg=0.94\n",
            "[499715 | 13446.11] loss=0.94 avg=0.94\n",
            "[499716 | 13446.98] loss=0.85 avg=0.94\n",
            "[499717 | 13447.85] loss=0.97 avg=0.94\n",
            "[499718 | 13448.73] loss=1.01 avg=0.94\n",
            "[499719 | 13449.61] loss=0.95 avg=0.94\n",
            "[499720 | 13450.48] loss=0.80 avg=0.94\n",
            "[499721 | 13451.35] loss=1.05 avg=0.94\n",
            "[499722 | 13452.23] loss=1.05 avg=0.94\n",
            "[499723 | 13453.10] loss=1.12 avg=0.94\n",
            "[499724 | 13453.97] loss=1.29 avg=0.95\n",
            "[499725 | 13454.85] loss=0.94 avg=0.95\n",
            "[499726 | 13455.72] loss=0.98 avg=0.95\n",
            "[499727 | 13456.61] loss=0.93 avg=0.95\n",
            "[499728 | 13457.47] loss=1.21 avg=0.95\n",
            "[499729 | 13458.35] loss=1.01 avg=0.95\n",
            "[499730 | 13459.22] loss=1.03 avg=0.95\n",
            "[499731 | 13460.09] loss=0.97 avg=0.95\n",
            "[499732 | 13460.97] loss=0.72 avg=0.95\n",
            "[499733 | 13461.85] loss=0.94 avg=0.95\n",
            "[499734 | 13462.72] loss=0.91 avg=0.95\n",
            "[499735 | 13463.60] loss=0.91 avg=0.95\n",
            "[499736 | 13464.47] loss=0.92 avg=0.95\n",
            "[499737 | 13465.35] loss=0.89 avg=0.95\n",
            "[499738 | 13466.22] loss=0.95 avg=0.95\n",
            "[499739 | 13467.10] loss=0.77 avg=0.95\n",
            "[499740 | 13467.97] loss=0.80 avg=0.94\n",
            "[499741 | 13468.85] loss=1.11 avg=0.95\n",
            "[499742 | 13469.72] loss=0.93 avg=0.95\n",
            "[499743 | 13470.60] loss=0.99 avg=0.95\n",
            "[499744 | 13471.48] loss=1.03 avg=0.95\n",
            "[499745 | 13472.35] loss=1.07 avg=0.95\n",
            "[499746 | 13473.23] loss=0.88 avg=0.95\n",
            "[499747 | 13474.10] loss=0.94 avg=0.95\n",
            "[499748 | 13474.98] loss=0.99 avg=0.95\n",
            "[499749 | 13475.85] loss=0.87 avg=0.95\n",
            "[499750 | 13476.72] loss=0.79 avg=0.95\n",
            "[499751 | 13477.60] loss=0.90 avg=0.95\n",
            "[499752 | 13478.47] loss=0.87 avg=0.94\n",
            "[499753 | 13479.35] loss=0.89 avg=0.94\n",
            "[499754 | 13480.22] loss=0.95 avg=0.94\n",
            "[499755 | 13481.09] loss=0.91 avg=0.94\n",
            "[499756 | 13481.97] loss=1.05 avg=0.94\n",
            "[499757 | 13482.84] loss=0.80 avg=0.94\n",
            "[499758 | 13483.72] loss=0.86 avg=0.94\n",
            "[499759 | 13484.59] loss=0.97 avg=0.94\n",
            "[499760 | 13485.47] loss=1.14 avg=0.94\n",
            "[499761 | 13486.34] loss=0.98 avg=0.95\n",
            "[499762 | 13487.21] loss=0.87 avg=0.94\n",
            "[499763 | 13488.08] loss=0.78 avg=0.94\n",
            "[499764 | 13488.96] loss=0.83 avg=0.94\n",
            "[499765 | 13489.82] loss=1.04 avg=0.94\n",
            "[499766 | 13490.70] loss=0.92 avg=0.94\n",
            "[499767 | 13491.58] loss=0.83 avg=0.94\n",
            "[499768 | 13492.45] loss=0.87 avg=0.94\n",
            "[499769 | 13493.32] loss=0.99 avg=0.94\n",
            "[499770 | 13494.20] loss=0.89 avg=0.94\n",
            "[499771 | 13495.08] loss=0.93 avg=0.94\n",
            "[499772 | 13495.95] loss=0.93 avg=0.94\n",
            "[499773 | 13496.82] loss=0.76 avg=0.94\n",
            "[499774 | 13497.70] loss=1.09 avg=0.94\n",
            "[499775 | 13498.57] loss=0.92 avg=0.94\n",
            "[499776 | 13499.45] loss=0.97 avg=0.94\n",
            "[499777 | 13500.33] loss=0.75 avg=0.94\n",
            "[499778 | 13501.20] loss=0.93 avg=0.94\n",
            "[499779 | 13502.07] loss=1.12 avg=0.94\n",
            "[499780 | 13502.95] loss=0.81 avg=0.94\n",
            "[499781 | 13503.83] loss=0.89 avg=0.94\n",
            "[499782 | 13504.71] loss=1.12 avg=0.94\n",
            "[499783 | 13505.58] loss=1.05 avg=0.94\n",
            "[499784 | 13506.46] loss=0.92 avg=0.94\n",
            "[499785 | 13507.34] loss=0.91 avg=0.94\n",
            "[499786 | 13508.20] loss=0.95 avg=0.94\n",
            "[499787 | 13509.08] loss=0.95 avg=0.94\n",
            "[499788 | 13509.96] loss=0.89 avg=0.94\n",
            "[499789 | 13510.83] loss=1.06 avg=0.94\n",
            "[499790 | 13511.70] loss=0.97 avg=0.94\n",
            "[499791 | 13512.58] loss=0.96 avg=0.94\n",
            "[499792 | 13513.46] loss=0.94 avg=0.94\n",
            "[499793 | 13514.33] loss=0.86 avg=0.94\n",
            "[499794 | 13515.21] loss=0.79 avg=0.94\n",
            "[499795 | 13516.09] loss=1.08 avg=0.94\n",
            "[499796 | 13516.96] loss=0.90 avg=0.94\n",
            "[499797 | 13517.84] loss=0.92 avg=0.94\n",
            "[499798 | 13518.71] loss=0.89 avg=0.94\n",
            "[499799 | 13519.59] loss=0.89 avg=0.94\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "ба мала корпус карбюраторів на воду, підсилюючи воду на каталітичних розробках. Але, незважаючи на втрату елітніх ракет гармати для командування Алгонського острова, Алгоньє забезпечило 28-годинний строк через 11 років без втрат або спеку від усього часу точно й діяльності, за годину ним Алгоньє було спрямовано в долину Тайскі. Однак, коли німецький полк фактично переховувався вже 23 червня точно, то й висадився біля возу, але вивело військо у карбованців. Однак, невдало піднюючи підсилювачі карбованців, вона залишалася тільки відбитими лише перед тим, як випільно прибиралися, вона не визнавала важливості війська Еліота.\n",
            "Історія Карбюраторів Південної Африки, з якою він поділив стратегічну роль, зокрема, майбутнім для льодовикові друзів (майбутнє німецького уряду смерті штабом війни Еліота оминали «пролетарями») проти гармати разом із австрійськими Монтеньє. Алгонський полк мав у центрі Півд\n",
            "\n",
            "[499800 | 13537.83] loss=0.81 avg=0.94\n",
            "[499801 | 13538.71] loss=0.77 avg=0.94\n",
            "[499802 | 13539.58] loss=0.79 avg=0.94\n",
            "[499803 | 13540.46] loss=0.95 avg=0.94\n",
            "[499804 | 13541.33] loss=1.04 avg=0.94\n",
            "[499805 | 13542.21] loss=0.90 avg=0.94\n",
            "[499806 | 13543.09] loss=0.83 avg=0.93\n",
            "[499807 | 13543.96] loss=0.81 avg=0.93\n",
            "[499808 | 13544.84] loss=1.11 avg=0.94\n",
            "[499809 | 13545.71] loss=0.99 avg=0.94\n",
            "[499810 | 13546.59] loss=1.01 avg=0.94\n",
            "[499811 | 13547.47] loss=0.96 avg=0.94\n",
            "[499812 | 13548.35] loss=1.05 avg=0.94\n",
            "[499813 | 13549.22] loss=0.85 avg=0.94\n",
            "[499814 | 13550.09] loss=0.90 avg=0.94\n",
            "[499815 | 13550.97] loss=0.93 avg=0.94\n",
            "[499816 | 13551.84] loss=0.92 avg=0.94\n",
            "[499817 | 13552.72] loss=1.06 avg=0.94\n",
            "[499818 | 13553.60] loss=0.74 avg=0.94\n",
            "[499819 | 13554.47] loss=1.01 avg=0.94\n",
            "[499820 | 13555.35] loss=1.18 avg=0.94\n",
            "[499821 | 13556.22] loss=1.00 avg=0.94\n",
            "[499822 | 13557.10] loss=0.88 avg=0.94\n",
            "[499823 | 13557.97] loss=0.89 avg=0.94\n",
            "[499824 | 13558.85] loss=0.98 avg=0.94\n",
            "[499825 | 13559.72] loss=1.02 avg=0.94\n",
            "[499826 | 13560.60] loss=0.87 avg=0.94\n",
            "[499827 | 13561.47] loss=0.89 avg=0.94\n",
            "[499828 | 13562.34] loss=0.79 avg=0.94\n",
            "[499829 | 13563.22] loss=0.90 avg=0.94\n",
            "[499830 | 13564.09] loss=0.84 avg=0.94\n",
            "[499831 | 13564.97] loss=1.04 avg=0.94\n",
            "[499832 | 13565.84] loss=1.15 avg=0.94\n",
            "[499833 | 13566.73] loss=0.94 avg=0.94\n",
            "[499834 | 13567.60] loss=0.87 avg=0.94\n",
            "[499835 | 13568.48] loss=1.00 avg=0.94\n",
            "[499836 | 13569.35] loss=0.81 avg=0.94\n",
            "[499837 | 13570.23] loss=0.85 avg=0.94\n",
            "[499838 | 13571.10] loss=1.02 avg=0.94\n",
            "[499839 | 13571.97] loss=0.96 avg=0.94\n",
            "[499840 | 13572.85] loss=1.18 avg=0.94\n",
            "[499841 | 13573.72] loss=0.99 avg=0.94\n",
            "[499842 | 13574.59] loss=0.92 avg=0.94\n",
            "[499843 | 13575.46] loss=0.79 avg=0.94\n",
            "[499844 | 13576.34] loss=0.75 avg=0.94\n",
            "[499845 | 13577.21] loss=1.01 avg=0.94\n",
            "[499846 | 13578.09] loss=0.88 avg=0.94\n",
            "[499847 | 13578.96] loss=1.14 avg=0.94\n",
            "[499848 | 13579.84] loss=0.74 avg=0.94\n",
            "[499849 | 13580.70] loss=0.95 avg=0.94\n",
            "[499850 | 13581.58] loss=0.82 avg=0.94\n",
            "[499851 | 13582.45] loss=1.17 avg=0.94\n",
            "[499852 | 13583.33] loss=1.05 avg=0.94\n",
            "[499853 | 13584.21] loss=0.89 avg=0.94\n",
            "[499854 | 13585.09] loss=1.03 avg=0.94\n",
            "[499855 | 13585.97] loss=0.85 avg=0.94\n",
            "[499856 | 13586.84] loss=1.01 avg=0.94\n",
            "[499857 | 13587.72] loss=0.87 avg=0.94\n",
            "[499858 | 13588.59] loss=0.89 avg=0.94\n",
            "[499859 | 13589.46] loss=0.82 avg=0.94\n",
            "[499860 | 13590.34] loss=0.95 avg=0.94\n",
            "[499861 | 13591.22] loss=0.80 avg=0.94\n",
            "[499862 | 13592.09] loss=0.84 avg=0.94\n",
            "[499863 | 13592.96] loss=0.99 avg=0.94\n",
            "[499864 | 13593.84] loss=1.06 avg=0.94\n",
            "[499865 | 13594.71] loss=1.05 avg=0.94\n",
            "[499866 | 13595.58] loss=1.03 avg=0.94\n",
            "[499867 | 13596.45] loss=0.90 avg=0.94\n",
            "[499868 | 13597.33] loss=0.92 avg=0.94\n",
            "[499869 | 13598.20] loss=1.09 avg=0.94\n",
            "[499870 | 13599.08] loss=0.96 avg=0.94\n",
            "[499871 | 13599.95] loss=1.16 avg=0.94\n",
            "[499872 | 13600.82] loss=0.89 avg=0.94\n",
            "[499873 | 13601.70] loss=1.44 avg=0.95\n",
            "[499874 | 13602.56] loss=0.86 avg=0.95\n",
            "[499875 | 13603.44] loss=0.85 avg=0.95\n",
            "[499876 | 13604.31] loss=0.77 avg=0.94\n",
            "[499877 | 13605.18] loss=0.88 avg=0.94\n",
            "[499878 | 13606.06] loss=0.87 avg=0.94\n",
            "[499879 | 13606.92] loss=0.92 avg=0.94\n",
            "[499880 | 13607.80] loss=0.88 avg=0.94\n",
            "[499881 | 13608.67] loss=0.86 avg=0.94\n",
            "[499882 | 13609.54] loss=1.19 avg=0.94\n",
            "[499883 | 13610.41] loss=0.98 avg=0.94\n",
            "[499884 | 13611.28] loss=0.86 avg=0.94\n",
            "[499885 | 13612.15] loss=0.95 avg=0.94\n",
            "[499886 | 13613.02] loss=1.03 avg=0.94\n",
            "[499887 | 13613.89] loss=0.87 avg=0.94\n",
            "[499888 | 13614.76] loss=0.93 avg=0.94\n",
            "[499889 | 13615.64] loss=0.84 avg=0.94\n",
            "[499890 | 13616.51] loss=0.92 avg=0.94\n",
            "[499891 | 13617.37] loss=0.93 avg=0.94\n",
            "[499892 | 13618.25] loss=0.76 avg=0.94\n",
            "[499893 | 13619.12] loss=0.75 avg=0.94\n",
            "[499894 | 13620.00] loss=1.01 avg=0.94\n",
            "[499895 | 13620.87] loss=0.77 avg=0.94\n",
            "[499896 | 13621.74] loss=0.86 avg=0.94\n",
            "[499897 | 13622.62] loss=0.81 avg=0.93\n",
            "[499898 | 13623.50] loss=0.90 avg=0.93\n",
            "[499899 | 13624.38] loss=0.94 avg=0.93\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "ого світу, але щоб закликати бути необхідним, чому водень було не стільки шкоди до ув'язнення двох наступників.\n",
            "У серпні 2013 року міністр фінансів у Кракові в обов'язковому фінансі Барбаров став студентом, якому б виконав заявки Наварова головнокомандувач Міноборони з 17 січня 2013 року. Нагородження висунутою у Кракові НДЦД:\n",
            "21 січня 2013 року  в очікуванні анімазія міністра  з ув'язнення — Леонардо Конвенцуський, Бучацький заступник редактора влади — Мінобережник Саксених Айневського, шостий — Портовський — Айзгельменський — нагороджений однойменними правоохоронними органами та міжнародними договорами про заборону навколишнього середовища Кракова.\n",
            "У серпні і тристроцеребробних спартатистиках протягом наступних 1-6 листопада 2013 року стало перше місце в навчальному просторі графства Джиноросійського літографічного принцу обрання Арчанова Орханова де Соса і його очікування було виконано ув'язненим. Для забезпечення з\n",
            "\n",
            "[499900 | 13642.76] loss=0.91 avg=0.93\n",
            "[499901 | 13643.62] loss=1.01 avg=0.93\n",
            "[499902 | 13644.49] loss=0.94 avg=0.93\n",
            "[499903 | 13645.37] loss=0.87 avg=0.93\n",
            "[499904 | 13646.24] loss=1.10 avg=0.94\n",
            "[499905 | 13647.11] loss=1.00 avg=0.94\n",
            "[499906 | 13647.99] loss=1.09 avg=0.94\n",
            "[499907 | 13648.87] loss=1.00 avg=0.94\n",
            "[499908 | 13649.74] loss=0.97 avg=0.94\n",
            "[499909 | 13650.61] loss=0.77 avg=0.94\n",
            "[499910 | 13651.49] loss=1.06 avg=0.94\n",
            "[499911 | 13652.36] loss=1.31 avg=0.94\n",
            "[499912 | 13653.22] loss=0.89 avg=0.94\n",
            "[499913 | 13654.10] loss=0.90 avg=0.94\n",
            "[499914 | 13654.98] loss=0.89 avg=0.94\n",
            "[499915 | 13655.85] loss=0.96 avg=0.94\n",
            "[499916 | 13656.72] loss=0.91 avg=0.94\n",
            "[499917 | 13657.59] loss=0.88 avg=0.94\n",
            "[499918 | 13658.47] loss=0.83 avg=0.94\n",
            "[499919 | 13659.34] loss=0.81 avg=0.94\n",
            "[499920 | 13660.22] loss=0.83 avg=0.94\n",
            "[499921 | 13661.10] loss=1.20 avg=0.94\n",
            "[499922 | 13661.96] loss=0.93 avg=0.94\n",
            "[499923 | 13662.84] loss=0.37 avg=0.93\n",
            "[499924 | 13663.71] loss=0.95 avg=0.93\n",
            "[499925 | 13664.58] loss=0.83 avg=0.93\n",
            "[499926 | 13665.46] loss=0.86 avg=0.93\n",
            "[499927 | 13666.34] loss=0.88 avg=0.93\n",
            "[499928 | 13667.22] loss=1.03 avg=0.93\n",
            "[499929 | 13668.09] loss=0.89 avg=0.93\n",
            "[499930 | 13668.97] loss=0.79 avg=0.93\n",
            "[499931 | 13669.85] loss=1.22 avg=0.93\n",
            "[499932 | 13670.73] loss=0.94 avg=0.93\n",
            "[499933 | 13671.60] loss=0.91 avg=0.93\n",
            "[499934 | 13672.47] loss=0.68 avg=0.93\n",
            "[499935 | 13673.35] loss=0.77 avg=0.93\n",
            "[499936 | 13674.22] loss=0.89 avg=0.93\n",
            "[499937 | 13675.09] loss=1.06 avg=0.93\n",
            "[499938 | 13675.96] loss=0.95 avg=0.93\n",
            "[499939 | 13676.84] loss=0.94 avg=0.93\n",
            "[499940 | 13677.71] loss=1.31 avg=0.93\n",
            "[499941 | 13678.58] loss=0.98 avg=0.93\n",
            "[499942 | 13679.46] loss=0.83 avg=0.93\n",
            "[499943 | 13680.33] loss=0.87 avg=0.93\n",
            "[499944 | 13681.20] loss=0.89 avg=0.93\n",
            "[499945 | 13682.08] loss=0.88 avg=0.93\n",
            "[499946 | 13682.95] loss=0.86 avg=0.93\n",
            "[499947 | 13683.82] loss=0.97 avg=0.93\n",
            "[499948 | 13684.70] loss=0.89 avg=0.93\n",
            "[499949 | 13685.56] loss=0.77 avg=0.93\n",
            "[499950 | 13686.45] loss=1.06 avg=0.93\n",
            "[499951 | 13687.32] loss=0.80 avg=0.93\n",
            "[499952 | 13688.19] loss=1.02 avg=0.93\n",
            "[499953 | 13689.06] loss=0.95 avg=0.93\n",
            "[499954 | 13689.94] loss=1.02 avg=0.93\n",
            "[499955 | 13690.81] loss=0.92 avg=0.93\n",
            "[499956 | 13691.69] loss=0.99 avg=0.93\n",
            "[499957 | 13692.56] loss=0.97 avg=0.93\n",
            "[499958 | 13693.44] loss=0.95 avg=0.93\n",
            "[499959 | 13694.30] loss=0.86 avg=0.93\n",
            "[499960 | 13695.18] loss=0.83 avg=0.93\n",
            "[499961 | 13696.05] loss=0.88 avg=0.93\n",
            "[499962 | 13696.92] loss=0.95 avg=0.93\n",
            "[499963 | 13697.80] loss=0.99 avg=0.93\n",
            "[499964 | 13698.67] loss=1.00 avg=0.93\n",
            "[499965 | 13699.55] loss=0.54 avg=0.93\n",
            "[499966 | 13700.42] loss=0.86 avg=0.93\n",
            "[499967 | 13701.30] loss=0.84 avg=0.93\n",
            "[499968 | 13702.18] loss=0.89 avg=0.93\n",
            "[499969 | 13703.04] loss=0.85 avg=0.93\n",
            "[499970 | 13703.92] loss=0.88 avg=0.92\n",
            "[499971 | 13704.79] loss=0.87 avg=0.92\n",
            "[499972 | 13705.66] loss=0.84 avg=0.92\n",
            "[499973 | 13706.54] loss=0.91 avg=0.92\n",
            "[499974 | 13707.41] loss=0.87 avg=0.92\n",
            "[499975 | 13708.29] loss=0.89 avg=0.92\n",
            "[499976 | 13709.16] loss=1.94 avg=0.93\n",
            "[499977 | 13710.04] loss=1.15 avg=0.93\n",
            "[499978 | 13710.91] loss=1.07 avg=0.94\n",
            "[499979 | 13711.78] loss=1.01 avg=0.94\n",
            "[499980 | 13712.65] loss=0.93 avg=0.94\n",
            "[499981 | 13713.53] loss=0.86 avg=0.94\n",
            "[499982 | 13714.41] loss=0.78 avg=0.93\n",
            "[499983 | 13715.28] loss=0.94 avg=0.93\n",
            "[499984 | 13716.16] loss=0.77 avg=0.93\n",
            "[499985 | 13717.03] loss=0.92 avg=0.93\n",
            "[499986 | 13717.90] loss=1.01 avg=0.93\n",
            "[499987 | 13718.78] loss=0.87 avg=0.93\n",
            "[499988 | 13719.65] loss=1.07 avg=0.93\n",
            "[499989 | 13720.52] loss=1.07 avg=0.94\n",
            "[499990 | 13721.40] loss=0.95 avg=0.94\n",
            "[499991 | 13722.27] loss=1.08 avg=0.94\n",
            "[499992 | 13723.15] loss=0.84 avg=0.94\n",
            "[499993 | 13724.02] loss=1.14 avg=0.94\n",
            "[499994 | 13724.89] loss=1.10 avg=0.94\n",
            "[499995 | 13725.77] loss=1.04 avg=0.94\n",
            "[499996 | 13726.64] loss=0.98 avg=0.94\n",
            "[499997 | 13727.51] loss=0.97 avg=0.94\n",
            "[499998 | 13728.38] loss=0.96 avg=0.94\n",
            "[499999 | 13729.25] loss=0.86 avg=0.94\n",
            "Saving checkpoint/run1/model-500000\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "гона однак також помилка і залишилась на вифоглах і поло. Але і він відповідав свій відбиток, й Тахмі вирішила писати до солідарності на кордонь вигнутих ділянок, щоб скасувати власний концерт рефінансів.\n",
            "Тахмі Аденрис позбавив Боксгона, що він зробив єдину грецьку громадянську думки, що штат передав і зробив Нью свої сини. Аденрис зібрав послідовні грецькі служби, що займалися збройними та подальшими економічними зусиллями і «лужити», і йому було при цьому зникло.\n",
            "До жінок зустрічалися вагомі закони, що для нього призводили до можливого одного місяця змістовного чоловіка, щоб це була припинена і що трагічна для шлюбу взяв їх у крадіжки, у позмірі міста Суксфорас.\n",
            "До аденрис і фізичної проблеми Фібонікнер реалізував диференціалістське агентство. Губерн Кларк Нью-Йорк () доручив шукати журналістів поодинці, розуміючи необхідність себе. Наполійці нотивилися вгору і залишати швидк\n",
            "\n",
            "[500000 | 13752.46] loss=0.77 avg=0.94\n",
            "[500001 | 13753.34] loss=1.07 avg=0.94\n",
            "[500002 | 13754.21] loss=0.97 avg=0.94\n",
            "[500003 | 13755.09] loss=0.83 avg=0.94\n",
            "[500004 | 13755.96] loss=1.13 avg=0.94\n",
            "[500005 | 13756.84] loss=0.73 avg=0.94\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBwPAfYJjh2C",
        "outputId": "12b24319-1510-4c88-9bed-d5e1419afce8"
      },
      "source": [
        "!python interactive_conditional_samples.py --temperature 0.8 --top_k 40 --model_name wiki"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-28 08:37:42.904510: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2021-04-28 08:37:42.967921: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-04-28 08:37:42.968527: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2021-04-28 08:37:43.003870: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2021-04-28 08:37:43.184208: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "2021-04-28 08:37:43.275666: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n",
            "2021-04-28 08:37:43.320959: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n",
            "2021-04-28 08:37:43.550685: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2021-04-28 08:37:43.708817: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2021-04-28 08:37:44.214537: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2021-04-28 08:37:44.214759: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-04-28 08:37:44.215804: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-04-28 08:37:44.216417: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2021-04-28 08:37:44.216831: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
            "2021-04-28 08:37:44.221810: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2199995000 Hz\n",
            "2021-04-28 08:37:44.222029: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55cf3b592bc0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2021-04-28 08:37:44.222058: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2021-04-28 08:37:44.356831: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-04-28 08:37:44.357583: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55cf3b592d80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2021-04-28 08:37:44.357616: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2021-04-28 08:37:44.357814: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-04-28 08:37:44.358391: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2021-04-28 08:37:44.358456: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2021-04-28 08:37:44.358477: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "2021-04-28 08:37:44.358496: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n",
            "2021-04-28 08:37:44.358513: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n",
            "2021-04-28 08:37:44.358535: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2021-04-28 08:37:44.358551: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2021-04-28 08:37:44.358569: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2021-04-28 08:37:44.358634: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-04-28 08:37:44.359177: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-04-28 08:37:44.359700: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2021-04-28 08:37:44.359796: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2021-04-28 08:37:44.361654: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2021-04-28 08:37:44.361687: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
            "2021-04-28 08:37:44.361697: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
            "2021-04-28 08:37:44.361848: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-04-28 08:37:44.362437: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-04-28 08:37:44.362952: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-04-28 08:37:44.362992: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14257 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "WARNING:tensorflow:From /content/drive/My Drive/ua_test/gpt-2/src/sample.py:60: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "WARNING:tensorflow:From /content/drive/My Drive/ua_test/gpt-2/src/sample.py:30: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /content/drive/My Drive/ua_test/gpt-2/src/sample.py:65: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.random.categorical` instead.\n",
            "Model prompt >>> Він народився та жив закордоном, але душею завжди був в Україні\n",
            "2021-04-28 08:38:56.375105: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "======================================== SAMPLE 1 ========================================\n",
            ". Зародження від класичної завжди від душей. Разом відбувається для вшанові завжди; закордоном з міста та народної завжди. Оскільки завжди міста були дизайні, деревлявили безпечення і завжди дозволу злі надати на книги з міста, передбачаючи завжди на конкуренція, станом у передбачу дерева завжди на книги підготові обсягу відділення. Метає завжди до все відбувається від деревлях завжди підпорядків.\n",
            "Біографія.\n",
            "Транспорт як регіон прийнято �\n",
            "================================================================================\n",
            "Model prompt >>> Однак ніхто не знав\n",
            "======================================== SAMPLE 1 ========================================\n",
            "жливе про витубу через кримські політичні центрого вплива.\n",
            "Підприємний публічний протестантий західник не значними для ресурсів і нагромадженнями послуг літературної центри. Він не значними західники не відпочивають великим середньо-диводолом з відступили не зміни виступати покликають середнього кубів, пронизованих, кубів, будь-який і принципом з епідеміній спеціального сфера в послугах.\n",
            "Розглядають також родом про підтримки конторажної �\n",
            "================================================================================\n",
            "Model prompt >>> Поттер був дуже засмучений\n",
            "======================================== SAMPLE 1 ========================================\n",
            " стосунки для військової держави: саме на посаді в Гейті, що донька вплинула до Лорфогенції при один державі, на посаді, посаді, посібне відносно до сезону, і не був стосунка двох підполків. Але вже зливого доєднання передбачою примсько вони повторні для прибережності довжини один із прикриті випадку на відомості від «буряк піддалів», а також одружити солдатувавського сезону.\n",
            "Важливості в Гейті.\n",
            "Найбільшим відомом з відомості відомим глиним місто�\n",
            "================================================================================\n",
            "Model prompt >>> Оди два три\n",
            "======================================== SAMPLE 1 ========================================\n",
            "валення в Халосланді. На поняття Юпітера прозовідний до Збарварних переважності, викладачем на галуну, до яких його родини на кілька адаптужньому свого насадження будови Юпітера.\n",
            "Значні лабораторії.\n",
            "За всіх контактів населення збільшення до статті у фінських контактів на початку 1700—1804 років Забарвацьких поділ. У початковому вигляді контактуальної фінської статті, а на початковому вигляді контактуальної статті на початковому вигляді статті. Контактаці\n",
            "================================================================================\n",
            "Model prompt >>> Один два три\n",
            "======================================== SAMPLE 1 ========================================\n",
            "валися домовим власникам автономічних фольклонів.\n",
            "У деяких націоналістей України наприкінці XVII століття підрозділів, що вбили подій з лангелію перешкоджаїцькою.\n",
            "У липні XVIII століття були запроваджені глибоко менше, що це не мали менше, що прибули двосторити відповідно від глибок менше. При повітрянні з лангелії взяли на потім від цього лангелії від глибок двостежу. Він традиції свого батька відкриття глибок вижину власником, при відсутн\n",
            "================================================================================\n",
            "Model prompt >>> Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/contextlib.py\", line 130, in __exit__\n",
            "    self.gen.throw(type, value, traceback)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/ops.py\", line 5480, in get_controller\n",
            "    yield g\n",
            "  File \"interactive_conditional_samples.py\", line 73, in interact_model\n",
            "    raw_text = input(\"Model prompt >>> \")\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"interactive_conditional_samples.py\", line 91, in <module>\n",
            "    fire.Fire(interact_model)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/fire/core.py\", line 141, in Fire\n",
            "    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/fire/core.py\", line 471, in _Fire\n",
            "    target=component.__name__)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/fire/core.py\", line 681, in _CallAndUpdateTrace\n",
            "    component = fn(*varargs, **kwargs)\n",
            "  File \"interactive_conditional_samples.py\", line 88, in interact_model\n",
            "    print(\"=\" * 80)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/client/session.py\", line 1634, in __exit__\n",
            "    close_thread.join(30.0)\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 1048, in join\n",
            "    self._wait_for_tstate_lock(timeout=max(timeout, 0))\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 1060, in _wait_for_tstate_lock\n",
            "    elif lock.acquire(block, timeout):\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "euZMjnWgV5ha",
        "outputId": "e4734242-deb3-4146-8841-6a6963ed4bbd"
      },
      "source": [
        "!python interactive_conditional_samples.py --temperature 0.8 --top_k 40 --model_name wikinew"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"interactive_conditional_samples.py\", line 91, in <module>\n",
            "    fire.Fire(interact_model)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/fire/core.py\", line 141, in Fire\n",
            "    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/fire/core.py\", line 471, in _Fire\n",
            "    target=component.__name__)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/fire/core.py\", line 681, in _CallAndUpdateTrace\n",
            "    component = fn(*varargs, **kwargs)\n",
            "  File \"interactive_conditional_samples.py\", line 73, in interact_model\n",
            "    raw_text = input(\"Model prompt >>> \")\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "so8MaeZa-RjT"
      },
      "source": [
        "extract wikipedia **textes**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8eSFIT9I2p9N",
        "outputId": "3438d402-eab9-4628-c7d9-71866d76968b"
      },
      "source": [
        "pip install wget"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-cp37-none-any.whl size=9681 sha256=0306a2e9f4579f1f50579551cb92536922308d9c1632679aa06018c566e4bb65\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTVhXNpI4p1j",
        "outputId": "bea6fa3c-b056-4775-ef79-805b07cb7a12"
      },
      "source": [
        "!wget -c https://dumps.wikimedia.org/ukwiki/latest/ukwiki-latest-pages-articles.xml.bz2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-27 07:44:37--  https://dumps.wikimedia.org/ukwiki/latest/ukwiki-latest-pages-articles.xml.bz2\n",
            "Resolving dumps.wikimedia.org (dumps.wikimedia.org)... 208.80.154.7, 2620:0:861:1:208:80:154:7\n",
            "Connecting to dumps.wikimedia.org (dumps.wikimedia.org)|208.80.154.7|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1689900861 (1.6G) [application/octet-stream]\n",
            "Saving to: ‘ukwiki-latest-pages-articles.xml.bz2’\n",
            "\n",
            "ukwiki-latest-pages 100%[===================>]   1.57G  4.57MB/s    in 5m 47s  \n",
            "\n",
            "2021-04-27 07:50:25 (4.64 MB/s) - ‘ukwiki-latest-pages-articles.xml.bz2’ saved [1689900861/1689900861]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1eZ32kJx7ZrL",
        "outputId": "a5961685-bdd5-42fe-f454-06e53448c7dd"
      },
      "source": [
        "ll"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 1650298\n",
            "drwx------ 6 root       4096 Apr 16 10:59 \u001b[0m\u001b[01;34mgpt-2\u001b[0m/\n",
            "-rw------- 1 root 1689900861 Apr 20 20:58 ukwiki-latest-pages-articles.xml.bz2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWw50P1B9F8h",
        "outputId": "08e714d0-b6f4-46dc-e95a-e7c6c0c591f5"
      },
      "source": [
        "pip install wikiextractor"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wikiextractor\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0c/bd/6b8ffc89fa4abefd801f7b0f83bc17382664484bd32eb6529b243d7a8f12/wikiextractor-3.0.4-py3-none-any.whl (46kB)\n",
            "\r\u001b[K     |███████                         | 10kB 14.4MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 20kB 12.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 30kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 40kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 2.7MB/s \n",
            "\u001b[?25hInstalling collected packages: wikiextractor\n",
            "Successfully installed wikiextractor-3.0.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7spCsts5747L",
        "outputId": "77b64b3b-4dfe-44a3-fcc6-d5c33837d088"
      },
      "source": [
        "!python -m wikiextractor.WikiExtractor ukwiki-latest-pages-articles.xml.bz2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO: Preprocessing 'ukwiki-latest-pages-articles.xml.bz2' to collect template definitions: this may take some time.\n",
            "INFO: Preprocessed 100000 pages\n",
            "INFO: Preprocessed 200000 pages\n",
            "INFO: Preprocessed 300000 pages\n",
            "INFO: Preprocessed 400000 pages\n",
            "INFO: Preprocessed 500000 pages\n",
            "INFO: Preprocessed 600000 pages\n",
            "INFO: Preprocessed 700000 pages\n",
            "INFO: Preprocessed 800000 pages\n",
            "INFO: Preprocessed 900000 pages\n",
            "INFO: Preprocessed 1000000 pages\n",
            "INFO: Preprocessed 1100000 pages\n",
            "INFO: Preprocessed 1200000 pages\n",
            "INFO: Preprocessed 1300000 pages\n",
            "INFO: Preprocessed 1400000 pages\n",
            "INFO: Preprocessed 1500000 pages\n",
            "INFO: Preprocessed 1600000 pages\n",
            "INFO: Preprocessed 1700000 pages\n",
            "INFO: Preprocessed 1800000 pages\n",
            "INFO: Preprocessed 1900000 pages\n",
            "INFO: Preprocessed 2000000 pages\n",
            "INFO: Preprocessed 2100000 pages\n",
            "INFO: Preprocessed 2200000 pages\n",
            "INFO: Preprocessed 2300000 pages\n",
            "INFO: Preprocessed 2400000 pages\n",
            "INFO: Loaded 145599 templates in 638.2s\n",
            "INFO: Starting page extraction from ukwiki-latest-pages-articles.xml.bz2.\n",
            "INFO: Using 1 extract processes.\n",
            "INFO: Extracted 100000 articles (647.8 art/s)\n",
            "INFO: Extracted 200000 articles (727.5 art/s)\n",
            "INFO: Extracted 300000 articles (834.3 art/s)\n",
            "INFO: Extracted 400000 articles (776.3 art/s)\n",
            "INFO: Extracted 500000 articles (864.4 art/s)\n",
            "INFO: Extracted 600000 articles (1122.6 art/s)\n",
            "INFO: Extracted 700000 articles (847.6 art/s)\n",
            "INFO: Extracted 800000 articles (765.2 art/s)\n",
            "INFO: Extracted 900000 articles (728.1 art/s)\n",
            "INFO: Extracted 1000000 articles (745.5 art/s)\n",
            "INFO: Extracted 1100000 articles (769.6 art/s)\n",
            "INFO: Extracted 1200000 articles (760.9 art/s)\n",
            "INFO: Extracted 1300000 articles (942.0 art/s)\n",
            "INFO: Extracted 1400000 articles (907.3 art/s)\n",
            "INFO: Extracted 1500000 articles (971.9 art/s)\n",
            "INFO: Extracted 1600000 articles (993.2 art/s)\n",
            "INFO: Extracted 1700000 articles (1000.2 art/s)\n",
            "INFO: Extracted 1800000 articles (1779.8 art/s)\n",
            "INFO: Extracted 1900000 articles (1063.1 art/s)\n",
            "INFO: Finished 1-process extraction of 1925546 articles in 2229.6s (863.6 art/s)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_jsfnci2c4x",
        "outputId": "c12177d9-7bc6-44d8-9c36-b96c03642bf7"
      },
      "source": [
        "pip install gensim"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efFrruG2qYyz",
        "outputId": "c63d70fb-648d-4e42-a87b-0660e40db839"
      },
      "source": [
        "ll"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 4\n",
            "drwxr-xr-x 1 root 4096 Jun 15 13:37 \u001b[0m\u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCdk7lvq2CSC"
      },
      "source": [
        "import glob\n",
        "from pathlib import Path\n",
        "from gensim.corpora import WikiCorpus\n",
        "import os\n",
        "\n",
        "filenames = [str(x) for x in Path(\"./text/\").glob(\"**/wiki*\")]\n",
        "# Add all texts into one file\n",
        "\n",
        "with open('corpus.txt', 'w', encoding='utf-8') as outfile:\n",
        "       for fname in filenames:\n",
        "            with open(fname, encoding=\"utf8\") as infile:\n",
        "                  \n",
        "                  outfile.write(infile.read())\n",
        "outfile.close()\n",
        "\n",
        "fin = open('corpus.txt', \"rt\", encoding='utf-8')\n",
        "#output file to write the result to\n",
        "fout = open(\"out.txt\", \"wt\", encoding='utf-8')\n",
        "#for each line in the input file\n",
        "for line in fin:\n",
        "\t#read replace the string and write to output file\n",
        "  fout.write(line.replace('</doc>', ''))\n",
        "#close input and output files\n",
        "fin.close()\n",
        "fout.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5r6qzgrfLdPa",
        "outputId": "edd6b2a9-ff65-4e7e-b4ae-20d867a0c490"
      },
      "source": [
        "cd /content/drive/MyDrive/ua_test/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: '/content/drive/MyDrive/ua_test/'\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebQJ-QIdMmib",
        "outputId": "ff264ce2-534a-4753-c7a4-61192fcd4d7e"
      },
      "source": [
        "cd //"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PqmzTaAxP7lA",
        "outputId": "bbc30446-43a8-47a8-e93d-17ec125c2db9"
      },
      "source": [
        "ll -a"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 64\n",
            "drwx------ 1 root 4096 Apr 29 13:04 \u001b[0m\u001b[01;34m.\u001b[0m/\n",
            "drwxr-xr-x 1 root 4096 Apr 29 13:03 \u001b[01;34m..\u001b[0m/\n",
            "-r-xr-xr-x 1 root 1169 Jan  1  2000 \u001b[01;32m.bashrc\u001b[0m*\n",
            "drwxr-xr-x 1 root 4096 Apr 23 13:27 \u001b[01;34m.cache\u001b[0m/\n",
            "drwxr-xr-x 1 root 4096 Apr 23 13:25 \u001b[01;34m.config\u001b[0m/\n",
            "drwxr-xr-x 3 root 4096 Apr 21 13:39 \u001b[01;34m.gsutil\u001b[0m/\n",
            "drwxr-xr-x 1 root 4096 Apr 23 13:25 \u001b[01;34m.ipython\u001b[0m/\n",
            "drwx------ 2 root 4096 Apr 23 13:25 \u001b[01;34m.jupyter\u001b[0m/\n",
            "drwxr-xr-x 2 root 4096 Apr 29 13:04 \u001b[01;34m.keras\u001b[0m/\n",
            "drwx------ 1 root 4096 Apr 23 13:25 \u001b[01;34m.local\u001b[0m/\n",
            "drwxr-xr-x 4 root 4096 Apr 23 13:26 \u001b[01;34m.npm\u001b[0m/\n",
            "-rw-r--r-- 1 root  148 Aug 17  2015 .profile\n",
            "-r-xr-xr-x 1 root  254 Jan  1  2000 \u001b[01;32m.tmux.conf\u001b[0m*\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}